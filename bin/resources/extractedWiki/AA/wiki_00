<doc id="1" url="https://de.wikipedia.org/wiki?curid=1" title="Alan Smithee">
Alan Smithee

Alan Smithee steht als Pseudonym für einen fiktiven Regisseur, der Filme verantwortet, bei denen der eigentliche Regisseur seinen Namen nicht mit dem Werk in Verbindung gebracht haben möchte. Von 1968 bis 2000 wurde es von der Directors Guild of America (DGA) für solche Situationen empfohlen, seither ist es Thomas Lee. "Alan Smithee" ist jedoch weiterhin in Gebrauch.
Alternative Schreibweisen sind unter anderem die Ursprungsvariante "Allen Smithee" sowie "Alan Smythee" und "Adam Smithee". Auch zwei teilweise asiatisch anmutende Schreibweisen "Alan Smi Thee" und "Sumishii Aran" gehören – so die Internet Movie Database – dazu.
Das Pseudonym entstand 1968 infolge der Arbeiten am Western-Film "Death of a Gunfighter" (deutscher Titel "Frank Patch – Deine Stunden sind gezählt"). Regisseur Robert Totten und Hauptdarsteller Richard Widmark gerieten in einen Streit, woraufhin Don Siegel als neuer Regisseur eingesetzt wurde.
Der Film trug nach Abschluss der Arbeiten noch deutlich Tottens Handschrift, der auch mehr Drehtage als Siegel daran gearbeitet hatte, weshalb dieser die Nennung seines Namens als Regisseur ablehnte. Totten selbst lehnte aber ebenfalls ab. Als Lösung wurde "Allen Smithee" als ein möglichst einzigartiger Name gewählt (bei der späteren Variante "Alan Smithee" war das Anagramm "The Alias Men" vermutlich kein Entstehungsgrund).
In den zeitgenössischen Kritiken wurde der Regisseur u. a. von Roger Ebert mit den Worten gelobt: 
1997 kam die Parodie "An Alan Smithee Film: Burn Hollywood Burn" (deutscher Titel "Fahr zur Hölle Hollywood") in die Kinos, was das Pseudonym einem größeren Publikum bekannt machte, nicht zuletzt weil Arthur Hiller, der eigentliche Regisseur des Films, selbst seinen Namen zurückzog und analog zum Filmtitel das Pseudonym "Alan Smithee" benutzte. Der Film gilt als einer der schlechtesten Filme der 1990er Jahre und gewann fünf Goldene Himbeeren.
Der Film "Supernova" ist der erste Post-Smithee-Film, dort führte ein gewisser "Thomas Lee" alias Walter Hill die Regie.
Die Verwendung dieses oder eines anderen Pseudonyms ist für Mitglieder der DGA streng reglementiert. Ein Regisseur, der für einen von ihm gedrehten Film seinen Namen nicht hergeben möchte, hat nach Sichtung des fertigen Films drei Tage Zeit, anzuzeigen, dass er ein Pseudonym verwenden möchte. Der Rat der DGA entscheidet binnen zwei Tagen über das Anliegen. Erhebt die Produktionsfirma Einspruch, entscheidet ein Komitee aus Mitgliedern der DGA und der Vereinigung der Film- und Fernsehproduzenten, ob der Regisseur ein Pseudonym angeben darf. Über die Beantragung muss der Regisseur Stillschweigen halten, ebenso darf er den fertigen Film nicht öffentlich kritisieren, wenn die DGA ihm die Verwendung eines Pseudonyms zugesteht. Ein Antrag des Regisseurs auf Pseudonymisierung kann abgelehnt werden, so durfte Tony Kaye den Namen Smithee bei dem Film "American History X" nicht einsetzen, obwohl er den Antrag stellte.
Auch bei nicht-US-amerikanischen Produktionen wird der Name verwendet, wie etwa beim Pilotfilm der Fernsehserie "Schulmädchen". 2007 sendete die ARD am 8. und 9. August den zweiteiligen TV-Film "Paparazzo". Auch in diesem Werk erscheint anstatt des eigentlichen Regisseurs Stephan Wagner Alan Smithee im Abspann.
Zu den Regisseuren, die das Pseudonym benutzt haben, gehören:
Der Pilotfilm der Serie "MacGyver" und die fünfte Folge der ersten Staffel führen einen Alan Smithee als Regisseur. Auf der TV-Serien-Seite TV Rage wird Jerrold Freedman als Regisseur des Pilotfilms angegeben. Der Regisseur der fünften Folge ist unbekannt.
Zu den Drehbuchautoren, die das Pseudonym benutzt haben, gehören:
Auch in Computerspielen wird dieses Pseudonym angegeben:
2014 produzierte die New Yorker Performance-Kompanie Big Dance Theater "Alan Smithee Directed this Play", das im August des Jahres auch in Berlin bei Tanz im August aufgeführt wurde. 

</doc>
<doc id="3" url="https://de.wikipedia.org/wiki?curid=3" title="Actinium">
Actinium

Actinium (latinisiert von "aktína" ‚Strahl‘) ist ein radioaktives chemisches Element mit dem Elementsymbol Ac und der Ordnungszahl 89. Im Periodensystem der Elemente steht es in der 3. IUPAC-Gruppe, der Scandiumgruppe. Das Element ist ein Metall und gehört zur 7. Periode, d-Block. Es ist der Namensgeber der Gruppe der Actinoide, der ihm folgenden 14 Elemente.
Actinium wurde im Jahr 1899 von dem französischen Chemiker André-Louis Debierne entdeckt, der es aus Pechblende isolierte und ihm zunächst Ähnlichkeiten mit dem Titan oder dem Thorium zuschrieb. Friedrich Giesel entdeckte Actinium unabhängig davon im Jahr 1902 und beschrieb eine Ähnlichkeit zum Lanthan; im Jahr 1904 benannte er es „Emanium“. Nach einem Vergleich der Substanzen im Jahr 1904 wurde Debiernes Namensgebung der Vorzug gegeben, da er es zuerst entdeckt hatte.
Die Geschichte der Entdeckung wurde in Publikationen von 1971 und später im Jahr 2000 immer noch als fraglich beschrieben. Sie zeigen, dass die Publikationen von 1904 einerseits und die von 1899 und 1900 andererseits Widersprüche aufweisen.
Da in Uranerzen nur wenig Actinium vorhanden ist, spielt diese Quelle keine Rolle für die Gewinnung. Technisch wird das Isotop Ac durch Bestrahlung von Ra mit Neutronen in Kernreaktoren hergestellt.
Durch den schnellen Zerfall des Actiniums waren stets nur geringe Mengen verfügbar. Die erste künstliche Herstellung von Actinium wurde im Argonne National Laboratory in Chicago durchgeführt.
Das Metall ist silberweiß glänzend und relativ weich. Aufgrund seiner starken Radioaktivität leuchtet Actinium im Dunkeln in einem hellblauen Licht.
Actinium ist das namensgebende Element der Actinoiden, ähnlich wie Lanthan für die Lanthanoiden. Die Gruppe der Elemente zeigt deutlichere Unterschiede als die Lanthanoide; daher dauerte es bis 1945, bis Glenn T. Seaborg die wichtigsten Änderungen zum Periodensystem von Mendelejew vorschlagen konnte: die Einführung der Actinoide.
Es ist sehr reaktionsfähig und wird von Luft und Wasser angegriffen, überzieht sich aber mit einer Schicht von Actiniumoxid, wodurch es vor weiterer Oxidation geschützt ist. Das Ac-Ion ist farblos. Das chemische Verhalten von Actinium ähnelt sehr dem Lanthan. Actinium ist in allen zehn bekannten Verbindungen dreiwertig.
Bekannt sind 26 Isotope, wovon nur zwei natürlich vorkommen. Das langlebigste Isotop Ac (Halbwertszeit 21,8 Jahre) hat zwei Zerfallskanäle: es ist ein Alpha- und Beta-Strahler. Ac ist ein Zerfallsprodukt des Uranisotops U und kommt zu einem kleinen Teil in Uranerzen vor. Daraus lassen sich wägbare Mengen Ac gewinnen, die somit ein verhältnismäßig einfaches Studium dieses Elementes ermöglichen. Da sich unter den radioaktiven Zerfallsprodukten einige Gammastrahler befinden, sind aber aufwändige Strahlenschutzvorkehrungen nötig.
Actinium wird zur Erzeugung von Neutronen eingesetzt, die bei Aktivierungsanalysen eine Rolle spielen. Außerdem wird es für die thermoionische Energieumwandlung genutzt.
Beim dualen Zerfall des Ac geht der größte Teil unter Emission von Beta-Teilchen in das Thoriumisotop Th, aber ca. 1 % zerfällt durch Alpha-Emission zu Francium Fr. Eine Lösung von Ac ist daher als Quelle für das kurzlebige Fr verwendbar. Letzteres kann dann regelmäßig abgetrennt und untersucht werden.
Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.
Nur eine geringe Anzahl von Actiniumverbindungen ist bekannt. Mit Ausnahme von AcPO sind sie alle den entsprechenden Lanthanverbindungen ähnlich und enthalten Actinium in der Oxidationsstufe +3. Insbesondere unterscheiden sich die Gitterkonstanten der jeweiligen Lanthan- und Actinium-Verbindungen nur in wenigen Prozent.
Actinium(III)-oxid (AcO) kann durch Erhitzen des Hydroxids bei 500 °C oder des Oxalats bei 1100 °C im Vakuum erhalten werden. Das Kristallgitter ist isotyp mit den Oxiden der meisten dreiwertigen Seltenerdmetalle.
Actinium(III)-fluorid (AcF) kann entweder in Lösung oder durch Feststoffreaktion dargestellt werden. Im ersten Fall gibt man bei Raumtemperatur Flusssäure zu einer Ac-Lösung und fällt das Produkt aus. im anderen Fall wird Actinium-Metall mit Fluorwasserstoff bei 700 °C in einer Platinapparatur behandelt.
Actinium(III)-chlorid (AcCl) wird durch Umsetzung von Actiniumhydroxid oder -oxalat mit Tetrachlormethan bei Temperaturen oberhalb von 960 °C erhalten.
Die Reaktion von Aluminiumbromid und Actinium(III)-oxid führt zum Actinium(III)-bromid (AcBr) und Behandlung mit feuchtem Ammoniak bei 500 °C führt zum Oxibromid AcOBr.
Gibt man Natriumdihydrogenphosphat (NaHPO) zu einer Lösung von Actinium in Salzsäure, erhält man weiß gefärbtes Actiniumphosphat (AcPO · 0,5 HO); ein Erhitzen von Actinium(III)-oxalat mit Schwefelwasserstoff bei 1400 °C für ein paar Minuten führt zu schwarzem Actinium(III)-sulfid (AcS).

</doc>
<doc id="5" url="https://de.wikipedia.org/wiki?curid=5" title="Ang Lee">
Ang Lee

Ang Lee (; * 23. Oktober 1954 in Pingtung, Taiwan) ist ein US-amerikanisch-taiwanischer Filmregisseur, Drehbuchautor und Produzent. Er ist als vielfach ausgezeichneter Regisseur bekannt für so unterschiedliche Filme wie "Eat Drink Man Woman", die Jane-Austen-Adaption "Sinn und Sinnlichkeit", den Martial Arts-Film "Tiger and Dragon" sowie "Brokeback Mountain", für den er 2006 den Regie-Oscar erhielt. Einen weiteren Oscar erhielt er 2013 für seine Regiearbeit an "".
Ang Lee wurde 1954 in Taiwan geboren. Seine Eltern, Emigranten aus China, lernten sich in Taiwan kennen, Lee ist ihr ältester Sohn. Die Großeltern väterlicher- und mütterlicherseits sind im Zuge der kommunistischen Revolution in China ums Leben gekommen. Da sein Vater als Lehrer häufiger die Arbeitsstelle wechselte, wuchs Ang Lee in verschiedenen Städten Taiwans auf.
Entgegen den Wünschen seiner Eltern, wie sein Vater eine klassische akademische Laufbahn einzuschlagen, interessierte sich Lee für das Schauspiel und absolvierte mit ihrem Einverständnis zunächst ein Theater- und Filmstudium in Taipeh. Im Anschluss daran ging er 1978 in die USA, um an der Universität von Illinois in Urbana-Champaign Theaterwissenschaft und -regie zu studieren. Nach dem Erwerb seines B.A. in Illinois verlegte er sich ganz auf das Studium der Film- und Theaterproduktion an der Universität von New York, das er 1985 mit einem Master abschloss. Danach entschloss er sich, mit seiner ebenfalls aus Taiwan stammenden Ehefrau zusammen in den USA zu bleiben.
Sein Interesse verschob sich trotz erster Erfahrungen mit dem Super-8-Film in Taiwan erst spät ganz auf Filmregie und -produktion – auch weil Lee seinen Berufswunsch seiner Familie und insbesondere seinem Vater gegenüber lange Zeit nicht eingestehen wollte.
Nach dem Studium konnte er zunächst keine eigenen Projekte umsetzen. Erst ab 1992, als er seinen ersten Langfilm fertigstellte, zeichnete sich eine kontinuierliche Karriere als Regisseur ab.
Als seine bisher größte Erfolge – sowohl beim Publikum als auch bei der Kritik – gelten das Martial Arts-Drama "Tiger and Dragon" mit einer pan-asiatischen Starbesetzung und der Post-Western-Liebesfilm Brokeback Mountain mit Heath Ledger und Jake Gyllenhaal. Für letzteren bekam Lee 2006 als erster asiatisch-stämmiger und nicht-weißer Regisseur den Oscar für die beste Regie. Außerdem wurden Lees Filme, neben vielen weiteren Preisen, mit mittlerweile zwei Goldenen Bären der Berlinale und zwei Goldenen Löwen der Filmfestspiele von Venedig ausgezeichnet.
Lee ist seit 1983 mit der Mikrobiologin Jane Lin verheiratet. Mit ihren Söhnen Haan (* 1984) und Mason (* 1990) leben die beiden in White Plains, Westchester County, im Bundesstaat New York. Ang Lee besitzt die US-amerikanische Staatsbürgerschaft.
Nach seinen ersten Filmerfahrungen in Taiwan setzte sich Lee erst wieder während seines Studiums in den USA ernsthaft mit dem Filmemachen auseinander. Im Rahmen seines Studiums in New York drehte er einige Kurzfilme und wirkte unter anderem beim Abschlussdreh seines Studienkollegen Spike Lee als Regieassistent mit. Sein eigener Abschlussfilm "Fine Line" gewann 1985 zwei Preise beim renommierten Filmfest seiner Universität. Erst 1992 gelang es ihm, nach dem Gewinn eines hochdotierten Drehbuchwettbewerbs in Taiwan, den ersten einer Reihe von drei Filmen zu drehen, die west-östliche Konflikte taiwanischer Familien zum Thema haben.
Diese ersten drei Langfilme, die Lee realisieren konnte, werden im Allgemeinen unter dem Begriff "Father Knows Best" gefasst. Diese Bezeichnung geht auf die wiederkehrende Figur des chinesischen Familienoberhaupts, gespielt jeweils vom taiwanischen Schauspieler Sihung Lung, zurück. Die drei Filme thematisieren, wie später noch öfter bei Ang Lee, familiäre Probleme, die aus dem Konflikt zwischen Selbstbestimmung und Tradition, zwischen Innen und Außen, zwischen Ost und West sowie zwischen den Generationen herrühren. Die Filme sind allesamt US-amerikanisch-taiwanische Koproduktionen. Anders als bei allen bislang folgenden Projekten handelt es sich bei den ersten Filmen Lees nicht um Adaptionen, sondern um Filme nach von ihm selbst geschriebenen Originaldrehbüchern.
Der erste Film, "Schiebende Hände" (1992), handelt vom Einzug eines chinesischen Vaters bei seinem erwachsenen Sohn und der US-amerikanischen Schwiegertochter in New York und den interkulturellen Problemen, die in der neuen Wohngemeinschaft entstehen. Dies war die erste Zusammenarbeit zwischen Lee und dem Drehbuchautor und Produzenten James Schamus – seitdem bildeten die beiden bei jedem Film Lees eine enge Arbeitsgemeinschaft. Wie in den beiden folgenden Filmen schrieben sie auch gemeinsam das Drehbuch. In allen weiteren Filmen Lees (mit Ausnahme des Kurzfilms "The Hire: Chosen") hat Schamus seither entscheidende Funktionen ausgeübt.
Auch die regelmäßige Zusammenarbeit mit dem Filmeditor Tim Squyres nahm in Lees Erstling ihren Anfang. Mit Ausnahme des Erfolgsfilms "Brokeback Mountain" von 2005 hat Squires jeden Film, den Ang Lee gedreht hat, geschnitten.
Nach dem Erfolg seines Erstlings konnte Lee als Nächstes "Das Hochzeitsbankett" (1993) drehen, eine Komödie über die fingierte Eheschließung eines homosexuellen Exil-Taiwaners in den USA. Erneut taucht hier die Figur des strengen, aber weisen Familienoberhaupts auf. Hatte "Schiebende Hände" zunächst vor allem in Taiwan für Aufmerksamkeit (und Preise) gesorgt, wurde mit dem zweiten Langfilm Lees auch Europa auf den aufstrebenden Regisseur aufmerksam: Der Film erhielt bei der Berlinale 1993 den "Goldenen Bären" als "Bester fremdsprachiger Film" und war zudem für einen Oscar nominiert. Er gilt darüber hinaus als einer der profitabelsten Low-Budget-Filme des Jahres 1993. Mit nur einer Million US-Dollar Produktionskosten erzielte er ein Einspielergebnis von über 23 Millionen US-Dollar.
Sihung Lung ist auch im letzten Teil der Trilogie, "Eat Drink Man Woman" (1994), die „kongeniale Verkörperung des chinesischen Familienoberhaupts“, das „Zentrum dieser Maskeraden, in denen es darum geht, ein altes Gesicht zu wahren und dann zu lernen, es zu verlieren, um ein neues, lebenstauglicheres zu gewinnen.“ Dieses Mal ist er der verwitwete Vater dreier Töchter, die ihr Leben und ihre Lieben auf unterschiedliche Art angehen und dabei ebenfalls innerfamiliäre Konflikte klären müssen. "Eat Drink Man Woman" wurde, anders als seine Vorgänger, in Taipeh gedreht. Im Mittelpunkt des Films stehen (der Titel deutet es an) die Liebe und das Essen. Ang Lee, privat ein passionierter Koch, legte hierbei besonders großen Wert auf die kulinarische Komponente als Stilmittel und konzipierte die Hauptfigur des älteren Witwers als berühmten Koch.
Mit dem Angebot der Produzentin Lindsay Doran, die von der britischen Schauspielerin Emma Thompson verfasste Adaption des Romans "Verstand und Gefühl" von Jane Austen in Großbritannien zu drehen, eröffnete sich Lee eine lange ersehnte neue Perspektive jenseits asiatisch geprägter Stoffe.
In einer neuen Trilogie setzt er sich mit unterschiedlichen Kulturen auseinander:
"Tiger and Dragon" sowie "Hulk" sind sehr unterschiedliche Action-Filme. Mit "Tiger and Dragon" gewann Lee zwei Golden Globes. Das Werk wurde außerdem mit vier Academy Awards (Oscars) prämiert, darunter der Trophäe für den besten fremdsprachigen Film. Für diesen Film wurde er 2001 auch mit einem Chlotrudis Award ausgezeichnet, seinen zweiten Chlotrudis erhielt er 2006 für "Brokeback Mountain".
Für "Brokeback Mountain" wurde Lee mit einer Vielzahl von Filmpreisen geehrt, darunter der Oscar für die beste Regie, der Goldene Löwe der Filmfestspiele von Venedig sowie die Auszeichnung der Hollywood Foreign Press Association als bester Regisseur des Jahres. 2007 verfilmte er mit "Gefahr und Begierde" eine Kurzgeschichte von Eileen Chang. Der Thriller spielt zur Zeit des Zweiten Weltkriegs in Shanghai und handelt von einer jungen chinesischen Agentin (gespielt von Tang Wei), die beauftragt wird, einen hochrangigen Verräter (Tony Leung Chiu Wai) zu liquidieren. Lees erste chinesischsprachige Spielfilmproduktion seit "Tiger and Dragon" war 2007 im offiziellen Wettbewerb der 64. Filmfestspiele von Venedig vertreten und brachte ihm erneut den Goldenen Löwen ein. Im selben Jahr wurde "Gefahr und Begierde" als offizieller taiwanischer Beitrag für die Nominierung um den besten fremdsprachigen Film bei der Oscar-Verleihung 2008 ausgewählt, später aber auf Empfehlung der Academy of Motion Picture Arts and Sciences wieder zurückgezogen und durch Chen Huai-Ens "Lian xi qu" ersetzt.
Ende Februar 2009 wurde bekannt gegeben, dass Lee die Jury der 66. Filmfestspiele von Venedig leiten werde. Zwei Monate später erhielt er für seine Komödie "Taking Woodstock" eine Einladung in den Wettbewerb der 62. Internationalen Filmfestspiele von Cannes.
2013 wurde er in die Wettbewerbsjury des 66. Filmfestivals von Cannes berufen.
Ang Lee ist ein international anerkannter und erfolgreicher Regisseur und gilt als einer der vielseitigsten Filmemacher der letzten Jahre. Häufig behandelt Lee in seinen Filmen das Thema Familie auf eine Art und Weise, die autobiographische Züge seines eigenen Lebens trägt. Er lässt seine Umgebung ganz bewusst auf sich einwirken und bringt diese in seine Filme ein.
Kennzeichnend für die meisten seiner Filme sind eine wenig geradlinige Erzählstruktur, die die Charaktere und die Geschichte aus verschiedenen Blickwinkeln darstellt. Er verknüpft die Konflikte des menschlichen Lebens mit traditionellen und innovativen Stilelementen.
Für Ang Lee sind die klassisch-soliden Erzählstrukturen zu langweilig, daher kombiniert er verschiedene Genres und Epochen. Er selbst sagte einmal:

</doc>
<doc id="7" url="https://de.wikipedia.org/wiki?curid=7" title="Anschluss (Soziologie)">
Anschluss (Soziologie)

Anschluss ist in der Soziologie ein Fachbegriff aus der Systemtheorie von Niklas Luhmann und bezeichnet die in einer sozialen Begegnung auf eine Selektion der anderen Seite folgende, selbst gewählte Selektion. Diese Selektionen beziehen sich aufeinander.
Die Anschlussfähigkeit ist die Kapazität von Systemen zu gewährleisten, dass sich an die Selektionen eines Systems weitere anschließen können. Alle sozialen Systeme reproduzieren sich über Kommunikation (z. B. Wirtschaftssystem oder Politik) oder Handlungen (Medizin und Erziehungssystem). Dies gelingt nur, wenn die einzelnen Einheiten aneinander anschlussfähig sind, was durch einen systemspezifischen Code geleistet wird, der als zentrale Logik (Leitunterscheidung) aller Kommunikation zugrunde liegt und sie als systemzugehörig erkennbar macht. Im Wirtschaftssystem beispielsweise sorgt der Code "zahlen/nicht zahlen" dafür, dass die Kommunikationen sich auf sich selbst beziehen und sich selbst reproduzieren kann, also dass auf jede Zahlung eine neue erfolgt. Dies funktioniert über das generalisierte Kommunikationsmedium Geld, das die letzte Zahlung mit der jetzigen verknüpft. Würde das Geld nicht mehr akzeptiert, folgt der Zahlung keine weitere Zahlung mehr und das System hätte seine Anschlussfähigkeit verloren. Die Anschlussfähigkeit innerhalb eines Systems wird als Selbstreferenz bezeichnet, im Gegensatz zum fremdreferentiellen Bezug auf die Umwelt (Welt, andere Systeme).
Den Begriff hat Luhmann auf eine Anregung eines Bielefelder Kollegen, des Philosophen Jürgen Frese entwickelt. Frese zeigte in einem Sektionsreferat des Achten Deutschen Kongresses für Philosophie in Heidelberg (1966, gedruckt 1967) mit dem Titel „Sprechen als Metapher für Handeln“, dass es fruchtbar ist, von den dominanten Handlungsmodellen Arbeit und Konsum abzurücken und ergänzend Sprechen als Modell für Handeln zu nutzen. Frese schreibt: „Die wichtigste Errungenschaft, die die Sprachmetapher für die Aufhellung des nicht-sprachlichen Handelns einbringt, ist ihre Leistung, Reihenbildung erklärbar zu machen. Fassen wir Satz und Handlung zum neutralen und an andere Philosopheme anschließbaren Begriff des Aktes zusammen, so können wir ... sagen: Der Sinn eines Aktes ist das als eine bestimmte Situation gegebene Ensemble der Möglichkeiten, an diesen Akt weitere Akte anzuschließen; d. h. der Sinn eines Aktes ist die Mannigfaltigkeit der Anschließbarkeiten, die er eröffnet.“ Diese Idee wurde von Luhmann aufgegriffen und im Rahmen seiner Systemtheorie weiterentwickelt. Frese selbst baute sie im Rahmen seiner Lehre von den Formularen weiter aus.

</doc>
<doc id="10" url="https://de.wikipedia.org/wiki?curid=10" title="Aussagenlogik">
Aussagenlogik

Die Aussagenlogik ist ein Teilgebiet der Logik, das sich mit Aussagen und deren Verknüpfung durch Junktoren befasst, ausgehend von strukturlosen Elementaraussagen (Atomen), denen ein Wahrheitswert zugeordnet wird. In der "klassischen Aussagenlogik" wird jeder Aussage genau einer der zwei Wahrheitswerte „wahr“ und „falsch“ zugeordnet. Der Wahrheitswert einer zusammengesetzten Aussage lässt sich ohne zusätzliche Informationen aus den Wahrheitswerten ihrer Teilaussagen bestimmen.
Historisch geht die Aussagenlogik zurück bis zu Aristoteles, der erstmals aussagenlogische Grundsätze diskutierte, nämlich in seiner Metaphysik den Satz vom Widerspruch und den Satz vom ausgeschlossenen Dritten, und der in seiner ersten Analytik den indirekten Beweis thematisierte. Die zweiwertige aussagenlogische Semantik entwickelten etwas später die megarischen Philosophen Diodoros Kronos und Philon. Die Aussagensemantik und -axiomatik kombinierte der Stoiker Chrysippos von Soli, der den ersten aussagenlogischen Kalkül formulierte. Die Weiterentwicklung der Aussagenlogik der Stoa durch das Mittelalter wird oft übersehen. Eine erste vollständige und entscheidbare Formalisierung für aussagenlogische Tautologien – allerdings noch nicht für das aussagenlogische Schließen – schuf George Boole 1847 mit seinem algebraischen Logikkalkül. Den ersten aussagenlogischen Kalkül mit Schlussregeln formulierte Gottlob Frege im Rahmen seiner Begriffsschrift 1879. Er war die Vorlage für den Aussagenkalkül von Bertrand Russell 1910, der sich später durchsetzte (s. u.).
Da in der heutigen Mathematik die klassische Aussagenlogik maßgeblich wurde, wird in diesem Artikel dieser moderne Haupttypus der Aussagenlogik behandelt. Allgemein ist die klassische Logik durch zwei Eigenschaften charakterisiert:
Das Prinzip der Zweiwertigkeit wird oft mit dem Satz vom ausgeschlossenen Dritten verwechselt.
Die "klassische Aussagenlogik" ist jenes Gebiet der klassischen Logik, das die innere Struktur von Sätzen (Aussagen) daraufhin untersucht, aus welchen anderen Sätzen (Teilsätzen) sie zusammengesetzt sind und wie diese Teilsätze miteinander verknüpft sind. Die innere Struktur von Sätzen, die ihrerseits nicht in weitere Teilsätze zerlegt werden können, wird von der Aussagenlogik nicht betrachtet. Ein Beispiel: Die Aussage „Alle Katzen sind Hunde, und die Erde ist eine Scheibe“ ist mit dem Bindewort „und“ aus den beiden kürzeren Aussagen „Alle Katzen sind Hunde“ und „Die Erde ist eine Scheibe“ zusammengesetzt. Diese beiden Aussagen lassen sich ihrerseits nicht mehr in weitere Aussagen zerlegen, sind aus aussagenlogischer Sicht also elementar oder atomar. Andere, auf die Aussagenlogik aufbauende logische Systeme betrachten die innere Struktur solcher atomaren Aussagen; ein wichtiges Beispiel ist die Prädikatenlogik.
In Abgrenzung zur klassischen Logik entstehen "nichtklassische Logiksysteme", wenn man das Prinzip der Zweiwertigkeit, das Prinzip der Extensionalität oder sogar beide Prinzipien aufhebt. Nichtklassische Logiken, die durch die Aufhebung des Prinzips der Zweiwertigkeit entstehen, heißen mehrwertige Logik. Die Zahl der Wahrheitswerte (in diesem Falle üblicher: Pseudowahrheitswerte) kann dabei endlich sein (z. B. dreiwertige Logik), ist aber oft auch unendlich (z. B. Fuzzy-Logik). Hingegen verwenden Logiken, die durch die Aufhebung der Extensionalität entstehen, Junktoren (Konnektive), bei denen sich der Wahrheitswert des zusammengesetzten Satzes nicht mehr eindeutig aus dem Wahrheitswert seiner Teile bestimmen lässt. Ein Beispiel für nichtextensionale Logik ist die Modallogik, die die einstelligen nichtextensionalen Operatoren „es ist notwendig, dass“ und „es ist möglich, dass“ einführt.
Logische Systeme stehen innerhalb der Logik nicht in einem Konkurrenzverhältnis um Wahrheit oder Richtigkeit. Die Frage, welches logische System für einen bestimmten Zweck genutzt werden soll, ist eher eine pragmatische.
Oft werden logische Systeme und logische Fragestellungen mit außerlogischen Fragen verwechselt oder vermischt, z. B. mit der metaphysischen Frage, welches logische System „richtig“ sei, d. h. die Wirklichkeit beschreibe. Zu dieser Frage gibt es unterschiedliche Standpunkte einschließlich des positivistischen Standpunkts, dass diese Frage sinnlos sei. Diese Fragen fallen aber in andere Gebiete, z. B. Philosophie, Wissenschaftstheorie und Sprachwissenschaft.
Wenn in diesem Artikel die klassische Aussagenlogik behandelt wird, so ist das also nicht als metaphysische Festlegung zu verstehen oder gar als Behauptung, dass „alle Aussagen wahr oder falsch sind“. Es ist lediglich so, dass die klassische Aussagenlogik einfach nur solche Aussagen behandelt, die wahr oder falsch sind. Das ist eine große formale Vereinfachung, die dieses System relativ leicht erlernbar sein lässt. Braucht man aus metaphysischen oder pragmatischen Gründen mehr als zwei Wahrheitswerte, kann die klassische Aussagenlogik als Ausgangspunkt dienen, um ein geeignetes logisches System aufzustellen.
Eine Aussage A ist ein Satz, der entweder wahr (w, wahr, true, 1) oder nicht wahr (f, falsch, false, 0) ist. Das gilt sowohl für einfache als auch für verknüpfte Aussagen. „Halbwahrheiten“ gibt es nicht. Eine Aussage kann sowohl der gewöhnlichen Sprache entstammen als auch der Sprache der Mathematik. Eine Elementaraussage ist eine Aussage, die keine aussagenlogischen Verknüpfungen ("nicht, und, oder, wenn … dann, genau dann wenn") enthält.
Beispiele für Elementaraussagen:
formula_2 ist offensichtlich wahr, formula_4 dagegen ist falsch. formula_1 muss man zunächst prüfen,
bevor man entscheiden kann, ob formula_1 wahr oder falsch ist. Ob formula_3 wahr ist, kann man derzeit nicht entscheiden. Das wird sich erst am Ende der nächsten Fußballsaison
herausstellen.
In der klassischen Aussagenlogik ist eine Aussage entweder wahr oder nicht wahr, auch wenn man den Wahrheitsgehalt nicht kennt. Das ist zum Beispiel bei den ungelösten mathematischen Problemen der Fall.
"Anmerkung:" formula_4 ist eine All-Aussage; die Struktur solcher Aussagen ist Gegenstand der Prädikatenlogik. Im Sinne der Aussagenlogik ist es eine Elementaraussage.
Die "Verneinung" bzw. "Negation" (auch: "Satzverneinung", "äußere Verneinung", "kontradiktorisches Gegenteil") einer Aussage A ist diejenige Aussage ¬A, die genau dann wahr ist, wenn A falsch ist, und die genau dann falsch ist, wenn A wahr ist. Einfacher: Die Verneinung einer Aussage A dreht den Wahrheitswert von A in sein Gegenteil um.
Man erhält die Verneinung einer Aussage A immer dadurch, dass man ihr die Formulierung „Es ist nicht der Fall, dass“ voranstellt. Zwar lässt sich ein natürlichsprachlicher Satz auch verneinen, indem man das Wort „nicht“ oder eine andere negative Formulierung an geeigneter Stelle einfügt – es ist aber nicht immer ganz einfach, zu erkennen, welche Formulierung zu verwenden und an welcher Stelle einzufügen ist. Formal schreibt man für „nicht A“ in der gebräuchlichsten Notation (Schreibweise) ¬A, auf Englisch und in der Schaltalgebra auch „NOT A“, gelegentlich auch „~A“.
Wir verneinen die obigen Beispiele:
Allgemein gilt für die Verneinung:
Eine "Konjunktion" ist eine aus zwei Aussagen zusammengesetzte Aussage, die die Wahrheit all ihrer Teilaussagen behauptet. Umgangssprachlich verbindet man zwei Aussagen A und B durch das Bindewort „und“ zu einer Konjunktion „A und B“, in der logischen Sprache verwendet man meist das Zeichen formula_22 (Schreibweise: formula_23), gelegentlich auch das kaufmännische Und, den Ampersand (&).
Die Aussage formula_23 ist immer dann wahr, wenn sowohl A als auch B jeweils wahr sind.
Andernfalls ist formula_23 falsch, nämlich dann, wenn entweder A oder B oder beide Aussagen falsch sind.
Beispiele für eine "Und"-Verknüpfung:
A: 9 ist durch 3 teilbar
B: 9 ist eine Quadratzahl
Diese Teilaussagen und ihre Negationen werden nun durch formula_22 miteinander verknüpft:
Nur formula_32 ist wahr, weil formula_15 wahr ist und auch formula_34 wahr ist.
formula_35 ist falsch, weil formula_16 falsch ist.
formula_37 ist falsch, weil formula_38 falsch ist.
formula_39 ist falsch, weil sowohl formula_16 als auch formula_38 falsch
ist.
Eine "Disjunktion" ist eine zusammengesetzte Aussage, die behauptet, dass mindestens eine ihrer Teilaussagen wahr ist. Die Disjunktion in diesem Sinn wird auch "nichtausschließendes Oder" genannt. (Aber Achtung: Die Bezeichnung „Disjunktion“ wurde und wird oft auch für das ausschließende Oder, „entweder … oder“, verwendet – man denke an das Konzept der disjunkten Mengen. Einige Autoren verwenden daher für das Nichtausschließende Oder den Begriff "Adjunktion".)
Das Formelzeichen „formula_42“ stammt von dem lateinischen Wort „vel“, was auf deutsch „oder“ bedeutet.
Die Aussage formula_43 ist immer dann wahr, wenn mindestens eine der Teilaussagen A oder B wahr ist, bzw. wenn beide Teilaussagen wahr sind. Andernfalls ist formula_43 falsch, nämlich dann, wenn sowohl A als auch B falsch sind.
Beispiel für eine "Oder"-Verknüpfung:
Diese Teilaussagen und ihre Negationen werden nun durch formula_42 miteinander verknüpft:
formula_51 ist wahr, weil sowohl formula_15 als auch formula_34 wahr sind.
formula_54 ist wahr, weil formula_34 wahr ist.
formula_56 ist wahr, weil formula_15 wahr ist.
Nur formula_58 ist falsch, weil sowohl formula_16 als auch formula_38 falsch sind.
Die "materiale Implikation", auch "Konditional" oder "Subjunktion" genannt, drückt die "hinreichende Bedingung" aus: Sie sagt, dass die Wahrheit des einen Satzes eine hinreichende Bedingung für die Wahrheit des anderen Satzes ist. Man schreibt
oder auch
oder auch nur
In einem Konditional nennt man A das "Antezedens", B das "Konsequens" oder "Sukzedens".
Beispiele:
Die Lesart „wenn … dann“ ist insofern problematisch, als mit dem natürlichsprachlichen „wenn … dann“ vor allem inhaltliche Zusammenhänge wie Kausalität oder zeitliche Nähe ausgedrückt werden. All das macht die materiale Implikation nicht, sie nennt nur den formalen Zusammenhang: „Dass es regnet, ist eine hinreichende Bedingung dafür, dass die Straße nass ist“. Zur Frage, "warum" das eine hinreichende Bedingung ist – ob auf Grund eines kausalen Zusammenhangs oder auch nur rein zufällig –, nimmt die materiale Implikation nicht Stellung.
Als "Umkehrschluss" bezeichnet man den Schluss von formula_61 auf formula_64. Für die Beispiele bedeutet das:
Umgangssprachlich lässt man sich gelegentlich zu weiteren – "falschen" – Aussagen
verleiten:
Das bedeutet: Wenn die Folgerung formula_61 wahr ist, dann erhält man aus der
Aussage ¬A keine Aussage über B; B kann wahr oder falsch sein. („Ex falso sequitur quodlibet“ – „Aus Falschem folgt Beliebiges“)
Die Implikation ist ein wichtiges Mittel in der Mathematik. Die meisten mathematischen Beweise verwenden das Konzept der Implikation.
Das "Bikonditional", oft auch "objektsprachliche Äquivalenz" oder "materiale Äquivalenz" genannt, drückt die "hinreichende und notwendige Bedingung" aus, sagt also, dass eine Aussage A genau dann zutrifft, wenn eine Aussage B zutrifft. Man schreibt:
und liest
Auch beim Bikonditional wird eine rein formale Aussage getroffen, die nichts über einen allfälligen inhaltlichen Zusammenhang von A und B aussagt.
Statt formula_66 zu sagen, kann man auch sagen, dass A eine hinreichende Bedingung für B und dass B eine hinreichende Bedingung für A ist, also formula_68. Tatsächlich sind diese beiden Aussagen logisch äquivalent.
Beispiel:
Das Bikonditional als zusammengesetzte Aussage innerhalb der logischen Sprache (siehe Objektsprache) wird oft mit dem Konzept der logischen Äquivalenz verwechselt oder vermischt. Die logische Äquivalenz ist eine metasprachliche, meist natürlichsprachlich formulierte Eigenschaft zweier Aussagen der logischen Sprache. Ein Zusammenhang zwischen logischer Äquivalenz und Bikonditional besteht nur insofern, als das Metatheorem gilt, dass ein Bikonditional formula_66 genau dann eine Tautologie ist, wenn die beiden Aussagen A und B logisch äquivalent sind.
Das ausschließende Oder (Kontravalenz oder Antivalenz), „entweder A oder B“, besagt, dass genau eine der beiden von ihm verknüpften Aussagen wahr ist. Entsprechend ist ein ausschließendes Oder nicht nur dann falsch, wenn sowohl A als auch B falsch sind, sondern auch, wenn "beide" wahr sind. (Einige Autoren verwenden für das Ausschließende Oder den Begriff "Alternative".)
Obwohl das ausschließende Oder ein Konzept ist, mit dem man in der natürlichen Sprache immer wieder zu tun hat, wird es in den meisten logischen Sprachen nicht als eigenständiger Junktor eingeführt. Stattdessen wird das ausschließende Oder zum Beispiel als verneintes Bikonditional ausgedrückt, also als formula_70.
Große Bedeutung genießt das ausschließende Oder hingegen in der Schaltalgebra, wo es meist als XOR "(eXclusive OR)" aufgeschrieben wird.
Die Verneinung der Konjunktion „A und B“ (in der logischen Schreibweise: formula_23) lautet „Es ist nicht der Fall, dass A und B zutreffen“ (in der logischen Schreibweise: formula_72).
Diese ist logisch äquivalent mit der Aussage
„A ist nicht der Fall, oder B ist nicht der Fall (oder beides)“ (in logischer Schreibweise: formula_73).
Ein Beispiel:
Wenn man die Aussage
verneinen möchte, dann kann man entweder sagen
oder man sagt
In der Schaltalgebra wird sehr oft der Junktor NAND verwendet, wobei „A NAND B“ denselben Wahrheitswertverlauf hat wie der Ausdruck formula_72.
Die Verneinung der Disjunktion „A oder B (oder beides)“ (in der logischen Schreibweise: formula_75) lautet „Es ist nicht der Fall, dass A oder B zutrifft“ (in logischer Schreibweise: formula_76).
Diese ist logisch äquivalent mit der Aussage
„A ist nicht der Fall, und B ist nicht der Fall“ (in logischer Schreibweise: formula_77).
Ein Beispiel:
Wenn man die Aussage
verneinen möchte, so sagt man
Nach dem Gesetz von De Morgan kann man nun aber auch sagen:
oder in schönerem Deutsch
In der Schaltalgebra wird das Konnektiv NOR verwendet, das denselben Wahrheitswertverlauf hat wie die Aussage formula_76.
Dieser Abschnitt soll den zunächst oft als kontraintuitiv empfundenen Zusammenhang zwischen hinreichender und notwendiger Bedingung, wie er im Abschnitt über die materiale Implikation angesprochen wurde, wiederaufgreifen und näher ausführen.
Betrachten wir einmal mehr die materiale Implikation formula_79.
Man sagt: A ist "hinreichend" für B: Schon wenn A der Fall ist, ist auch B der Fall.
Umgekehrt kann man aber auch sagen: B ist "notwendig" für A. Ohne B kann A nicht erfüllt sein.
Wie kommt dieser Zusammenhang zustande?
Wir wissen, dass die Wahrheit von A die Wahrheit von B nach sich zieht, denn A ist ja hinreichende Bedingung für B. Somit ist es einfach nicht möglich, dass A eintritt, ohne dass B damit ebenfalls eintreten würde: B ist also gezwungenermaßen der Fall, wenn A der Fall ist. B ist „notwendig“ für A.
Dieser Zusammenhang ist in Wahrheit also ziemlich einfach; Hauptgrund dafür, dass er anfangs oft als kontraintuitiv empfunden wird, ist wahrscheinlich die Schwierigkeit, zwischen den vielen Bedeutungen des umgangssprachlichen „wenn … dann“ einerseits und der rein formalen hinreichenden und notwendigen Bedingung andererseits strikt zu trennen.
Mit dem umgangssprachlichen „wenn … dann“ möchte man fast immer einen inhaltlichen (kausalen und/oder temporalen) Zusammenhang zwischen Antecedens und Konsequens ausdrücken: „Regen verursacht Straßennässe“, „Zuerst fällt der Regen, erst nachher wird die Straße nass“. Wenn man die hinreichende Bedingung in diesem Sinn missversteht, dann ist es klar, dass die in umgekehrter Reihenfolge formulierte notwendige Bedingung „Nur wenn die Straße nass ist, regnet es“ seltsam aussieht: „Regen verursacht doch Straßennässe, wie kann daraus je gefolgert werden, dass Straßennässe Regen verursacht?“
All dies sagt die materiale Implikation aber nicht aus. „A ist eine hinreichende Bedingung für B“ meint schlicht, dass wenn die Aussage A wahr ist, auch die Aussage B wahr ist – zeitlos und zusammenhanglos, nicht etwa „später“ oder „weil“.
Analog sagt die notwendige Bedingung, „B ist eine notwendige Bedingung für A“, lediglich das aus, dass B wahr ist, sofern A es ist. Genau das ist aber die Definition des Konditionals A → B.
Spätestens beim "lauten" Lesen von Sätzen wie:
wird der selbstbewusste Laie verlangen, dass ihm erklärt wird, was das soll.
Die Antwort des Logikers: Es soll versucht werden, Sicherheit in die Regeln des logischen Schließens zu bringen. Seit den Sophisten ist dem Abendland klar, dass scheinbar zwingende Schlüsse zu offensichtlich absurden Ergebnissen führen können. Immer wieder wurden Paradoxien formuliert und von großen Denkern als Herausforderung empfunden. Logiker versuchen deshalb, die Regeln des Argumentierens so streng wie möglich zu fassen.
Das einleitende Beispiel macht klar, dass dazu eine "Trennung der Sprachebenen" unerlässlich ist: Die formale Aussage A∧B soll dadurch erklärt werden, dass auf einer metasprachlichen Ebene über die Aussage A wie auch über die Aussage B geredet wird.
"Ein" Versuch dies durchzuführen, besteht darin, die Aussagenlogik als formales System, konkret als Kalkül (eine bestimmte Art eines formalen Systems) zu definieren. Die Begriffe „wahr“ und „falsch“ kommen in diesem System zunächst überhaupt nicht vor. Stattdessen werden Axiome gesetzt, die einfach als Zeichenketten angesehen werden, aus denen weitere ableitbare Zeichenketten aufgrund von bestimmten Schlussregeln hergeleitet werden.
Natürlich ist das Ziel dabei, dass in einem formalen System nur Zeichenketten (Sätze) hergeleitet werden können, die bei einer plausiblen Interpretation auch wahr sind. Andererseits sollen alle Sätze, die als „wahr“ interpretierbar sind, auch hergeleitet werden können. Das erste ist die Forderung nach "Korrektheit", das zweite die nach "Vollständigkeit" des formalen Systems; beide Eigenschaften sind unter Kalkül: Der Begriff Kalkül in der Logik beschrieben.
Für die klassische Aussagenlogik, mit der wir es hier zu tun haben, gibt es Kalküle (formale Systeme), die sowohl korrekt als auch vollständig sind. Für komplexere logische Systeme (z. B. Mengenlehre) ist es aber "unmöglich", einen vollständigen Kalkül aufzustellen, der auch korrekt ist – diese Erkenntnis wurde 1931 von Kurt Gödel bewiesen (Gödelscher Unvollständigkeitssatz).
Es gibt viele verschiedene Möglichkeiten, die Syntax („Grammatik“) einer logischen Sprache formal zu definieren; meist geschieht das im Rahmen eines Kalküls. Die folgende Definition ist daher nur als Beispiel dafür zu verstehen, wie ein Kalkül für die klassische Aussagenlogik aussehen kann. Weitere Beispiele für konkrete Kalküle finden sich unter Baumkalkül, Begriffsschrift, Systeme natürlichen Schließens, Sequenzenkalkül oder Resolutionskalkül. Ein weiterer axiomatischer Kalkül ist als Beispiel im Artikel Hilbert-Kalkül angegeben, ein graphischer Kalkül im Artikel Existential Graphs.
Als "Bausteine" der aussagenlogischen Sprache sollen "Satzbuchstaben" („atomare Formeln“, Satzkonstanten), "Junktoren" und "Gliederungszeichen" verwendet werden. Satzbuchstaben sollen die Zeichen P, P, P, … sein. Junktoren sollen die Zeichen ¬, ∧, ∨, → und ↔ sein. Als Gliederungszeichen sollen die runden Klammern dienen.
Formal lässt sich das z. B. auf folgende Weise ausdrücken:
Sei V die (abzählbar unendliche) Menge der "atomaren Formeln" (Satzbuchstaben):
Sei J die Menge der Junktoren und Gliederungszeichen:
Das "Alphabet" der logischen Sprache sei die Menge V ∪ J, also die Vereinigungsmenge von atomaren Formeln, Junktoren und Gliederungszeichen.
Die "Formationsregeln" legen fest, wie man aus den Bausteinen der aussagenlogischen Sprache Sätze (Formeln) bilden kann.
Hier sollen "aussagenlogische Formeln" als Worte über dem Alphabet der logischen Sprache, also über V ∪ J wie folgt induktiv definiert werden:
"Schlussregeln" sind allgemein Transformationsregeln (Umformungsregeln), die auf bestehende Formeln angewandt werden und aus ihnen neue Formeln erzeugen. Wenn man einen Kalkül für ein logisches System aufstellt, dann wählt man die Transformationsregeln so, dass sie aus bestehenden Formeln solche Formeln erzeugen, die aus den Ausgangsformeln semantisch "folgen" – deshalb die Bezeichnung „Schlussregel“ ("eine Schlussfolgerung ziehen").
Innerhalb der Syntax sind die Schlussregeln allerdings rein formale Transformationsregeln, denen für sich keinerlei inhaltliche Bedeutung zukommt.
An konkreten Schlussregeln sollen hier nur zwei angegeben werden: Der Modus ponendo ponens und die Substitutionsregel.
"Axiome" sind ausgezeichnete (im Sinn von: hervorgehobene) Formeln der aussagenlogischen Sprache. Die Auszeichnung besteht darin, dass sie innerhalb eines Beweises oder einer Herleitung (siehe unten) ohne weitere Rechtfertigung verwendet werden.
Pragmatisch wählt man solche Formeln als Axiome, die semantisch gesehen Tautologien sind, also immer zutreffen, und die dabei helfen, Beweise zu verkürzen. Innerhalb der Syntax sind die Axiome allerdings rein formale Objekte, denen keinerlei inhaltliche Bedeutung oder Rechtfertigung zukommt.
Axiome sind im Allgemeinen optional, d. h. ein Kalkül kann auch ganz ohne Axiome auskommen, wenn er ausreichend viele bzw. mächtige Schlussregeln hat. Axiomfreie Kalküle sind zum Beispiel die Systeme natürlichen Schließens oder Baumkalküle.
Hier soll exemplarisch ein axiomatischer Kalkül gezeigt werden, und zwar jener, den Whitehead und Russell in ihren 1910–1913 entstandenen Principia Mathematica vorstellten. Der Principia Mathematica-Kalkül für die Aussagenlogik umfasst die folgenden Axiome (von denen das vierte redundant, d. h. nicht unbedingt erforderlich, weil aus den anderen Axiomen herleitbar ist):
Um aus diesen Axiomen auch solche gültigen Sätze herleiten zu können, die andere als die in den Axiomen vorkommende Junktoren enthalten, werden diese durch folgende Festlegung auf die vorhandenen Junktoren zurückgeführt:
Alternativ zu – wie hier – konkreten Axiomen kann man auch "Axiomenschemata" angeben, in welchem Fall man auch ohne Substitutionsregel auskommt. Interpretiert man die obigen Axiome als Axiomenschemata, dann stünde z. B. das erste Axiomenschema, formula_91, für unendlich viele Axiome, nämlich alle Ersetzungsinstanzen dieses Schemas.
Eine Herleitung ist eine Liste von aufsteigend nummerierten Sätzen, die mit einer oder mehreren Annahmen (den Prämissen der Herleitung) oder Axiomen beginnt. Alle auf diese folgenden Sätze sind entweder ebenfalls Axiome (bei manchen Kalkülen sind auch weitere Annahmen zulässig) oder sind aus einer oder mehreren der vorangehenden Zeilen durch Anwendung von Schlussregeln entstanden. Der letzte Satz in der Liste ist die Konklusion der Herleitung.
Eine Herleitung ohne Prämissen heißt "Beweis". Oft werden aber die Wörter „Herleitung“ und „Beweis“ synonym gebraucht.
Wenn es gelingt, aus einer Menge von Annahmen (Prämissen) Δ eine Konklusion P herzuleiten, dann schreibt man auch formula_99.
Gelingt es, einen Satz P ohne die Verwendung von Annahmen herzuleiten (zu beweisen), dann schreibt man auch: formula_100. In diesem Fall wird P "Theorem" genannt.
Das Zeichen formula_101 geht auf die Begriffsschrift zurück, jenes Werk, in dem Gottlob Frege 1879 die erste Formalisierung der Prädikatenlogik angegeben hat.
In der klassischen Aussagenlogik wählt man die Schlussregeln so, dass sich mit ihrer Hilfe "alle" gültigen Argumente (und "nur" gültige Argumente) herleiten lassen; die Frage der Gültigkeit wird im folgenden Abschnitt, „Semantik“, behandelt.
Außerhalb der Logik bezeichnet Semantik ein Forschungsgebiet, das sich mit der Bedeutung von Sprache und deren Teilen befasst. Oft wird auch das Wort "Semantik" gleichbedeutend mit dem Wort "Bedeutung" verwendet.
Auch innerhalb der Logik geht es bei Semantik um Bedeutung: Darum nämlich, den Ausdrücken einer formalen Sprache – zum Beispiel der hier behandelten Sprache der Aussagenlogik – eine Bedeutung zuzuordnen. In der Logik wird auch das meist sehr formal unternommen.
Im Zentrum der (formalen) Semantik steht eine Auswertungsfunktion (andere Bezeichnungen lauten Bewertungsfunktion, Denotationsfunktion, Wahrheitswertefunktion), die den Formeln der logischen Sprache eine Bedeutung zuordnet. Formal gesprochen ist die Auswertungsfunktion eine Abbildung von der Menge der Formeln der Sprache in die Menge der Wahrheitswerte. Oft wird die Auswertungsfunktion mit dem Großbuchstaben V bezeichnet.
In der klassischen Aussagenlogik ist die Auswertungsfunktion sehr einfach: Das Prinzip der Zweiwertigkeit fordert, dass sie für jede zu bewertende Formel genau einen von genau zwei Wahrheitswerten liefern muss; und das Prinzip der Extensionalität fordert, dass die Bewertungsfunktion beim Bewerten eines komplexen Satzes nur die Bewertung von dessen Teilsätzen berücksichtigen muss.
Jedem Atom, also jedem Satzbuchstaben (Atom) wird durch Festsetzung ein Wahrheitswert zugeordnet. Man sagt: Die Atome werden interpretiert. Es wird also z. B. festgelegt dass P wahr ist, dass P falsch ist und dass P ebenfalls falsch ist. Damit ist der Bewertung der Bausteine der logischen Sprache Genüge getan. Formal ist eine solche Bewertung – "Interpretation" genannt und oft mit dem Kleinbuchstaben v bezeichnet – eine Funktion im mathematischen Sinn, d. h. eine Abbildung von der Menge der Atome in die Menge der Wahrheitswerte.
Wenn die Auswertungsfunktion V auf ein Atom angewandt wird, d. h. wenn sie ein Atom bewerten soll, liefert sie die Interpretation dieses Atoms im Sinn des obigen Absatzes. Mit anderen Worten, sie liefert den Wert, den die Bewertung v dem Atom zuordnet.
Um die zusammengesetzten Formeln bewerten zu können, muss für jeden Junktor definiert werden, welchen Wahrheitswert die Bewertungsfunktion für die unterschiedlichen Wahrheitswertkombinationen liefert, den seine Argumente annehmen können. In der klassischen Aussagenlogik geschieht das meist mittels Wahrheitstabellen, weil es nur überschaubar wenige Möglichkeiten gibt.
Der einstellige Junktor ¬, die Negation, ist in der klassischen Aussagenlogik so definiert, dass er den Wahrheitswert seines Arguments ins Gegenteil umkehrt, also „verneint“: Ist die Bewertung einer Formel X wahr, dann liefert die Bewertungsfunktion für ¬X falsch; wird aber X falsch bewertet, dann liefert die Bewertungsfunktion für ¬X wahr. Die Wahrheitstabelle sieht folgendermaßen aus:
Die Wahrheitswertverläufe der verwendeten zweistelligen Konnektive sind in der klassischen Aussagenlogik wie folgt definiert:
Allgemein gibt es für die klassische Aussagenlogik vier einstellige und sechzehn zweistellige Junktoren. Die hier behandelte logische Sprache beschränkt sich nur deshalb auf die Junktoren ¬, ∧, ∨, → und ↔, weil diese am gebräuchlichsten sind und weil sie auch inhaltlich noch am ehesten aus der Alltagssprache bekannt sind. Aus formaler Sicht ist die einzige Bedingung, die man bei der Wahl von Junktoren erfüllen möchte, die, dass sich mit den gewählten Junktoren auch alle anderen theoretisch möglichen Junktoren ausdrücken lassen; man sagt: Dass die Menge der gewählten Junktoren funktional vollständig ist. Diese Anforderung ist bei der hier getroffenen Wahl erfüllt.
Näheres zur Frage, wie viele und welche Junktoren es gibt und wie viele Junktoren man benötigt, um funktionale Vollständigkeit zu erreichen, ist im Kapitel Junktor beschrieben.
"Semantische Gültigkeit" ist eine Eigenschaft von Formeln oder von Argumenten. (Ein Argument ist die Behauptung, dass aus einigen Aussagen – den Prämissen – eine bestimmte Aussage – die Konklusion – folgt.)
Eine "Formel" der aussagenlogischen Sprache heißt genau dann semantisch gültig, wenn die Formel unter allen Interpretationen – d. h. unter allen Zuordnungen von Wahrheitswerten zu den in ihr vorkommenden Atomen – wahr ist; wenn sie sozusagen allgemeingültig ist; mit anderen Worten: Wenn die Wahrheitstabelle für diese Aussage in jeder Zeile das Ergebnis "wahr" zeigt. Man nennt semantisch gültige Formeln auch Tautologien und schreibt, wenn formula_102 eine Tautologie ist, formal wie folgt:
Ein "Argument" heißt genau dann semantisch gültig, wenn unter der Voraussetzung, dass alle Prämissen wahr sind, auch die Konklusion wahr ist. In der Formulierung von Gottfried Wilhelm Leibniz: "Aus Wahrem folgt nur Wahres." Diese Definition muss natürlich ebenfalls formal gefasst werden, und das geschieht wie folgt: Ein Argument ist genau dann semantisch gültig, wenn alle Zuordnungen von Wahrheitswerten zu den in Prämissen und Konklusion vorkommenden Atomen, unter denen die Bewertungsfunktion für alle Prämissen den Wert "wahr" liefert, auch für die Konklusion den Wert "wahr" liefert.
Um auszudrücken, dass aus einer Menge formula_104 von Formeln (der Prämissenmenge) eine Formel formula_102 (die Konklusion) semantisch folgt, schreibt man formal wie folgt:
Beachte die graphische Ähnlichkeit und die inhaltliche Verschiedenheit zwischen formula_99 (Kapitel „Herleitung und Beweis“) und formula_106 ("Siehe:" Semantische Folgerung): Die erste Formulierung – formula_99 – drückt die "syntaktische" Gültigkeit des Arguments aus, sagt also, dass aus den Formeln in formula_104 mit den Schlussregeln des gewählten Kalküls die Formel formula_102 "hergeleitet" werden kann. formula_106 hingegen behauptet die "semantische" Gültigkeit, die in der klassischen Aussagenlogik wie in den vorangegangenen Absätzen als das Leibniz’sche "Aus Wahrem folgt nur Wahres" definiert ist.
Neben der Eigenschaft der Gültigkeit (Allgemeingültigkeit) gibt es einige andere wichtige Eigenschaften: Erfüllbarkeit, Widerlegbarkeit und Unerfüllbarkeit. Im Gegensatz zur Gültigkeit, die Eigenschaft von Formeln oder von Argumenten sein kann, sind Erfüllbarkeit, Widerlegbarkeit und Unerfüllbarkeit Eigenschaften von Sätzen oder von Satzmengen.
Die Frage, ob eine Formel (oder eine Formelmenge) eine der genannten Eigenschaften hat, ist ebenso wie die Frage, ob eine Formel allgemeingültig, d. h. eine Tautologie ist, für allgemeine Formeln nicht effizient lösbar: Zwar ist die Wahrheitstafel ein Entscheidungsverfahren für jede dieser Fragen, doch umfasst eine Wahrheitstafel für eine Aussage bzw. eine Aussagemenge in n Atomen formula_113 Zeilen; das Wahrheitstafelverfahren ist nichts anderes als ein Brute-Force-Verfahren.
Jede dieser Fragestellungen kann auf die Frage zurückgeführt werden, ob eine bestimmte Formel erfüllbar ist:
Die Frage, ob eine Aussage erfüllbar ist, wird Erfüllbarkeitsproblem oder "SAT-Problem" (nach dem englischen Wort für Erfüllbarkeit, "satisfiability") genannt. Das SAT-Problem spielt eine wichtige Rolle in der theoretischen Informatik und Komplexitätstheorie. Das Erfüllbarkeitsproblem für allgemeine (beliebige) Formeln ist NP-vollständig, d. h. (unter der Voraussetzung, dass P ungleich NP) nicht in polynomialer Laufzeit lösbar.
Für bestimmte echte Teilmengen der Formeln der aussagenlogischen Sprache ist das SAT-Problem dennoch schneller, d. h. in polynomial beschränkter Rechenzeit lösbar. Eine solche Teilmenge sind die Horn-Formeln, das sind Konjunktionen von Disjunktionen, deren Disjunkte verneinte oder unverneinte Atome sind, wobei innerhalb einer solchen Disjunktion allerdings höchstens ein Atom unverneint sein darf.
Wenn man die Semantik betrachtet, die hier für die klassische Aussagenlogik aufgestellt wurde, dann erkennt man gewisse Gesetzmäßigkeiten. Wird z. B. die Auswertungsfunktion auf eine Aussage der Form X ∧ W angewendet, wobei W eine beliebige wahre Aussage sein soll, dann stellt man fest, dass die Auswertungsfunktion für X ∧ W immer den Wahrheitswert "wahr" liefert, wenn V(X)=wahr ist (das heißt V(X∧W)=V(X)). Von der Struktur her gleichwertige Gesetzmäßigkeiten gelten auch in anderen Semantiken, auch in solchen, die für ganz andere, nichtlogische Systeme aufgestellt werden. Für die Arithmetik gilt z. B., dass die dortige Bewertungsfunktion (hier V genannt) für einen Ausdruck der Form X + Y immer den Wert von X liefert, sofern der Wert von Y null ist: V(X+Y)=V(X), wenn V(Y) = null ist.
Eine formale Wissenschaft, die solche strukturellen Gesetzmäßigkeiten untersucht, ist die abstrakte Algebra (meist Teilgebiet der Mathematik, aber auch der Informatik). In der abstrakten Algebra wird zum Beispiel untersucht, für welche Verknüpfungen es ein neutrales Element gibt, d. h. ein Element "N", das für eine Verknüpfung "op" dazu führt, dass (für beliebiges X) gilt: X "op" "N" = X. So würde man aus algebraischer Sicht sagen, dass es für die klassische aussagenlogische Konjunktion genau ein neutrales Element gibt, nämlich "wahr", und dass es für die Addition in der Arithmetik ebenfalls genau ein neutrales Element gibt, nämlich die Zahl Null. Nur am Rande sei erwähnt, dass es auch für andere Junktoren neutrale Elemente gibt; das neutrale Element für die Disjunktion ist "falsch": V(X ∨ F) = V(X), wenn V(F)=falsch ist.
Die formale Algebra betrachtet formale Semantiken rein nach ihren strukturellen Eigenschaften. Sind diese identisch, dann besteht zwischen ihnen aus algebraischer Sicht kein Unterschied. Aus algebraischer Sicht, genauer: Aus Sicht der formalen Algebra ist die Semantik für die klassische Aussagenlogik eine zweiwertige Boolesche Algebra. Andere formale Systeme, deren Semantiken jeweils eine Boolesche Algebra bilden, sind die Schaltalgebra und die elementare Mengenlehre. Aus algebraischer Sicht besteht daher zwischen diesen Disziplinen kein Unterschied.
Jede aussagenlogische Formel lässt sich in eine äquivalente Formel in
konjunktiver Normalform und eine äquivalente Formel in
disjunktiver Normalform umformen.
In der Metatheorie werden die Eigenschaften von logischen Systemen untersucht: Das logische System ist in der Metatheorie der Untersuchungsgegenstand.
Eine metatheoretische Fragestellung ist zum Beispiel die, ob in einem Kalkül ein Widerspruch hergeleitet werden kann.
Der vorliegende Abschnitt soll einige wichtige metatheoretische Fragestellungen aus dem Blickwinkel der Aussagenlogik betrachten.
Ein metatheoretisches Resultat ist zum Beispiel die Feststellung, dass alle korrekten Kalküle auch konsistent sind. Ein anderes metatheoretisches Resultat ist die Feststellung, dass ein konsistenter Kalkül nicht automatisch korrekt sein muss: Es ist ohne weiteres möglich, einen Kalkül aufzustellen, in dem zwar kein Widerspruch hergeleitet werden kann, in dem aber z. B. die nicht allgemeingültige Aussage der Form „A ∨ B“ hergeleitet werden kann. Ein solcher Kalkül wäre aus ersterem Grund konsistent, aus letzterem Grund aber nicht korrekt.
Ein weiteres, sehr einfaches Resultat ist die Feststellung, dass ein vollständiger Kalkül nicht automatisch auch korrekt oder nur konsistent sein muss. Das einfachste Beispiel wäre ein Kalkül, in dem "jede" Formel der aussagenlogischen Sprache herleitbar ist. Da jede Formel herleitbar ist, sind alle Tautologien herleitbar, die ja Formeln sind: Das macht den Kalkül vollständig. Da aber jede Formel herleitbar ist, ist insbesondere auch die Formel P ∧ ¬ P und die Formel A ∨ B herleitbar: Ersteres macht den Kalkül inkonsistent, letzteres inkorrekt.
Das Ideal, das ein Kalkül erfüllen sollte, ist Korrektheit und Vollständigkeit: Wenn das der Fall ist, dann ist er der ideale Kalkül für ein logisches System, weil er alle semantisch gültigen Sätze (und nur diese) herleiten kann. So sind die beiden Fragen, ob ein konkreter Kalkül korrekt und/oder vollständig ist und ob es für ein bestimmtes logisches System überhaupt möglich ist, einen korrekten und vollständigen Kalkül anzugeben, zwei besonders wichtige metatheoretische Fragestellungen.
Die klassische Aussagenlogik, wie sie hier ausgeführt wurde, ist ein formales logisches System. Als solches ist sie eines unter vielen, die aus formaler Sicht gleichwertig nebeneinander stehen und die ganz bestimmte Eigenschaften haben: Die meisten sind konsistent, die meisten sind korrekt, etliche sind vollständig, und einige sind sogar entscheidbar. Aus formaler Sicht stehen die logischen Systeme in keinem Konkurrenzverhalten hinsichtlich Wahrheit oder Richtigkeit.
Von formalen, innerlogischen Fragen klar unterschieden sind außerlogische Fragen: Solche nach der Nützlichkeit (Anwendbarkeit) einzelner Systeme für einen bestimmten Zweck und solche nach dem philosophischen, speziell metaphysischen Status einzelner Systeme.
Die Nützlichkeitserwägung ist die einfachere, bezüglich deren Meinungsunterschiede weniger tiefgehend bzw. weniger schwerwiegend sind. Klassische Aussagenlogik zum Beispiel bewährt sich in der Beschreibung elektronischer Schaltungen (Schaltalgebra) oder zur Formulierung und Vereinfachung logischer Ausdrücke in Programmiersprachen. Prädikatenlogik wird gerne angewandt, wenn es darum geht, Faktenwissen zu formalisieren und automatisiert Schlüsse daraus zu ziehen, wie das unter anderem im Rahmen der Programmiersprache Prolog geschieht. Fuzzy-Logiken, nonmonotone, mehrwertige und auch parakonsistente Logiken sind hochwillkommen, wenn es darum geht, mit Wissensbeständen umzugehen, in denen Aussagen mit unterschiedlich starkem Gewissheitsgrad oder gar einander widersprechende Aussagen abgelegt werden sollen und dennoch sinnvolle Schlüsse aus dem Gesamtbestand gezogen werden sollen. Auch wenn es je nach Anwendungsfall sehr große Meinungsunterschiede geben kann, welches logisches System besser geeignet ist, ist die Natur des Problems für alle Beteiligten unmittelbar und in gleicher Weise greifbar. Einzelwissenschaftliche Überlegungen und Fragestellungen spielen sich überwiegend in diesem Bereich ab.
(Noch) kontroverser als solche pragmatischen Überlegungen sind Fragestellungen philosophischer und metaphysischer Natur. Geradezu paradigmatisch ist die Frage, „welches logische System richtig ist“, wobei „richtig“ hier gemeint ist als: Welches logische System nicht nur einen Teilaspekt der Wirklichkeit modellhaft vereinfacht, sondern die Wirklichkeit, das Sein als Ganzes adäquat beschreibt. Zu dieser Fragestellung gibt es viele unterschiedliche Meinungen einschließlich der vom philosophischen Positivismus eingeführten Meinung, dass die Fragestellung als Ganzes sinnlos ist.
In den Bereich metaphysischer Fragestellungen fällt auch die Frage, ob es so etwas wie ein "metaphysisches" Prinzip der Zweiwertigkeit gebe, ob also Aussagen über die Wirklichkeit durchgehend ins Schema wahr/falsch passen oder nicht. Diese Frage ist unabhängig von der Frage, ob die Beschäftigung mit zwei- oder mehrwertigen Logiken praktisch sinnvoll ist: Selbst wenn ein metaphysisches Prinzip der Zweiwertigkeit herrscht, könnte man anwendungspraktisch mehrwertige Logiken nützen, etwa dazu, epistemische Sachverhalte zu fassen, zum Beispiel aus Aussagen zu schließen, die zwar metaphysisch wahr oder falsch sind, von denen aber nicht oder noch nicht bekannt ist, welches von beidem der Fall ist. Umgekehrt kann man auch dann, wenn ein solches metaphysisches Prinzip nicht gilt, zweiwertige Logik wegen ihrer Einfachheit für solche Anwendungen bevorzugen, bei denen nur mit solchen Sätzen umgegangen werden muss, die tatsächlich wahr oder falsch sind.
Die Frage nach einem metaphysischen Prinzip der Zweiwertigkeit ist wie die meisten metaphysischen Fragen nicht endgültig zufriedenstellend beantwortet. Ein früher Einwand gegen ein solches Prinzip, den Aristoteles zur Diskussion stellte, war das Thema der Aussagen über zukünftige Sachverhalte („Morgen wird es regnen“). Wenn Aussagen über Zukünftiges schon heute wahr oder falsch wären, so wird argumentiert, dann müsse die Zukunft bis ins letzte Detail vorbestimmt sein. Ein anderer Einwand, der vorgebracht wird, ist, dass es Aussagen gibt, deren Wahrheit praktisch oder theoretisch nicht festgestellt werden kann – zum Beispiel lässt sich die Wahrheit von „Der Rasen vor dem Weißen Haus bestand am 1. Februar 1870 aus genau 6.120.375,4 Grashalmen“ einfach nicht feststellen.
Befürworter eines metaphysischen Zweiwertigkeitsprinzips berufen sich oft auf das Verhalten von Metatheoretikern, also von Mathematikern oder Logikern, die Aussagen "über" formale Systeme treffen: Egal wie mehrwertig oder nichtklassisch das untersuchte System ist, die dabei getroffenen Metavermutungen, Metabehauptungen und Metafeststellungen sind immer zweiwertig: Ein Kalkül, auch ein parakonsistenter oder nonmonotoner, wird immer als "entweder" konsistent "oder" inkonsistent betrachtet, und ein logisches System ist immer "entweder" korrekt oder inkorrekt, vollständig oder nicht vollständig, entscheidbar oder unentscheidbar, niemals „ein bisschen“ von beidem. Befürworter deuten das als Hinweis darauf, dass es in der Wirklichkeit tatsächlich eine strenge Unterscheidung nach wahr und falsch gebe oder dass es zumindest sinnvoll ist, eine solche anzunehmen.
Eine andere philosophische Fragestellung ist die nach dem metaphysischen Status des Untersuchungsgegenstands der Logik, also danach, was logische Systeme, Kalküle, Wahrheitswerte eigentlich „sind“.
Der platonische Standpunkt besteht darin, dass die in der Logik verwendeten Zeichen und Konstrukte eine außerlogische Bedeutung haben, dass sie Namen für real existierende (wenn auch natürlich nicht-physikalische) Gegenstände sind. In diesem Sinn gäbe es so etwas wie "das Wahre" und "das Falsche", abstrakte Gegenstände, die von den Zeichen „wahr“ und „falsch“ benannt werden.
Der Gegenpol zum Platonismus wäre der Nominalismus, der Existenz nur den Zeichen zuspricht, die in der Logik manipuliert werden. Gegenstand der Logik sind Zeichen, und die Tätigkeit der Logiker ist die Manipulation von Zeichen. Die Zeichen bezeichnen aber nichts, so etwas wie das Wahre oder das Falsche gibt es also nicht. Im Grundlagenstreit der Mathematik entspräche der nominalistischen Position die formalistische Richtung.
Eine Mittelstellung nähme der philosophische Konstruktivismus ein, demzufolge die Zeichen zwar keine unabhängig existierenden Gegenstände bezeichnen, durch den Umgang mit den Zeichen aber Gegenstände konstruiert werden.

</doc>
<doc id="13" url="https://de.wikipedia.org/wiki?curid=13" title="Liste von Autoren/A">
Liste von Autoren/A


</doc>
<doc id="14" url="https://de.wikipedia.org/wiki?curid=14" title="Liste von Autoren/H">
Liste von Autoren/H


</doc>
<doc id="15" url="https://de.wikipedia.org/wiki?curid=15" title="Liste von Autoren/C">
Liste von Autoren/C


</doc>
<doc id="16" url="https://de.wikipedia.org/wiki?curid=16" title="Liste von Autoren/I">
Liste von Autoren/I


</doc>
<doc id="17" url="https://de.wikipedia.org/wiki?curid=17" title="Liste von Autoren/K">
Liste von Autoren/K


</doc>
<doc id="18" url="https://de.wikipedia.org/wiki?curid=18" title="Liste von Autoren/J">
Liste von Autoren/J


</doc>
<doc id="19" url="https://de.wikipedia.org/wiki?curid=19" title="Liste von Autoren/V">
Liste von Autoren/V


</doc>
<doc id="20" url="https://de.wikipedia.org/wiki?curid=20" title="Liste von Autoren/G">
Liste von Autoren/G


</doc>
<doc id="21" url="https://de.wikipedia.org/wiki?curid=21" title="Liste von Autoren/W">
Liste von Autoren/W


</doc>
<doc id="22" url="https://de.wikipedia.org/wiki?curid=22" title="Liste von Autoren/B">
Liste von Autoren/B


</doc>
<doc id="23" url="https://de.wikipedia.org/wiki?curid=23" title="Liste von Autoren/D">
Liste von Autoren/D


</doc>
<doc id="24" url="https://de.wikipedia.org/wiki?curid=24" title="Liste von Autoren/S">
Liste von Autoren/S


</doc>
<doc id="25" url="https://de.wikipedia.org/wiki?curid=25" title="Liste von Autoren/T">
Liste von Autoren/T


</doc>
<doc id="26" url="https://de.wikipedia.org/wiki?curid=26" title="Liste von Autoren/M">
Liste von Autoren/M


</doc>
<doc id="27" url="https://de.wikipedia.org/wiki?curid=27" title="Liste von Autoren/O">
Liste von Autoren/O


</doc>
<doc id="28" url="https://de.wikipedia.org/wiki?curid=28" title="Liste von Autoren/F">
Liste von Autoren/F


</doc>
<doc id="29" url="https://de.wikipedia.org/wiki?curid=29" title="Liste von Autoren/E">
Liste von Autoren/E


</doc>
<doc id="30" url="https://de.wikipedia.org/wiki?curid=30" title="Liste von Autoren/L">
Liste von Autoren/L


</doc>
<doc id="31" url="https://de.wikipedia.org/wiki?curid=31" title="Liste von Autoren/N">
Liste von Autoren/N


</doc>
<doc id="32" url="https://de.wikipedia.org/wiki?curid=32" title="Liste von Autoren/P">
Liste von Autoren/P


</doc>
<doc id="33" url="https://de.wikipedia.org/wiki?curid=33" title="Liste von Autoren/Q">
Liste von Autoren/Q


</doc>
<doc id="34" url="https://de.wikipedia.org/wiki?curid=34" title="Liste von Autoren/R">
Liste von Autoren/R


</doc>
<doc id="35" url="https://de.wikipedia.org/wiki?curid=35" title="Liste von Autoren/U">
Liste von Autoren/U


</doc>
<doc id="36" url="https://de.wikipedia.org/wiki?curid=36" title="Liste von Autoren/Y">
Liste von Autoren/Y


</doc>
<doc id="37" url="https://de.wikipedia.org/wiki?curid=37" title="Liste von Autoren/Z">
Liste von Autoren/Z


</doc>
<doc id="38" url="https://de.wikipedia.org/wiki?curid=38" title="Anthony Minghella">
Anthony Minghella

Anthony Minghella, CBE (* 6. Januar 1954 auf der Isle of Wight, Großbritannien; † 18. März 2008 in London) war ein britischer Filmregisseur, Filmproduzent, Drehbuchautor, Dramatiker, Hörspiel-Autor, Theater- und Opern-Regisseur.
Minghella war der Sohn italienisch-schottischer Eltern, die auf der Isle of Wight eine Fabrik für Eiscreme betrieben. Nach seinem Schulabschluss studierte er an der Universität Hull, wo er eine Zeit lang als Dozent tätig war. 1978 drehte er einen ersten Kurzfilm. Seit 1981 war er als Autor und Story Editor tätig. Er wurde mit Theaterstücken, Rundfunkhörspielen, der Fernsehserie "Inspector Morse" und vielen Drehbüchern für Film und Fernsehen bekannt. Er entwickelte die Drehbücher für die 1988 erfolgreich ausgestrahlte Fernsehserie The Storyteller von Muppets-Erfinder Jim Henson.
Auch als Produzent war er erfolgreich, darunter für die Filme "Der stille Amerikaner", "Die Dolmetscherin" und "Der Vorleser", für den er 2008 posthum für den Oscar (Kategorie „Bester Film“) nominiert wurde. Gemeinsam mit seinem Freund und Kollegen Sydney Pollack gründete er die Produktionsfirma Mirage Enterprises. Der Regisseur Minghella galt als ein guter Schauspielerführer: Unter seiner Regie brachten es zahlreiche Darsteller zu Oscar-Nominierungen, zwei Schauspielerinnen erhielten die Auszeichnung als „Beste Nebendarstellerin“: Juliette Binoche ("Der englische Patient") und Renée Zellweger ("Unterwegs nach Cold Mountain").
Gegen Ende seines Lebens kehrte Minghella zu seinen Anfängen im Radio und auf der Bühne zurück: 2006 wurde sein Hörspiel "Eyes Down Looking" mit Jude Law zu Ehren von Samuel Beckett auf BBC Radio 3 ausgestrahlt, ein Jahr zuvor hatte seine Inszenierung der Puccini-Oper Madame Butterfly in der English National Opera in London Premiere und wurde auch in der Nationaloper von Vilnius und in der Metropolitan Opera in New York gezeigt. Am Ende des Films "Abbitte" von Joe Wright (2007) hat er einen Kurzauftritt als Talkshow-Moderator neben Vanessa Redgrave. Seine letzte Arbeit als Drehbuchautor war das Skript für den Musical-Film "Nine" (gemeinsam mit Michael Tolkin). Zu seinen letzten Regiearbeiten zählt der Pilotfilm zur Krimiserie "Eine Detektivin für Botswana" (Originaltitel: ), den die BBC fünf Tage nach seinem Tod erstmals ausstrahlte.
Minghella war mit der aus Hongkong stammenden Choreographin, Produzentin und Schauspielerin Carolyn Choa ("Wie verrückt und aus tiefstem Herzen") verheiratet. Der Ehe entstammen zwei Kinder, die in der Filmbranche tätig sind: Tochter Hannah Minghella in der Produktion und Sohn Max Minghella als Schauspieler ("Agora – Die Säulen des Himmels"). Die Tante Edana Minghella und der Onkel Dominic Minghella (u.a. für die deutsche Fernsehserie "Doktor Martin") sind Drehbuchautoren.
Minghella starb im Alter von 54 Jahren in einem Londoner Krankenhaus an inneren Blutungen infolge der Operation eines Tonsillenkarzinoms und eines Karzinoms im Nacken.
1984 erhielt Minghella den Londoner Kritikerpreis als meistversprechender junger Dramatiker, 1986 den Kritikerpreis für sein Stück "Made in Bangkok" als bestes Stück der Saison. 1997 erhielt er für "Der englische Patient" den Oscar in der Rubrik "Beste Regie", 1999 eine Oscar-Nominierung in der Kategorie „Bestes adaptiertes Drehbuch“ für "Der talentierte Mr. Ripley", bei dem er auch Regie führte.
2001 wurde Minghella zum Commander of the British Empire (CBE) ernannt. Von 2003 bis 2007 war er Präsident des British Film Institute. Seit 1997 trägt das Anthony Minghella Theatre auf der Isle of Wight seinen Namen.

</doc>
<doc id="39" url="https://de.wikipedia.org/wiki?curid=39" title="US-amerikanischer Film">
US-amerikanischer Film

Die Geschichte des US-amerikanischen Films ist ein Kapitel der Filmgeschichte, das gerade wegen der hervorgehobenen Stellung der Vereinigten Staaten als Filmnation sowohl für die Filmkunst als auch für die Ökonomie des Films relevant ist. Weltbekannt ist die Filmstadt Hollywood als Zentrum der US-amerikanischen Filmindustrie, weshalb der Name oft auch als Synonym für die gesamte amerikanische Film-Branche steht. Synonym für Hollywoods Filmindustrie wird wiederum der Begriff "Traumfabrik" ( "Dreamfactory") verwendet.
Bis 1912 konzentrierten sich die US-amerikanischen Filmunternehmen auf den inneramerikanischen Filmwettbewerb. Erst danach stieg ihr Einfluss auf dem Weltmarkt. Und zwar so rapide, dass sie bereits 1914, zu Beginn des Ersten Weltkriegs, die Hälfte der Welt-Filmproduktion stellten.
Der harte Wettkampf zwischen dem Edison Trust und den von Carl Laemmle angeführten „Independents“ hatte wirksame Instrumente geschaffen, die, am nationalen Konkurrenten erprobt und verfeinert, nun mit zunehmender Härte die internationalen Mitbewerber trafen. Dennoch war die Vormachtstellung Hollywoods längst nicht unangreifbar, erst eine politische Entwicklung verschaffte ihr die nötige Ruhe zur Restrukturierung: Der Krieg in Europa.
Die französische Filmproduktion, Hauptkonkurrent der US-Amerikaner, kam mit dem Ausbruch des Krieges sofort und vollständig zum Erliegen, denn Pathé wandelte seine Rohfilm-Fabrik in eine Munitionsfabrik um und seine Studios in Kasernen. Ähnlich, und doch weniger extrem, brach die italienische Produktion beim Kriegseintritt des Landes 1916 ein.
Nachdem absehbar war, dass der Krieg sehr lange dauern konnte, bemühten sich die Franzosen, wieder ins Geschäft zu kommen. Die Position, die sie vor Ausbruch des Krieges innehatten, erreichten sie nicht mehr. Zudem beschloss das Deutsche Reich 1916 das generelle Filmeinfuhrverbot, was die europäischen Filmnationen ihres wichtigsten Absatzmarktes beraubte. Auch der Export nach Übersee gestaltete sich zunehmend schwierig, denn die Militärs beanspruchten viele Transportkapazitäten für sich. Außerdem führten deutsche U-Boote und kleinere Kreuzer einen Handelskrieg gegen die Entente-Mächte, wobei auch zivile Frachter versenkt wurden, da man die Entente verdächtigte, sie für Waffenlieferungen zu missbrauchen (z. B. die Versenkung der RMS Lusitania).
Die Macht der Motion Picture Patents Company (MPPC) war 1914 bereits weitgehend gebrochen, die später folgenden Gerichtsurteile waren nur noch Formalitäten. Sowohl die nationale als auch die internationale Konkurrenz der Independents waren also ausgeschaltet. Die US-Filmwirtschaft verlor zwar einen Teil des europäischen Absatzmarktes, doch der Bedarf an frischen Filmen innerhalb der Vereinigten Staaten war höher als in ganz Europa zusammen, so gab es beispielsweise 1916 bereits ca. 28.000 Kinos in ganz Amerika.
Auch in der übrigen Welt nahmen die Hollywood-Unternehmen eine dominierende Stellung ein, sie stellten zum Beispiel einen Großteil der in Australien und Südamerika gezeigten Filme, die ab ca. 1916 direkt vertrieben wurden (früher war es üblich, an lokale Zwischenhändler zu verkaufen).
Nach Robert C. Allen und Douglas Gomery basiert der freie Wettbewerb zwischen Unternehmen auf vier Punkten:
Der erste Versuch, den freien Wettbewerb zu zerstören und ein Oligopol zu bilden, wurde mittels der Patente betrieben. MPPC versuchte, den Zugang fremder Unternehmen zu behindern, indem sie diesen durch Lizenzgebühren den Wettbewerb erschwerte. Um das System durchzusetzen, sollte zudem eine hohe Marktdurchdringung erfolgen. Auf ihrem Höhepunkt kontrollierte die MPPC via Lizenz den Großteil der Kinos. Auch der Zugang zu Filmmaterial war nicht ohne Lizenz möglich, da Eastman Kodak einen Exklusivvertrag mit der MPPC geschlossen hatte.
Der Edison-Trust attackierte also vor allem die Punkte 2-4. Das System scheiterte endgültig mit der Annullierung der Edison-Patente durch den Obersten Gerichtshof der Vereinigten Staaten, sein Niedergang jedoch hatte schon wesentlich früher begonnen.
Den freien Zugang zum Filmmaterial erlangten die Independents durch den Bau eigener Kameras und durch die Aufhebung des Patents auf Rohfilme 1912. Und um mit dem Trust konkurrieren zu können, begannen sie, ihre Filme von denen der MPPC unterscheidbar zu machen. Hierbei entstanden der Feature Film und das „Starsystem“.
Die MPPC war zwar nicht blind gegenüber diesen Neuerungen, auch sie drehte Feature Films, durch ihre Struktur und vor allem durch ihre Kundenstruktur, war sie dennoch nicht in der Lage, mit diesen neuen Instrumenten zu experimentieren. Der Trust wollte Massenware verkaufen um eine bestimmte Marge zu erwirtschaften. Teure Stars hätten nur die Kosten hochgetrieben, und Feature Films bargen ein nicht zu unterschätzendes Risiko, für das die Kunden des Trusts nicht aufkommen wollten. So konnten die „Independents“ den ersten Punkt des freien Wettbewerbs unterhöhlen und einzigartige Filmerlebnisse statt austauschbarer Produkte bieten, was dem Publikumsinteresse deutlich entgegenkam und vor allem finanzkräftigere Mittelschichten erschloss.
Der Feature Film kommt ca. 1909 auf und wird nur von den Independents ernsthaft weiterentwickelt, beispielsweise von Famous Players, die später nur noch Features produzieren. Famous Players sind auch die erste Gesellschaft, die das Starsystem konsequent nutzt, nach früheren Versuchen, z. B. von I.M.P.
Durch die oben genannten Schritte schaffen es die Independents, sich eine Position im Markt zu sichern und immer weiter auszubauen. Für nationales und internationales Wachstum fehlen ihnen effiziente Strukturen, zum Beispiel in der Distribution. Noch bis in die Mitte der 1910er Jahre hält sich das alte States-Rights-System, in dem der Produzent lokale Franchise-Rechte seines Films an einen Distributor verkauft, der diese dann innerhalb seines festgelegten Gebiets an Kinos weiter verleiht.
Diese Situation ändert sich erstmals 1914 mit der Fusion von elf regionalen Distributoren zu Paramount, die als erste landesweite Rechte handelt. Durch ihre schiere Größe kann das Unternehmen wesentlich kosteneffizienter arbeiten als die Mitbewerber, ganz abgesehen davon, dass dieses System auch für die Produktionsgesellschaft erhebliche Vorteile mit sich bringt. Das alte System kommt bis 1918 zum Erliegen.
Kurz nach ihrer Gründung schließt Paramount Fünfjahresverträge mit Famous Players, Lasky und Bosworth ab, die später auf 25 Jahre verlängert werden. Hier zeichnet sich ein Trend ab, der 1914 zunehmend an Bedeutung gewinnt: Die Verflechtung der bisher getrennten Bereiche Distribution, Produktion und Vorführung, ein Phänomen, das in der Fachliteratur als Vertikale Integration bezeichnet wird. Die Bindung durch die Fünfjahresverträge ist vorteilhaft für alle Beteiligten: Jeder profitiert vom Erfolg des anderen. Wenn das Lasky-Programm sehr gut ist, wird das Paramount-Sortiment von mehr Kinos gekauft, wovon auch Famous Players und Bosworth profitieren, da ihr Programm so auch eine größere Verbreitung findet. Die Kooperation führt dann auch, zwei Jahre später, zur Fusion der genannten und noch einiger weiterer Unternehmen.
Doch es lassen sich durchaus auch frühere Beispiele für vertikale Integration finden. So sind 1912 unter dem Namen Universal erstmals alle drei Bereiche des Filmbusiness vereint. Es fehlte allerdings eine große First-Run-Kinokette. Dennoch schien der Branche die Fusion so bedrohlich, dass die Gründung von Mutual eine direkte Gegenmaßnahme darstellen sollte. Auch hier fanden sich viele Unternehmen unter einem Dach zusammen, denen es explizit nur um Distribution und Produktion ging.
Auch William Fox besitzt 1913 ein Distributions- und ein Produktionsunternehmen, die allerdings erst später zusammengeführt werden. Von Seiten der Kinokettenbesitzer ist zunächst wenig zu hören, erst 1915 schließen sich drei große Ketten, Rowland, Clarke und Mayer, zur Metro Pictures Corporation zusammen, einer Produktionsgesellschaft.
Die wirklich große Reaktion der Kinobesitzer kam erst 1917. Zu diesem Zeitpunkt war die fusionierte Paramount zur dominanten Gesellschaft geworden, die ihre Filme mittels Block-Booking vertrieb. Das hieß, um einen Film mit einem Star vom Kaliber einer Mary Pickford zu bekommen, musste man ein komplettes Paket erwerben, dessen große Mehrheit bestenfalls als durchschnittlich zu bezeichnen war. Andererseits konnte man dem Kauf der Pakete schlecht entgehen, wenn man nicht sein Publikum an ein anderes Kino verlieren wollte, das ebendiesen Mary-Pickford-Film zeigte.
Um dieses System zu durchbrechen, schlossen sich 26 der größten nationalen First-Run-Kinokettenbesitzer zum First National Exhibitors Circuit zusammen. Mit ihrer erheblichen Kaufkraft wollten sie gemeinsame Einkäufe tätigen und auch distribuieren. Zuerst war es das Ziel, Stars zu kaufen, ihre Filme zu finanzieren und im Gegenzug das Aufführungsrecht zu erwerben sowie das Recht, die entstandenen Filme regional weiter zu verleihen.
Sehr bald kam auch eine eigene Produktion dazu. Zwischen 1917 und 1918 nahm First National Charlie Chaplin und Mary Pickford für jeweils eine Million Dollars unter Vertrag. Beide erhielten vollständige künstlerische Freiheit. First National kontrollierte zu diesem Zeitpunkt bereits ca. 600 Kinos, 200 davon Erstaufführungshäuser.
Aus den First-Run-Kinos stammten bis zu 50 Prozent der Einnahmen der Produzenten, außerdem waren Kinos die verlässlichsten Geldverdiener im recht unsteten Filmgeschäft, da das Betreiberrisiko viel geringer war als beispielsweise in der Produktion. Darüber hinaus entschied der Erfolg in den First-Runs über eine lukrative Distribution.
Wenn Paramount also seine Abnehmer und sein Publikum nicht verlieren wollte, musste ein Gegenschlag erfolgen. Also stieg die Gesellschaft, mit finanzieller Unterstützung des Bankhauses Kuhn, Loeb & Co., ins Geschäft mit den Kinos ein, anfangs mit einer Summe von 10 Millionen Dollars. Somit wurde Paramount der erste vollintegrierte, oder komplett vertikal integrierte Filmkonzern.
So wurden aus den alten Independents die Inhaber des zweiten Oligopols. Am Ende der 1910er Jahre war der erste Punkt des freien Wettbewerbs durch das Starsystem und Feature-Filme außer Kraft gesetzt, der zweite Punkt durch die schiere Größe der Unternehmen: Weniger als zehn Unternehmen kontrollierten über 50 Prozent des Marktes. Durch die Vereinigung der Distribution und durch den beginnenden Kampf um die Kinos waren auch die letzten beiden Bedingungen für einen funktionierenden Wettbewerb ausgehebelt.
Ein neues Unternehmen konnte weder einen genügenden Zugang zu den Kinos noch Zugriff auf die Stars, also auf die essentiellen Ressourcen der Filmproduktion erhalten. Auch waren die Produktionskosten stark gestiegen. Zwischen 50.000 und 100.000 US-Dollar pro Film waren normal, nach oben gab es keine Beschränkungen. Ein Großteil dieses Geldes floss in die Taschen der Stars, der Rest wurde in bessere Ausstattung investiert, eine weitere Hürde für Neueinsteiger.
Um dem Trend zu höheren Gagen entgegenzuwirken, und um, wie später in einer Anhörung des Obersten Gerichtshofs bekannt wurde, ein Monopol zu errichten, planten First National und Paramount eine Fusion im Wert von 40 Millionen US-Dollar. Es war geplant, mit jedem bedeutenden Kinobesitzer in den Vereinigten Staaten einen Fünf-Jahres-Vertrag abzuschließen. Die Stars hätten dann keine Grundlage mehr für irgendwelche Forderungen gehabt.
Die Pläne zu diesem Merger wurden von einem Privatdetektiv aufgedeckt, der im Auftrag von Charlie Chaplin, Mary Pickford, Douglas Fairbanks und D. W. Griffith herausfinden sollte, warum weder First National noch Paramount ihre Verträge verlängerte. Natürlich waren sie entsetzt über solche Aussichten und beschlossen, dem entgegenzuwirken, indem sie ihr eigenes Unternehmen gründeten.
1919 entstand United Artists als Gesellschaft für den Filmvertrieb. Finanziert wurde das Unternehmen durch die Morgan-Gruppe sowie durch eine Einlage von 100.000 US-Dollar für Vorzugs-Anteilscheine durch die Eigentümer. Daneben existierten auch normale Anteilscheine, bei deren Weiterverkauf United Artists ein Vorkaufsrecht hatte.
Die Gesellschaft hatte keine eigenen Studios, sondern nutzte die Studios seiner Mitglieder. Sie war errichtet worden als reine Dienstleistungsgesellschaft, die nicht auf Rendite arbeiten sollte, sondern den Besitzern größtmögliche Autonomie und Profite aus dem Geschäft mit ihren Filmen einräumte. Es gab kein Block-Booking, jeder Film wurde individuell vertrieben und musste allein durch seine künstlerischen Qualitäten überzeugen. Die Verleihgebühren der United Artists lagen deutlich unter denen von First National und Paramount, stellten also eine erhebliche Bedrohung für die marktbeherrschende Stellung der beiden dar.
Die Fusion der beiden Giganten war auch gescheitert, weil ihr wichtigstes Kapital, die Stars, sich auf und davon gemacht hatte. First National war also immer noch Konkurrent Paramounts, und die United Artists mit ihren qualitativ sehr hochwertigen Filmen und ihrer enormen Beliebtheit brachten das Unternehmen weiter in Bedrängnis. Also versuchte Paramount das, was man heute eine feindliche Übernahme nennen würde: Stück für Stück wurden die in der First National zusammengeschlossenen Kinoketten aufgekauft.
Auch andere Unternehmen versuchten nun, Kontrolle über die Erstaufführungshäuser zu erlangen, sogar United Artists sah sich später, 1924, mangels Abnehmern gezwungen, eine eigene Kette zu gründen. Wie auch schon in der Vergangenheit, wurden die Kämpfe um die Kinos mit harten Bandagen ausgetragen, vor allem Paramounts „dynamite gang“, auch „wrecking crew“ genannt, wurde ihrem Ruf gerecht. Eine weit verbreitete Methode, Kinos an sich zu binden, war das Blocksystem.
Seit 1917 begannen US-amerikanische Unternehmen, ihre Gewinne auf der Basis von in- und ausländischen Verkäufen zu schätzen. Aus dieser Gewinnschätzung ergab sich das Budget der Produktion, das dadurch erhöht wurde, was für die ausländische Konkurrenz doppelt schlecht war. Die Produktionskosten eines Filmes wurden in den Vereinigten Staaten amortisiert, und später wurden die Filme billig im Ausland angeboten, wodurch die internationale Konkurrenz nicht mehr mithalten konnte.
US-amerikanische Filme galten als qualitativ besser und waren im Erwerb trotzdem günstiger als z. B. deutsche Produktionen. Auch waren die Infrastruktur und die Rationalisierung der Produktionsabläufe nirgends so weit gediehen wie in Hollywood, ein Resultat auch des wachsenden Einflusses der Banken.
Als der Erste Weltkrieg vorbei war, und die Menschen in den bislang abgeschnittenen Ländern wie Deutschland oder Österreich erstmals wieder Hollywood-Produktionen zu sehen bekamen, erlebten sie einen wahren Quantensprung in der Qualität. Die führenden europäischen Filmproduktionsländer, deren isolierte Filmindustrien fünf Jahre lang unter dem Ersten Weltkrieg gelitten hatten, und zudem mit viel geringeren Budgets zu kämpfen hatten, konnten der Konkurrenz aus den Vereinigten Staaten nur noch wenig entgegensetzen. Bis 1927 erhöhte sich der Anteil der amerikanischen Filmproduktion an der Weltfilmproduktion auf nahezu 90 %, was zu Beginn der 1920er Jahre die Filmwirtschaft in England, Frankreich, Italien, Deutschland und Österreich schwer in Bedrängnis brachte und die dortige Filmproduktion stark zurückgehen ließ. Zahlreiche europäische Filmproduktionsgesellschaften mussten schließen. 1925 wurden alleine nach Österreich 1200 US-Produktionen exportiert, obwohl der Bedarf der dortigen Kinos auf lediglich rund 350 geschätzt wurde. In vielen Ländern wurden Filmkontingente eingeführt, die die erlaubte Anzahl an Filmimporten aus den Vereinigten Staaten regelten.
Da rund 45 % der Gewinne zu dieser Zeit aus Europa kamen, wurden die Restriktionen in Europa von den amerikanischen Filmmagnaten mit Argwohn betrachtet. Zumeist erfolglos wurde gegen Einfuhrbeschränkungen Lobbying betrieben. In Ungarn jedoch wurden die geplanten Einfuhrbeschränkungen nicht eingeführt, nachdem die US-amerikanische Filmindustrie den ungarischen Behörden damit gedroht hatte, keine Filme mehr in Ungarn zu zeigen.
1927 waren nach Zahlen des US-Handelsdepartements beim amerikanischen Film 350.000 Personen beschäftigt. Zur Filmproduktion wurden rund 500.000 Kilometer Filmband verbraucht, wofür mehr Silber benötigt wurde, als der Umlauf an Silbermünzen in den Vereinigten Staaten ausmachte. Es wurden Filme im Ausmaß von 75.000 Kilometer Filmband und einem damaligen Wert von rund 320 Millionen Mark exportiert. Ende des Jahres 1927 zählten die Vereinigten Staaten 21.642 Kinos, die in jenem Jahr insgesamt 3 Milliarden Mal besucht wurden, was wiederum einen Erlös aus dem Eintrittsgeld von rund 2,5 Milliarden Dollar ergab.
Während Amerika den weltweiten Filmmarkt fast ohne nennenswerte Konkurrenz dominierte, hatten ausländische Produktionen am US-Markt kaum eine Chance. Spielten in manchen Ländern jährlich bis zu 1000 oder mehr US-Filmproduktionen in den Kinos, liefen in den gesamten Vereinigten Staaten im Jahr 1927 nur 65 ausländische Filme, davon 38 aus Deutschland, neun aus England, sechs aus Frankreich, vier aus Russland, je zwei aus Österreich und Italien und je einer aus China und Polen. Selbst diese Filme waren zumeist nur wenig verbreitet und liefen fast ausschließlich auf so genannten Filmkunstbühnen.
Ab 1933, verstärkt jedoch ab Beginn des Zweiten Weltkriegs und der Ausbreitung des Deutschen Reichs auf immer weitere Teile Europas, setzte eine Emigrationswelle von zumeist jüdischen Filmschaffenden aus Europa ein. Waren deren Auswanderungsziele zu Beginn noch häufig europäische Städte mit Filmindustrie wie Wien, Paris oder London, kristallisierte sich bald die aufstrebende Filmindustrie Hollywoods als begehrtestes und vielversprechendstes Ziel der Emigranten heraus – verstärkt durch gezieltes Anwerben europäischer Filmgrößen durch Hollywood-Studiobosse.
Von den etwa 2000 jüdischen Filmschaffenden, die im Deutschen Reich keine Arbeit mehr fanden und auswandern mussten, fanden sich letztendlich rund 800 in Hollywood wieder – darunter fast die gesamte Elite des deutschsprachigen Filmschaffens dieser Zeit. Vielen gelang dort eine ruhmvolle Karriere, viele, vor allem jene, die 1938 und noch später ohne Arbeitsangebot in Hollywood ankamen, konnten nicht mehr an ihre bisherige Karriere anschließen und kamen nur in schlecht bezahlten und unbedeutenden Positionen unter oder mussten nach einer Weile gar das Filmgeschäft aufgeben. Statt der bisher aus Berlin und Wien gewohnten Kaffeehäuser, wo man sich einst regelmäßig traf, wurden nun große Appartements und Villen von in Hollywood erfolgreichen Emigranten neue Treffpunkte. Beliebte Treffpunkte der Film- und Theaterschaffenden waren die Adressen von Henry Koster, Paul Henreid, Ernst Deutsch-Dryden, Paul Kohner und später auch von Sam Spiegel. Die literarische Emigration, inklusive Drehbuchautoren, traf sich häufig bei Salka Viertel und bei Brecht.
Hollywood
Experimentalfilm
Dokumentarfilm
Independent film

</doc>
<doc id="51" url="https://de.wikipedia.org/wiki?curid=51" title="Vorsätze für Maßeinheiten">
Vorsätze für Maßeinheiten

Vorsätze für Maßeinheiten, auch Einheitenvorsätze, "Einheitenpräfixe" oder kurz "Präfixe" oder "Vorsätze" genannt, dienen dazu, Vielfache oder Teile von Maßeinheiten zu bilden, um Zahlen mit vielen Stellen zu vermeiden.
SI-Präfixe sind die für die Verwendung im Internationalen Einheitensystem (SI) definierte Dezimal-Präfixe. Sie basieren auf Zehnerpotenzen mit ganzzahligen Exponenten. Man unterscheidet zwischen dem Namen des Präfix und seinem Symbol. Die Symbole sind international einheitlich. Die Namen unterscheiden sich je nach Sprache.
Die Zeichen für Teile einer Einheit werden als Kleinbuchstaben geschrieben, während die meisten Zeichen für Vielfache einer Einheit als Großbuchstaben geschrieben werden. Ausnahmen von dieser Systematik sind aus historischen Gründen die Zeichen für Deka (da), Hekto (h) und Kilo (k).
Das Mikro-Zeichen „µ“ stammt als einziges Präfix-Symbol aus der griechischen Schrift, was beim Maschinenschreiben und Drucken in der Praxis Schwierigkeiten bereitet hat. In der elektrotechnischen Literatur wurde deshalb ersatzweise häufig ein „u“ verwendet. Das wurde in der Internationalen Norm ISO 2955 von 1983, die 2001 zurückgezogen wurde, auch so empfohlen. Für Deutschland gelten weiterhin die Empfehlungen der DIN-Norm DIN 66030 „Informationstechnik – Darstellung von Einheitennamen in Systemen mit beschränktem Schriftzeichenvorrat“ vom Mai 2002. In Österreich sieht das Maß- und Eichgesetz „μ“ vor. Beim Austausch medizinischer Daten gemäß dem HL7-Standard ist das „u“ anstelle von „µ“ zugelassen.
Die Einheitenvorsatzzeichen werden wie die Einheitenzeichen in aufrechter (nicht kursiver) Schrift geschrieben, unabhängig von der Schriftart (Schriftauszeichnung) des umgebenden Textes. Zwischen Einheitenvorsatzzeichen und Einheitenzeichen wird kein Zwischenraum geschrieben.
Der Name eines Einheitenvorsatzes bildet mit dem zugehörigen Einheitennamen ein zusammengesetztes Wort. Beispiele sind „Nanometer“, oder „Milligramm“. Wenn aus dem Zusammenhang klar ist, welche Einheit gemeint ist, wird dieses zusammengesetzte Wort in der Umgangssprache häufig auf den Vorsatz verkürzt. So ist von „Kilo“ die Rede, wenn „Kilogramm“ (kg) gemeint ist. Im technischen Bereich wird der Mikrometer (µm) kurz als „Mü“ bezeichnet; im Englischen ist die Bezeichnung „micron“ für Mikrometer üblich. Im Österreichischen wird die Abkürzung „Deka“ für die Masseeinheit „Dekagramm“ (dag) verwendet, unter Handwerkern auch „Zenti“ für Zentimeter (cm).
Im Flächenmaß Hektar verschwindet ausnahmsweise an der Wortfügestelle das "o" von "hekto", was den Doppelselbstlaut vermeidet.
Bis 1960 waren in Frankreich die Vorsätze „Myria“ (gr. = zehntausend) mit dem Zeichen ma für das 10-fache und „dimi“ mit Zeichen dm für das 10-fache genormt. Statt „myria“ wurde Anfang des 19. Jahrhunderts auf einen Vorschlag von Thomas Young hin z. T. auch „myrio“ geschrieben.
Bis um 1900 wurde in Österreich „Centimeter“ mit C geschrieben.
Früher waren in Deutschland auch das Symbol D und in Großbritannien dk für „Deka“ üblich, in Österreich war das Zeichen dk bis Mitte der 1950er Jahre gesetzlich vorgeschrieben.
In DIN 1301 Teil 1 vom Dezember 1993 wurde der SI-Vorsatz für 10 „Yocto“ geschrieben; diese Schreibweise wurde in der Ausgabe vom Oktober 2002 zu „Yokto“ korrigiert.
Bis 1950 wurden die elektrische Kapazität von Kondensatoren, aber auch die Selbstinduktion von Spulen in cm (Centimeter) des CGS-Einheitensystems angegeben, dann wurde pF (Piko-Farad) auch als µµF geschrieben. Der besseren Lesbarkeit wegen findet sich auf oft kleinen Bauteilen statt µF gelegentlich MF (oder MFD. im Englischen) oder KV statt kV für Spannung und MEGOHM statt MΩ für Widerstand.
Im Sprachgebrauch von Internetbenutzern wird zunehmend das SI-Präfix k verwendet, in Kontexten, wo das sonst kaum üblich ist, z. B. bei Zeit- und Stückzahlangaben. Vergleiche auch den besonders speziellen Fall der Bezeichnung Y2K für das Jahr-2000-Problem oder W2K für Microsoft Windows 2000. Im kaufmännisch-technischen Umfeld wird das Präfix k außerdem häufig mit Währungseinheiten verwendet, etwa als k€. Die dort ebenfalls verwendete Kombination T€ stammt nicht aus diesem SI, sondern bedeutet „Tausend Euro“.
In der Datenverarbeitung werden SI-Präfixe auch für Datenmengen (Bits und Bytes) verwendet, allerdings oft in der Bedeutung als Binärpräfix (Vielfache von 1024, z. B. 2, 2, 2 usw.). Bis heute werden bei Datenmengen je nach Kontext, unter Umständen je nach betrachtetem Speichermedium, die SI-Präfixe als Dezimalpräfixe oder Binärpräfixe verwendet, was insbesondere bei höheren Werten zu erheblichen Abweichungen führt.
Die für die Normung in der Elektrotechnik zuständige International Electrotechnical Commission hat daher zuerst in der Norm IEC 60027-2 (ersetzt durch IEC 80000-13:2008) besondere, an die SI-Präfixe angelehnte Binärpräfixe gemäß unten stehender Tabelle definiert und empfiehlt deren Verwendung für Datenmengen. Die dezimalen SI-Präfixe sollen bei Datenmengen das gleiche bedeuten wie bei SI-Einheiten (Dezimalpräfixe). Das für die SI-Präfixe zuständige Internationale Büro für Maß und Gewicht (BIPM) empfiehlt ebenfalls die Anwendung dieser Norm:
Das binäre Präfixsymbol entsteht durch das Anhängen von -i an das entsprechende dezimale Präfixsymbol. Ki wird dabei im Gegensatz zu k groß geschrieben. Das binäre Präfix selbst entsteht durch das Anhängen von -bi an die ersten beiden Buchstaben des entsprechenden dezimalen Präfixes.
Beispiel für die Verwendung: Ein Kibibyte wird geschrieben als 1 KiB = 2 B = 1024 B, wobei B für Byte steht.

</doc>
<doc id="53" url="https://de.wikipedia.org/wiki?curid=53" title="Abkürzungen/Gesetze und Recht">
Abkürzungen/Gesetze und Recht

Viele, aber nicht alle, Fachautoren kürzen ohne Punkt und Leerzeichen ab: So wird die Abbreviatur für "in der Regel", normalerweise "i. d. R.", mit "idR" abbreviert.
Ungeachtet dessen werden Akronyme, wie sonst auch, ohne Punkte gebildet.
Folgende Liste von Abkürzungen aus der Rechtssprache versucht nur sprachlich korrekte Abbreviaturen, also mit Punkt und Leerzeichen, aufzuführen, sofern nicht die geraffte Schreibweise in den Alltag Einzug gefunden hat und dort dominiert (z. B. "eG", "GbR" oder "MdB") oder die Abbreviatur nur in juristischen Schriften und dort auch nur zusammengeschrieben auftaucht.
(Tierschutztransportverordnung) (siehe Tiertransport#Gesetzliche Bestimmungen)

</doc>
<doc id="54" url="https://de.wikipedia.org/wiki?curid=54" title="Liste von Abkürzungen (Computer)">
Liste von Abkürzungen (Computer)

Dies ist eine Liste technischer Abkürzungen, die im IT-Bereich verwendet werden.

</doc>
<doc id="56" url="https://de.wikipedia.org/wiki?curid=56" title="Liste von Unternehmen mit Namensherkunftserklärungen">
Liste von Unternehmen mit Namensherkunftserklärungen

Dies ist eine Liste von Firmen (Namen der Unternehmen) sowie ihrer Herkunft (Etymologie). Häufig sind die Firmen Abkürzungen oder Akronyme der Namen der Gründer, voriger Gesellschaften oder des Unternehmensziels.

</doc>
<doc id="61" url="https://de.wikipedia.org/wiki?curid=61" title="ISO 4217">
ISO 4217

ISO 4217 ist die von der Internationalen Organisation für Normung publizierte Norm für Währungs-Abkürzungen, die im internationalen Zahlungsverkehr zur eindeutigen Identifizierung benutzt werden sollen. Am 1. August 2015 wurde die neue Version ISO 4217:2015 veröffentlicht. Diese 8. Version ersetzt den Vorgänger aus dem Jahr 2008.
Die Abkürzungen umfassen jeweils drei Buchstaben. Die ersten beiden sind üblicherweise die Landeskennung nach ISO 3166-1 ALPHA-2 (beispielsweise "AU" für Australien), der letzte Buchstabe ist in der Regel der Anfangsbuchstabe des Währungsnamens, so beispielsweise "D" für "Dollar". Gemeinsam ergibt dies "AUD" als genormte Abkürzung für den Australischen Dollar.
Währungen, die nicht von einem Einzelstaat herausgegeben werden, haben als ersten Buchstaben ein "X"; die beiden folgenden Buchstaben geben den Namen der Währung an. Dies ist sowohl bei den meisten Währungsunionen der Fall (z. B. der Ostkaribische Dollar ("XCD")), als auch bei den IWF-Sonderziehungsrechten ("XDR").
Von diesen Regeln wird in den folgenden Fällen abgewichen:
Auch für nicht-geldliche Wertaufbewahrungs- und Transaktionsmittel gibt es Kodierungen. So wird eine Feinunze Gold (= 31,1034768 Gramm) beispielsweise mit "XAU" abgekürzt (zusammengesetzt aus X und dem chemischen Symbol für Gold: "Au"), Silber entsprechend mit "XAG". Transaktionen, in denen keine Währung verwendet wird, werden mit "XXX" gekennzeichnet.
Neben der Buchstabenkodierung werden auch dreistellige Zifferncodes verwendet. Dabei bedeuten die Zahlenbereiche
Teilweise wird bei Änderung der Währung und des Buchstaben-Codes die bisherige numerische Kodierung beibehalten, insbesondere wenn sich lediglich der Name des Zahlungsmittels geändert hat.
Auch wenn eine direkte Währungsumstellung erfolgt, bleibt meist der numerische Code unverändert.
Diese Codes sind deshalb ohne Kenntnis des Zeitpunkts nicht immer so eindeutig einer bestimmten Währung des betreffenden Landes zuzuordnen, wie das mit den Buchstaben-Codes möglich ist. Allerdings sind sie für konkrete finanzielle Transaktionen vorgesehen, so dass die Angabe bei mehreren Möglichkeiten eigentlich nur bedeutet: „In der am Tag der Wertstellung gültigen Landeswährung“.

</doc>
<doc id="76" url="https://de.wikipedia.org/wiki?curid=76" title="Achsensprung (Film)">
Achsensprung (Film)

Ein Achsensprung ist ein Filmschnitt, mit dem die Beziehungssachse der Figuren oder Gruppen übersprungen wird. Blickachsen
oder Beziehungsachsen zwischen den Akteuren untereinander oder dem Point of Interest des Protagonisten bilden eine gedachte Linie. Auf die Leinwand projiziert, stellt diese Linie eine "links-rechts-" und "oben-unten-Beziehung" zwischen den Akteuren dar. Unter Achsensprung bezeichnet man einen Schnitt, bei dem sich dieses Verhältnis umkehrt. Es wird zwischen Seitenachsensprung und dem Höhenachsensprung unterschieden. Letzterer wird als weniger desorientierend vom Zuschauer empfunden, da die Leinwand weniger hoch als breit ist. 
Ein Achsensprung kann beim Zuschauer Desorientierung verursachen, da die Anordnung und Blickrichtung der Akteure im Frame sich relativ zum Zuschauer zu verändern scheint.
Aktionsachse (Handlungsachse)
ist die gedachte Linie, in deren Richtung sich die Handlung oder das Inertialsystem der Filmwelt bewegt. Bei einer Autofahrt zum Beispiel ist die Aktionsachse so stark, dass die Beziehungsachsen an Bedeutung verlieren. Die Orientierung bleibt trotz eventuellem Achsensprung bewahrt. Wenn man aus der Fahrerseite filmt, bewegt sich die Landschaft scheinbar von rechts nach links; filmt man aus der Beifahrerseite, bewegt sie sich scheinbar von links nach rechts. Diese Änderung der Bewegungsrichtung ist aber nicht irritierend. Analog werden zwei Autos, die bei einer Parallelmontage in die gleiche Richtung fahren (oft von links nach rechts, weil das unserer Leserichtung entspricht), als einander verfolgend wahrgenommen; wenn eines jedoch von links nach rechts und das andere von rechts nach links fährt, erwartet der Zuschauer einen Zusammenstoß.
Im Continuity Editing des klassischen Hollywoodkinos wird der Achsensprung als Fehler betrachtet und dementsprechend vermieden. 
Der Grundsatz, Achsensprünge zu vermeiden, wird "180-Grad-Regel" genannt.
In manchen Fällen kann ein bewusster Achsensprung auch Stilmittel sein, um beispielsweise Verwirrung oder einen Kippmoment zu symbolisieren; Stanley Kubrick wird in diesem Zusammenhang häufig genannt. In Werbespots werden Achsensprünge oft verwendet, um einen rasanten Effekt zu bewirken. Bekannt ist auch eine Szene aus "Herr der Ringe", in welcher Sméagol mit sich selbst spricht. Da er mit den Schnitten wechselnd von der einen zur anderen Seite spricht (Achsensprung), entsteht der Eindruck zweier gleich aussehender Personen, womit der gespaltene Charakter der Figur unterstrichen wird.
Im Gegensatz zum Achsensprung handelt es sich hierbei um eine Bewegung der Kamera (Steadicam oder einer Dollyfahrt) über die Achse oder um eine Änderung der Bewegungsachse bzw. der Blickrichtung der Figuren, wodurch eine neue Achse definiert wird. Der Achsenwechsel wird vom Zuschauer nicht als störend wahrgenommen, weil sich die Bewegung fließend vollzieht. Diese Bewegung wird mitunter auch als Crab bezeichnet. Außerdem kann ein Zwischenschnitt in eine Totale eine Achsenüberschreitung möglich machen, da so die räumliche Anordnung der Akteure für den Zuschauer deutlich wird, oder der Zwischenschnitt auf einen Closeup, da sich der Betrachter danach wieder neu räumlich orientiert.

</doc>
<doc id="77" url="https://de.wikipedia.org/wiki?curid=77" title="Alfred Hitchcock">
Alfred Hitchcock

Sir Alfred Joseph Hitchcock KBE (* 13. August 1899 in Leytonstone, England; † 29. April 1980 in Los Angeles, Kalifornien) war ein britischer Filmregisseur, Drehbuchautor, Filmproduzent und Filmeditor. Er siedelte 1939 in die USA über und nahm am 20. April 1955 zusätzlich die amerikanische Staatsbürgerschaft an.
Hitchcock gilt hinsichtlich seines Stils bis heute als einer der einflussreichsten Filmregisseure. Er etablierte die Begriffe Suspense und MacGuffin. Sein Genre war der Thriller, charakteristisch seine Verbindung von Spannung mit Humor. Die wiederkehrenden Motive seiner Filme waren Angst, Schuld und Identitätsverlust. Mehrfach variierte er das Thema des unschuldig Verfolgten.
Hitchcock legte großen Wert auf die künstlerische Kontrolle über das Werk des Autors. Sein Gesamtwerk umfasst 53 Spielfilme und gehört gemessen am Publikumserfolg sowie der Rezeption durch Kritik und Wissenschaft zu den bedeutendsten der Filmgeschichte. Auch dank seiner bewussten Selbstvermarktung zählt Hitchcock heute zu den bekanntesten zeitgeschichtlichen Persönlichkeiten. Er ist dem Autorenfilm zuzurechnen.
Am 3. Januar 1980 wurde er von Königin Elisabeth II. zum Knight Commander des Order of the British Empire ernannt.
Alfred Hitchcock wurde am 13. August 1899 als jüngster Sohn des Gemüsehändlers William Hitchcock (1862–1914) und dessen Ehefrau Emma Jane Whelan (1863–1942) in Leytonstone bei London geboren. Durch den Altersunterschied von sieben beziehungsweise neun Jahren zu seinen Geschwistern, durch seine römisch-katholische Erziehung in einem von der anglikanischen Kirche geprägten Land und nicht zuletzt durch sein Äußeres – er war klein und schon als Kind korpulent – hatte er eine einsame Kindheit.
Zwischen 1910 und 1913 war er Schüler des St.-Ignatius-College, einer Londoner Jesuiten-Schule. Er verließ das College mit knapp 14 Jahren und besuchte stattdessen Abendkurse auf der Londoner Universität, diverse Handwerkskurse und später wenige Monate lang die "School of Engineering and Navigation". Zudem belegte er Kurse in Technischem Zeichnen sowie in Kunstgeschichte an der Londoner Kunstakademie. Seine Freizeit verbrachte er großteils mit dem Lesen von Fahrplänen und dem Studium von Stadtplänen und Landkarten. Mit fortschreitendem Alter flüchtete er sich in Romane, besuchte Theatervorstellungen und ging oft ins Kino. Außerdem verfolgte er Mordprozesse im Gerichtshof Old Bailey und besuchte gerne das Black Museum von Scotland Yard. Der Tod des Vaters Ende 1914, zu dem er kein enges Verhältnis hatte, band Hitchcock noch enger an seine Mutter.
1915 nahm er eine Stelle als technischer Angestellter bei der "W.T. Henley Telegraph Company" an, die elektrische Leitungen herstellte. Wegen seines zeichnerischen Talents wurde er bald in die Werbeabteilung versetzt. Unter seinem bis zuletzt gebrauchten Spitznamen „Hitch“ veröffentlichte er in der Betriebszeitschrift seine ersten gruseligen Kurzgeschichten.
Im Frühjahr 1920 hörte Hitchcock von der Neugründung eines Studios der amerikanischen Produktionsgesellschaft Paramount Famous Players-Lasky im Londoner Stadtbezirk Islington. Er bewarb sich mit einer Mappe mit Illustrationen und wurde als Zeichner von Zwischentiteln angestellt. In den Jahren 1921 und 1922 zeichnete er die Titel für mindestens zwölf Filme. Nebenbei entwarf er Kostüme, Dekorationen und Szenenbilder. Auch durch Überarbeitungen von Drehbüchern machte er auf sich aufmerksam. Bei zwei Filmen arbeitete er mit George Fitzmaurice zusammen, dessen genaue Produktionsplanung ihn sehr beeinflusste.
1922 bekam Hitchcock Gelegenheit, sich als Regisseur zu versuchen. Mit dem Autor Seymour Hicks stellte er die letzten Filmszenen von "Always Tell Your Wife" fertig, nachdem der ursprüngliche Regisseur gefeuert worden war. Bald darauf konnte er einen eigenen Film drehen, "Number 13" (in einigen Quellen "Mrs. Peabody)", der jedoch unvollendet blieb, da "Famous Players-Lasky" im Laufe der Dreharbeiten das Studio wegen finanzieller Schwierigkeiten schließen musste. Das leerstehende Gelände wurde an unabhängige Produzenten vermietet, darunter auch an Michael Balcon, der das Studio 1924 schließlich erwarb. Er stellte Hitchcock als Regieassistent ein, sowie (auf dessen Empfehlung) die Filmeditorin Alma Reville. Die beiden kannten sich seit 1921, seitdem sie gelegentlich an denselben Filmen gearbeitet hatten. Bis 1925 entstanden fünf Filme, bei denen Hitchcock dem Regisseur Graham Cutts assistierte und zu Cutts’ wachsendem Unmut mehr und mehr künstlerischen Einfluss gewann. Neben dem Drehbuch kümmerte er sich auch um die Bauten, das Szenenbild, die Besetzung, die Kostüme sowie die Ausstattung und nahm so mit der Zeit die Aufgaben eines Produktionsleiters wahr.
Hitchcocks letzte Zusammenarbeit mit Graham Cutts führte ihn 1924/25 nach Deutschland. Der unter der Beteiligung der deutschen UFA produzierte Film "Die Prinzessin und der Geiger" entstand in den Babelsberger Filmstudios – damals den modernsten der Welt. Dabei hatte Hitchcock die Möglichkeit, Friedrich Wilhelm Murnau bei den Arbeiten an "Der letzte Mann" zu beobachten; von diesem beeindruckt übernahm er einige Techniken Murnaus für die Szenenbilder seiner aktuellen Produktion. Durch diesen und weitere Besuche konnte Hitchcock fließend Deutsch sprechen; später sprach er zum Beispiel einige Trailer seiner Filme selbst.
Zurück in England übertrug ihm Michael Balcon 1925 die Regie für einen eigenen Film. Das Projekt führte den jungen Hitchcock wieder nach Deutschland. Nur die Münchner Lichtspielkunst (Emelka) fand sich bereit, den Film des unbekannten Regie-Debütanten mitzuproduzieren. Für das Melodram "Irrgarten der Leidenschaft" (1925) verpflichtete Balcon kostspielige Stars aus Hollywood. Alma Reville, mittlerweile Hitchcocks Verlobte, war als Regieassistentin und Editorin Mitglied des sehr kleinen Filmteams. Balcon war mit Hitchcocks ambitionierter Arbeit zufrieden und vertraute ihm eine weitere deutsch-englische Koproduktion an: "Der Bergadler" wurde noch im selben Jahr, diesmal in Tirol, gedreht. Doch beide Filme, die 1925 beziehungsweise 1926 in Deutschland in den Kinos anliefen, wurden in England zunächst nicht veröffentlicht. Der englische Verleiher und Geldgeber C. M. Woolf war, im Gegensatz zu Balcon, nicht von Hitchcocks betont expressionistischem Stil überzeugt.
Mit dem 1926 gedrehten Film "Der Mieter" um einen einzelgängerischen Pensionsgast, der verdächtigt wird, ein Serienmörder zu sein, hatte Hitchcock sein Thema gefunden. Doch nicht zuletzt wegen dessen expressionistischer Bildgestaltung lehnte es Woolf abermals ab, den Film zu veröffentlichen. Balcon zog daraufhin den jungen Ivor Montagu hinzu, der Erfahrung mit Filmüberarbeitungen hatte; mit Hitchcock zusammen wurden einige Änderungen vorgenommen. Der überragende Erfolg bei einer Pressevorführung ebnete dann den Weg zur Veröffentlichung seiner ersten beiden Filme. "Der Mieter" kam 1927 in kurzer Abfolge mit "Irrgarten der Leidenschaft" und "Der Bergadler" in die Kinos und bedeutete für Hitchcock den Durchbruch als Regisseur.
Für Balcons Gainsborough Pictures drehte Hitchcock 1927 noch die zwei Melodramen "Abwärts" und "Easy Virtue". Beiden Filmen war kein Erfolg beschieden. Bereits zuvor hatte er beschlossen, bei deutlich höherem Gehalt zu der neu gegründeten Firma British International Pictures (BIP) des Produzenten John Maxwell zu wechseln. Dort entstand mit dem Boxerdrama "Der Weltmeister" sein erster Film nach einem Originaldrehbuch. Die Presse reagierte äußerst positiv.
Obwohl die drei folgenden Stummfilme "The Farmer’s Wife," "Champagne" und "Der Mann von der Insel Man" abgesehen von einzelnen Szenen als Fingerübungen gelten, hatte sich Hitchcock in Großbritannien innerhalb kurzer Zeit einen Namen gemacht: Die junge britische Filmindustrie, sehr darauf bedacht, sich von der amerikanischen abzuheben, war nur allzu gerne bereit, ihn als kommenden Regiestar zu feiern.
Im Dezember 1926 heirateten Alfred Hitchcock und Alma Reville, die für die Hochzeit zum katholischen Glauben konvertierte. 1928 wurde ihre gemeinsame Tochter Patricia geboren. Beruflich blieb Alma bis zum Schluss seine engste Mitarbeiterin und Beraterin.
Das Aufkommen des Tonfilms hielten viele Regisseure für das Ende ihrer Kunstform. Hitchcock hingegen nutzte das Potential der neuen Technik. "Erpressung" (1929) wurde ursprünglich als Stummfilm produziert. Die Produzenten erlaubten Hitchcock jedoch, eine Filmrolle mit Tonmaterial nachzudrehen. Er versah daraufhin einzelne Schlüsselszenen mit wirkungsvollen Toneffekten und gesprochenem Dialog, wobei die tschechische Schauspielerin Anny Ondra, die ihre Rolle stumm spielen musste, von der englischen Schauspielerin Joan Barry simultan synchronisiert wurde. "Erpressung" war der erste britische Tonfilm und wurde ein großer Erfolg. Hitchcock nutzte seine gewonnene Popularität und gründete mit der "Hitchcock Baker Productions Ltd." eine Gesellschaft zur Vermarktung seiner Person.
Auf Geheiß seines Studios drehte er "Juno and the Paycock" (1930) sowie einige Szenen für die Musikrevue "Elstree Calling". Mit "Mord – Sir John greift ein!" fand er wieder zu seinem Thema und auch nach Deutschland zurück: In Berlin stellte er die deutsche Sprachversion des Films unter dem Titel "Mary" her. Es folgten drei Filme, von denen Hitchcock nur die Komödie "Endlich sind wir reich" wirklich interessierte: In dem zusammen mit seiner Frau und Val Valentine verfassten Drehbuch verarbeitete er unter anderem die Erfahrungen seiner noch jungen Ehe. Den ihm aufgezwungenen Thriller "Nummer siebzehn" beschloss Hitchcock aus Protest zu sabotieren und zu einer wirren, albernen Parodie zu machen. Die turbulente Verbindung zwischen Humor und Spannung lässt "Nummer siebzehn" aus heutiger Sicht als einen Vorläufer späterer Klassiker des Genres erscheinen. Hitchcocks Vertrag mit der "British International Pictures" endete nach sechs Jahren mit einem Einsatz als Produzent "(Lord Camber’s Ladies)". Die Zusammenarbeit hatte zunehmend unter dem Konflikt zwischen Hitchcocks Streben nach künstlerischer Kontrolle und den Vorschriften des Studios gelitten. Doch auch den folgenden Film "Waltzes from Vienna", ein „Musical ohne Musik“ (Hitchcock) für den unabhängigen Produzenten Tom Arnold, drehte er betont lustlos: „Ich hasse dieses Zeug. Melodrama ist das einzige, was ich wirklich kann.“
Unmittelbar nach "Waltzes from Vienna" nahm er die fruchtbare Zusammenarbeit mit dem Produzenten Michael Balcon wieder auf. Als erster Film für die "Gaumont British" entstand der Thriller "Der Mann, der zuviel wußte" (1934). Das Drehbuch erarbeitete Hitchcock im Wesentlichen mit seiner Frau Alma und dem Drehbuchautor Charles Bennett. Der Film wurde sowohl von der Kritik als auch vom Publikum enthusiastisch aufgenommen. Der humorvolle Spionagethriller "Die 39 Stufen" (1935, Drehbuch: Charles Bennett) gilt als Blaupause späterer Verfolgungsthriller. Eine turbulente Szene folgt auf die nächste, es gibt keine Übergänge und kaum Zeit für den Zuschauer, über die manches Mal fehlende Logik nachzudenken. Hitchcock ordnete nach eigenem Bekunden alles dem Tempo unter. Der überragende Erfolg des Films sollte ihm recht geben. Es folgten "Geheimagent" (1936) und "Sabotage" (1936), die insbesondere in Hitchcocks eigener späterer Bewertung gegenüber den beiden Vorgängerfilmen abfielen. Doch die psychologisch vielschichtige Behandlung des Themas „Schuld“ weist bereits auf spätere Werke hin.
Nach "Sabotage" endete abrupt die zweite erfolgreiche Phase der Zusammenarbeit mit Michael Balcon, als die Produktionsfirma Gaumont British von deren Besitzern geschlossen und Balcon entlassen wurde. Die beiden folgenden Filme drehte Hitchcock daher wieder für die Gainsborough Pictures – diesmal allerdings ohne seinen ehemaligen Förderer. "Jung und unschuldig" (1937) war eine weitere, unbeschwerte Variation der Geschichte vom unschuldig Verfolgten. Der gefeierte Thriller "Eine Dame verschwindet" (1938) spielt überwiegend in einem fahrenden Zug. Die Dreharbeiten fanden jedoch ausschließlich in einem kleinen Londoner Studio statt, was dank technisch anspruchsvoller Rückprojektionen möglich wurde.
Hitchcock festigte mit diesen sechs Filmen seine Ausnahmestellung innerhalb des britischen Kinos. Ende der 1930er Jahre beauftragte er die "Selznick-Joyce-Agentur", deren Mitinhaber Myron Selznick, der ältere Bruder des Hollywood-Moguls David O. Selznick, war, seine Interessen wahrzunehmen. Hitchcock, dessen Ruf mittlerweile bis nach Hollywood gelangt war, unterzeichnete schließlich 1938 einen Vertrag für die Produktionsgesellschaft von David O. Selznick, der damals gerade mit der Vorproduktion zu "Vom Winde verweht" beschäftigt war. In Gedanken bereits in Hollywood, drehte Hitchcock in England noch einen letzten Film für die Produktionsfirma des nach England emigrierten deutschen Produzenten Erich Pommer. Doch der Kostümfilm "Riff-Piraten" wurde von der Presse durchweg verrissen.
In seinen ersten Jahren in Hollywood stieß Hitchcock auf unerwartete Schwierigkeiten. David O. Selznick übte starke Kontrolle über die Filme seines Studios aus und achtete darauf, dass sich der freiheitsliebende Hitchcock möglichst eng an die literarische Vorlage seines ersten Hollywoodfilmes hielt. Trotz dieser Spannungen wurde "Rebecca" für den britischen Regisseur ein erfolgreicher Einstand in Hollywood: Das psychologisch dichte und düster-romantische Melodram war 1940 elfmal für den Oscar nominiert und gewann schließlich zwei der Trophäen (Kamera und Produktion).
In den nächsten Jahren machte Selznick sein Geld mit Hitchcock, indem er ihn für beträchtliche Summen an andere Studios auslieh. Der Krieg in Europa weitete sich aus, als der unabhängige Produzent Walter Wanger Hitchcock für ein aktuelles Kriegsdrama engagierte. "Der Auslandskorrespondent" blieb Hitchcocks Naturell entsprechend jedoch ein weitgehend unpolitischer Spionagethriller. Nur der nachgedrehte Schlussmonolog, gerichtet an die noch neutralen USA, wirkte aufrüttelnd. Kurz nach Fertigstellung des Films wurde England von Deutschland bombardiert. Der rechtzeitig ausgewanderte Hitchcock musste sich daraufhin scharfe Kritik von ehemaligen britischen Kollegen, allen voran Michael Balcon, gefallen lassen.
Mit "Verdacht" (1941, RKO), der ersten Zusammenarbeit mit Cary Grant, und "Saboteure" (1942, Universal) blieb Hitchcock bei seinen klassischen Themen. Zwischen diesen Produktionen drehte er, für ihn und andere ungewohnt, seine einzige Screwball-Komödie. Obwohl damals durchaus positiv aufgenommen, zeigte er sich mit "Mr. und Mrs. Smith" (1941, RKO) nicht zufrieden.
Weit mehr am Herzen lag ihm die Arbeit an dem Drama "Im Schatten des Zweifels" (1943, Universal). Hitchcocks Filme gelten allgemein als stark von seinem Charakter geprägt. Dieses Familienmelodram wird als einer seiner persönlichsten Filme bezeichnet: In allen Hauptfiguren spiegeln sich demnach Eigenschaften und Ängste Hitchcocks. Als während der Dreharbeiten Hitchcocks Mutter in London starb, verstärkte dies die autobiografischen Tendenzen.
Wie viele britische Regisseure leistete Hitchcock seine Beiträge für die Kriegspropaganda und drehte unter anderem Kurzfilme zur Unterstützung der französischen Résistance. Auch in seine nächste Hollywood-Produktion arbeitete er stark propagandistische Töne ein, doch sein stets bewusst irritierender Umgang mit Klischees sorgte diesmal für Kontroversen: In einem kleinen Rettungsboot sehen sich englische und amerikanische Schiffbrüchige einem intellektuell überlegenen Nazi gegenüber. Dennoch wurde der formalistisch strenge Psychothriller "Das Rettungsboot" (1943, 20th Century Fox) dreimal für den Oscar nominiert (Drehbuch, Kamera und Regie).
Psychologie, wichtige Komponente seines Werks, stand im Mittelpunkt von "Ich kämpfe um dich" (1945), der nach langer Zeit wieder für Selznick entstand. Dieser war vom Thema Psychoanalyse schnell begeistert und ließ Hitchcock ungewohnt viel freie Hand, doch kürzte er den Film nach der ersten Probevorführung um rund zwanzig Minuten. Die erfolgreiche Zusammenarbeit mit Ingrid Bergman in der Hauptrolle wurde in der folgenden Produktion "Berüchtigt" (1946) fortgesetzt, die Selznick allerdings wieder an RKO verkaufte. Die Geschichte um eine Spionin (Bergman), die aus Pflichtgefühl von ihrem Liebhaber (Cary Grant) gedrängt wird, mit dem Feind zu schlafen, bot für Hitchcocks Obsessionen eine breite Projektionsfläche.
Mit dem Gerichtsdrama "Der Fall Paradin" (1947) lief der Vertrag Hitchcocks mit Selznick aus. Selznick behielt, bei der Stoffauswahl angefangen, bei dieser relativ chaotisch verlaufenden Produktion die Oberhand. Dass Hitchcock währenddessen Vorbereitungen für seine eigene Produktionsfirma traf, verstärkte die Spannungen zwischen den machtbewussten Männern. Dennoch bot Selznick Hitchcock – erfolglos – eine Vertragsverlängerung an.
Bereits im April 1946, rund zwei Jahre bevor der Vertrag mit Selznick auslaufen sollte, gründete Hitchcock mit dem befreundeten Kinokettenbesitzer Sidney Bernstein die Produktionsfirma Transatlantic Pictures, für die er seinen ersten Farbfilm inszenierte, "Cocktail für eine Leiche" (1948) mit James Stewart in einer der Hauptrollen. Der Film blieb jedoch vor allem wegen eines anderen Hitchcock-Experiments in Erinnerung; jede Einstellung des kammerspielartigen Films dauert so lange, wie es das Filmmaterial in der Kamera erlaubte, also rund zehn Minuten. Durch geschickte Übergänge sollte so der Eindruck entstehen, dass sich die Geschichte in Echtzeit und von nur einer Kamera gefilmt ereignete.
"Sklavin des Herzens" (1949), ein für Hitchcock untypischer, melodramatischer Kostümfilm, war vor allem ein Vehikel für Ingrid Bergman. Trotz der Starbesetzung und der technischen Raffinessen wurde er kommerziell ein ähnlicher Misserfolg wie "Cocktail für eine Leiche" - Transatlantic ging daraufhin in Konkurs.
Nachdem sein Berater und Agent Myron Selznick 1944 gestorben war, wurden Hitchcocks Interessen von mehreren anderen Personen wahrgenommen, bevor er 1948 mit Lew Wasserman zusammentraf. Wasserman war seit 1946 Präsident der weltgrößten Künstleragentur Music Corporation of America (MCA), der sich Hitchcock 1948 anschloss. Es begann eine enge wie äußerst lohnende Zusammenarbeit.
Hitchcock schloss mit Warner Bros. einen lukrativen Vertrag über vier Filme ab, bei denen er als Regisseur und Produzent, angefangen bei der Stoffauswahl, völlig freie Hand hatte. Der erste dieser Filme war der Thriller "Die rote Lola" (1950) mit Marlene Dietrich, der im Londoner Theatermilieu spielte. Eines seiner Lieblingsmotive stellte er auf den Kopf; am Ende entpuppt sich der „unschuldig Verfolgte“ als der wahre Mörder. Hitchcock drehte in seiner Heimat, spürte allerdings wieder die alten Ressentiments, die nach seiner Auswanderung entstanden waren. Der Film selbst war nicht sonderlich erfolgreich.
Im April 1950 begann Hitchcock, regelmäßige Kolloquien an den Universitäten von Kalifornien und Südkalifornien abzuhalten, in denen unter anderem Previews seiner aktuellen Filme gezeigt wurden. Diese Tradition sollte er die kommenden 20 Jahre beibehalten.
"Der Fremde im Zug" (1951, nach einem Roman von Patricia Highsmith) brachte schließlich nach fünf Jahren Flaute wieder einen überragenden Erfolg. Mit diesem Film begann die dreizehnjährige Zusammenarbeit mit dem Kameramann Robert Burks. Wie schon in "Die rote Lola" spielte Hitchcocks Tochter Patricia eine Nebenrolle. 1952 folgte mit "Ich beichte" der eindeutigste filmische Bezug auf Hitchcocks starke katholische Prägung. Obwohl von der Kritik geschätzt, floppte der Film an den Kinokassen, was Hitchcock vor allem der Humorlosigkeit des Publikums anlastete.
Als Anfang der 1950er Jahre das Fernsehen Einzug in die Wohnzimmer hielt, versuchte die Kinoindustrie, mit neuen technischen Verfahren wie dem Breitbildformat Cinemascope oder dem 3D-Verfahren den Zuschauerschwund aufzuhalten. So drängte Warner Bros. Hitchcock, seinen nächsten Film in 3D zu drehen. Über diese Entscheidung, die zur Einschränkung der Bewegungsfreiheit der Kamera führte, war Hitchcock nicht glücklich; er setzte auch nur wenige explizite 3-D-Effekte ein. "Bei Anruf Mord" (1954) ist die Verfilmung eines damals sehr populären Theaterstücks von Frederick Knott, der auch das Drehbuch schrieb. Mit Hauptdarstellerin Grace Kelly drehte Hitchcock im Anschluss noch zwei weitere Filme, ehe sie sich aus dem Filmgeschäft zurückzog.
Die Erfahrung mit dem aufgezwungenen 3D-Verfahren zeigte Hitchcock die Grenzen bei Warner Brothers. Er schloss daher 1953 einen Vertrag mit Paramount ab, der ihm völlige künstlerische Freiheit garantierte. 1954 begann die für Hitchcock erfolgreichste Zeit mit "Das Fenster zum Hof". Neben Grace Kelly ist ein weiteres Mal James Stewart zu sehen. Die Hauptfigur sitzt während des gesamten Films im Rollstuhl und beobachtet durch ein Teleobjektiv das Geschehen in den gegenüberliegenden Wohnungen – sozusagen stellvertretend für den Zuschauer, aber auch stellvertretend für Hitchcock selbst, der in diesem Film den voyeuristischen Aspekt des Filmemachens aufzeigt.
"Über den Dächern von Nizza" (1955) ist ein leichter, romantischer Thriller, in dem neben Grace Kelly – nach zwei Jahren Filmpause – wieder Cary Grant spielte. Wohl um dem Glamour dieses an der Côte d’Azur angesiedelten Films etwas entgegenzusetzen, drehte Hitchcock noch im selben Jahr die kostengünstig produzierte schwarze Komödie "Immer Ärger mit Harry," in der Shirley MacLaine neben John Forsythe ihren ersten Filmauftritt hatte. Edmund Gwenn, der bereits in früheren Hitchcock-Filmen mitgewirkt hatte, spielte fast achtzigjährig eine seiner wenigen Hauptrollen. Obwohl Hitchcock in vielen seiner Filme schwarzen Humor untergebracht hat, ist es eine der wenigen echten Komödien von ihm.
1955 nahm Hitchcock – rund fünf Jahre nach seiner Frau – die amerikanische Staatsbürgerschaft an. Im selben Jahr begann er mit Doris Day und James Stewart die Dreharbeiten zu "Der Mann, der zuviel wußte" (1956), dem einzigen Remake eines seiner Filme in seiner Karriere. Ebenfalls 1955 startete die wöchentliche Fernsehserie "Alfred Hitchcock Presents" (ab 1962 "The Alfred Hitchcock Hour)". Hitchcock war Produzent, trat in vielen Folgen als Moderator auf und inszenierte insgesamt 18 Folgen. Auch für die Fernsehserien "Suspicion" und "Startime" nahm er für je eine Folge im Regiestuhl Platz. Nach zehn Jahren beendete er seine Fernseharbeit, an der er zunehmend das Interesse verloren hatte. Hinzu kam, dass die Produktion den Auftraggebern zu teuer wurde und die Zeit von Serien mit jeweils abgeschlossenen Folgen, sogenannten „Anthologies“, zu Ende ging.
Mit "Der falsche Mann" wurde er 1956 einem seiner Grundprinzipien, der strikten Trennung von Leben und Fiktion, untreu. In dem Schwarzweißfilm mit Henry Fonda und Vera Miles wird an authentischen Schauplätzen die auf Tatsachen beruhende Geschichte eines zu unrecht Verurteilten erzählt. Der Film entstand noch einmal für Warner Bros., da Hitchcock dem Studio bei seinem Ausscheiden noch einen Film ohne Regiegage zugesagt hatte. Allerdings war "Der falsche Mann", der viele Stilelemente des Film noir und ein trostloses Ende aufweist, kommerziell ein Flop.
1957 drehte Hitchcock seinen letzten Film für Paramount: "Vertigo – Aus dem Reich der Toten" (1958). Das Drehbuch entstand in gemeinsamer intensiver Arbeit von Hitchcock und Samuel A. Taylor, und in wenige seiner Filmfiguren projizierte Hitchcock so viel von seiner eigenen Persönlichkeit wie in den von James Stewart verkörperten Scottie Ferguson, der versucht, eine Frau nach seiner Vorstellung umzuformen. Zu seiner Entstehungszeit nicht besonders erfolgreich, zählt der Film inzwischen – ähnlich wie der folgende "Der unsichtbare Dritte" – zu den bedeutendsten Werken Hitchcocks. Hitchcock und sein Drehbuchautor Ernest Lehman konzipierten "Der unsichtbare Dritte" (1959, MGM) als eine Abfolge von Abenteuern, in der ein Unschuldiger (Cary Grant in seinem letzten Hitchcock-Film) um seine Reputation und sein Leben kämpft. Die elegante Leichtigkeit der Erzählung beeinflusste viele nachfolgende Abenteuer- und Agentenfilme, was sich u. a. in den James-Bond-Filmen der darauf folgenden Jahre zeigt. Für Hitchcock selbst blieb es für lange Zeit der letzte vorwiegend heitere Film.
Das im Anschluss vorbereitete Projekt mit Audrey Hepburn in der Hauptrolle wurde durch Hitchcock gestoppt, als Hepburn wegen einer geplanten Vergewaltigungsszene absagte. Mit seiner bewusst kostengünstigen Produktion "Psycho" (1960) folgte Hitchcocks wohl bekanntester Film: Die in einer Woche Dreharbeit entstandene „Duschszene“ zählt heute zu seinen meistanalysierten Filmszenen. Ungewöhnlich war auch der Tod einer Hauptfigur nach nur einem Drittel des Films. Die zeitgenössischen Kritiken fielen unerwartet barsch aus, doch das Publikum machte "Psycho" zu Hitchcocks größtem kommerziellen Erfolg.
Nachdem zwei angedachte Projekte scheiterten – unter anderem, weil Walt Disney dem "Psycho"-Regisseur die Dreherlaubnis in Disneyland verweigerte – nahm Hitchcock erst Mitte 1961 seinen nächsten Film in Angriff: "Die Vögel" (1963), ein weiterer Horrorfilm, der nicht zuletzt durch seine Dramaturgie und die eingesetzte Tricktechnik – etwa dem Sodium Vapor Process – stilbildend wirkte. So setzt der deutsche Komponist Oskar Sala statt Filmmusik elektronisch erzeugte Geräusche ein. Hitchcock entdeckte die Hauptdarstellerin Tippi Hedren im Werbefernsehen. Obwohl sie keine Filmerfahrung besaß, nahm er sie für die nächsten sieben Jahre unter Vertrag.
"Die Vögel" entstand für Universal, die kurz zuvor teilweise von MCA übernommen worden waren und für die Hitchcock von nun an alle Filme drehen sollte. Lew Wasserman, bis zu diesem Zeitpunkt Agent Hitchcocks, wurde Präsident von Universal und gab seine Agententätigkeit auf. Hitchcock selbst trat seine Rechte an "Psycho" und seiner Fernsehserie ab und wurde im Gegenzug MCAs drittgrößter Aktionär.
Nach "Die Vögel" gibt es in Hitchcocks Werk einen Bruch. Die folgenden drei Filme der 1960er Jahre blieben künstlerisch und kommerziell hinter den vorangegangenen Erfolgen zurück. Konflikte mit seiner Hauptdarstellerin Tippi Hedren prägten die Dreharbeiten so weit, dass er das Gelingen seines nächsten Films "Marnie" (1964) bewusst zu sabotieren schien. Das Psychogramm einer verstörten, traumatisierten Frau bedient sich psychologischer Erklärungsmodelle, die überholt und undifferenziert wirken, und enthält für Hitchcock untypisch viele handwerkliche Fehler.
Dieser erste künstlerische und kommerzielle Misserfolg seit rund fünfzehn Jahren war in mehrfacher Hinsicht ein Wendepunkt in Hitchcocks Karriere. Tippi Hedren war die letzte typische „Hitchcock-Blondine“, "Marnie" der letzte Film, den Hitchcocks langjähriger Kameramann Robert Burks drehte. Kurz nach Abschluss der Dreharbeiten verstarb zudem Hitchcocks Filmeditor George Tomasini, mit dem er zehn Jahre lang zusammengearbeitet hatte.
Filmproduktionen wurden immer aufwändiger, der Erfolg an der Kinokasse immer wichtiger. Diverse Projekte, die Hitchcock reizten und die er mehr oder weniger intensiv plante, kamen so aus Angst der Produzenten nicht zustande – etwa "Mary Rose", die geplante Verfilmung eines skurrilen Theaterstücks. Mit "R.R.R.R.", einem Film mit vielen Verwicklungen über eine italienische Ganoven-Familie in New York, wollte er Jahre nach "Der unsichtbare Dritte" wieder einen komischen Thriller drehen und damit alle Nachahmer "(Charade", "Topkapi" und andere) übertreffen. Das weit fortgeschrittene Projekt scheiterte schließlich an unüberbrückbaren sprachlichen und kulturellen Problemen mit den italienischen Mitarbeitern.
Am 7. März 1965 erhielt Hitchcock für seinen „historischen Beitrag zum amerikanischen Kino“ den "Milestone Award" der "Producers Guild Of America" – die erste von vielen Ehrungen für sein Lebenswerk.
Mit "Der zerrissene Vorhang" (1966) kehrte Hitchcock schließlich zum Genre des Spionagefilms zurück, in dem er bereits in den 1930er Jahren in England große Erfolge gefeiert hatte. Die Premiere dieses 50. Hitchcock-Filmes sollte mit einer groß angelegten Marketingkampagne begleitet werden. Nicht nur aus diesem Grund setzte Universal die aktuellen Stars Paul Newman und Julie Andrews gegen Hitchcocks Widerstand als Hauptdarsteller durch. Überdies kam es zum Bruch mit dem Komponisten Bernard Herrmann, als dieser nicht die von Universal gewünschte, auch für den Schallplattenverkauf geeignete Unterhaltungsmusik vorlegte. Auch an anderen wichtigen Positionen seines Stabes musste Hitchcock auf vertraute Mitarbeiter verzichten. "Der zerrissene Vorhang" fällt handwerklich und dramaturgisch gegenüber Hitchcocks letzten Filmen (einschließlich "Marnie)" deutlich ab und wurde von der Kritik durchweg verrissen.
Universal forderte von ihm zeitgemäßere Themen ein. Als das von ihm und Howard Fast detailliert ausgearbeitete Drehbuch über einen homosexuellen Frauenmörder abgelehnt wurde, zog er sich für ein Jahr ins Privatleben zurück. Anfang 1968 entschloss er sich unter dem Druck der langen Pause seit dem letzten Film und der noch längeren Zeitspanne seit dem letzten Erfolg, den Spionageroman "Topas" von Leon Uris zu verfilmen, dessen Rechte Universal kurz zuvor erworben hatte. "Topas" wurde dann fast ausschließlich mit europäischen Schauspielern besetzt und völlig ohne Hollywood-Stars. Die für amerikanische Zuschauer bekanntesten Gesichter waren der Fernsehschauspieler John Forsythe und der aus Kanada stammende John Vernon. Das endgültige Drehbuch wurde erst während der laufenden Dreharbeiten geschrieben, der Schluss nach einer katastrophalen Preview improvisiert. Publikum und Kritik reagierten mit Ablehnung auf Hitchcocks bis dahin teuersten Film, doch er zeigte sich zuversichtlich: „Ich habe meinen letzten Film noch nicht gedreht. "Topas" ist mein 51. Film, aber wann ich meinen letzten Film drehen werde, ist von mir, meinen Finanziers und Gott noch nicht entschieden worden.“
Im Spätsommer 1970 nahm Hitchcock sein nächstes Projekt in Angriff und reiste dafür wieder in seine Heimat, wo er diesmal begeistert empfangen wurde. "Frenzy" (1972) spielt in London, dem Hitchcock eine liebevolle Hommage erweist, und ist in seinen Worten „… die Geschichte eines Mannes, der impotent ist, und sich deshalb durch Mord ausdrückt“. Zunächst verliefen die Drehbucharbeit und auch die Dreharbeiten, die Hitchcock so ernst nahm wie lange nicht mehr, weitgehend reibungsfrei. Doch als seine Frau Alma einen Herzinfarkt erlitten hatte, wurde Hitchcock müde und untätig; die Crew war, ähnlich wie bei den drei vorangegangenen Filmen, wieder weitgehend auf sich alleine gestellt. Dennoch wurde "Frenzy" ein brutaler, zum Teil bitterer und mit tiefschwarzem britischen Humor durchzogener Film und ein großer Erfolg. Nur in England war man enttäuscht und bemängelte vor allem die anachronistisch wirkende Darstellung von London und des britischen Lebens.
Im Frühjahr 1973 entschloss sich Hitchcock, den Roman "The Rainbird Pattern" von Victor Canning zu verfilmen. Doch die Arbeit am Drehbuch mit Ernest Lehman "(Der unsichtbare Dritte)" ging diesmal nicht mehr so reibungslos vonstatten: Hitchcock war merklich müde geworden, seine körperlichen Schmerzen betäubte er zunehmend mit Alkohol. Zwei Jahre benötigte die Fertigstellung des Drehbuchs, so lange wie nie zuvor in seiner Karriere.
Mit "Familiengrab," wie der Film schließlich hieß, kehrte Hitchcock zum scheinbar heiteren, diesmal jedoch morbid akzentuierten Unterhaltungsthriller zurück. Wie stets legte er Wert auf eine ausgeklügelte Bildsprache, die erneut mit Hilfe von Storyboards erarbeitet wurde.
Die Dreharbeiten gestalteten sich reibungslos und in einer entspannten Atmosphäre. Hitchcock, der sich im Rahmen seiner gesundheitlichen Möglichkeiten mit einem lange nicht gezeigten Elan in die Dreharbeiten einbrachte, zeigte sich zu Neuerungen bereit: Er war offen für Improvisationen seiner Schauspieler und nahm noch während der Dreharbeiten Änderungen am Ablauf vor. Die Überwachung der Schnittarbeiten musste er weitgehend seinen Mitarbeiterinnen Peggy Robertson und Suzanne Gauthier überlassen, da sich sein Gesundheitszustand deutlich verschlechterte. Zudem erlitt Alma einen zweiten Schlaganfall.
"Familiengrab" wurde nach seiner Premiere im Frühjahr 1976 überwiegend freundlich aufgenommen, und Hitchcock schöpfte aus der Sympathie, die ihm entgegenschlug, für kurze Zeit Kraft, neue Filmideen aufzugreifen. Sein erst Anfang 1978 in Angriff genommenes Projekt, die Verfilmung des Romans "The Short Night" von Ronald Kirkbride, wurde aufgrund seines sich weiter verschlechternden Gesundheitszustands jedoch etwa ein Jahr später von Universal gestoppt. Im März 1979 wurde er vom American Film Institute für sein Lebenswerk geehrt. Zwei Monate später schloss er sein Büro auf dem Gelände der "Universal"-Studios. Am 3. Januar 1980 wurde Hitchcock in den britischen Adelsstand erhoben.
Am Morgen des 29. April 1980 starb Alfred Hitchcock in seinem Haus in Los Angeles an Nierenversagen. Sein Körper wurde eingeäschert, die Asche an einem unbekannten Ort verstreut.
In rund fünfzig Jahren hat Alfred Hitchcock dreiundfünfzig Spielfilme als Regisseur begonnen und beendet. Die weitaus größte Zahl dieser Filme gehört dem Genre des Thrillers an und weist ähnliche Erzählmuster und Motive auf, wiederkehrende Elemente, visuelle Stilmittel und Effekte, die sich wie ein roter Faden durch sein Gesamtwerk ziehen.
Das Grundmotiv in Hitchcocks Filmen bildet meist die Angst vor Vernichtung der Existenz vor dem Hintergrund bürgerlicher Werte.
Dabei bezieht sich diese Angst nicht nur auf Mörder, Gangster oder Spione, welche die bürgerliche Ordnung angreifen: Die Hauptfiguren befinden sich sogar häufig in der Situation, auch von Vertretern des Gesetzes bedroht zu werden.
Zu diesem Motiv der Angst kommt – Hitchcocks katholischer Prägung entsprechend – jenes von Schuld und Sühne hinzu. Der unschuldig Verfolgte in seinen Filmen ist „unschuldig, aber nur in Bezug auf das, was man ihm vorwirft.“ Das heißt, die Figur wird durch das, was ihr im Laufe des Filmes widerfährt, im übertragenen Sinne für andere Defizite oder Vergehen bestraft: In "Bei Anruf Mord" etwa wird die Hauptfigur des Mordes verdächtigt; tatsächlich musste sie aus Notwehr töten. Das folgende Unheil kann jedoch als Strafe für ihren begangenen Ehebruch angesehen werden.
Eine Variation dieses Themas ist die Übertragung der Schuld auf eine andere Person. Unschuldige werden zu (Mit-)Schuldigen an Verbrechen anderer, da sie aus persönlichen Gründen nicht zur Aufklärung beitragen können. Zentral sind hierbei die beiden Filme "Ich beichte" und "Der Fremde im Zug", in denen die jeweiligen Protagonisten von Morden, die andere begangen haben, profitieren und, nachdem sie selbst unter Verdacht geraten, keine Möglichkeit haben, sich zu entlasten. In "Vertigo" macht der wahre Mörder die Hauptfigur durch ein Komplott zunächst scheinbar zum Schuldigen am Tod der ihr anvertrauten Person. Später macht sich das Opfer der Intrige tatsächlich am Tod der Frau schuldig, die er liebt.
Falsche Verdächtigungen, aber auch ausgeprägte Schuldkomplexe, gehen bei Hitchcocks Filmen mit der Bedrohung der Identität einher. Seine traumatisierten oder verfolgten Figuren nehmen selbst falsche Namen an oder werden – aus unbekannten Gründen – für jemand anderen gehalten. Das Motiv des Identitätsverlusts spielt Hitchcock, angefangen von seinem ersten bis zu seinem letzten Film, in unterschiedlichsten Varianten durch, besonders einprägsam in "Vertigo": Die weibliche Hauptfigur wird zunächst im Rahmen eines Mordkomplotts in eine andere Person (die anschließend ermordet wird) verwandelt und nimmt daraufhin wieder ihre eigentliche Identität an, nur um anschließend wieder in die andere Person zurückverwandelt zu werden.
Oft stehen Schuld und Bedrohung in Zusammenhang mit sexuellen Aspekten. In "Der Fall Paradin" genügt bereits der Gedanke an Ehebruch, um das Leben des Protagonisten zu gefährden. In "Berüchtigt" ist der Zusammenhang zwischen Sex, Schuld und Bedrohung zentrales Thema. Hitchcocks Verbindung von Sex und Gewalt wird in Mordszenen deutlich, die er oft wie Vergewaltigungen inszeniert, etwa der Schlusskampf zwischen Onkel und Nichte Charlie in "Im Schatten des Zweifels", die Scherenszene in "Bei Anruf Mord" und die Duschszene in "Psycho". Darüber hinaus spielt Sexualität gerade in abnorm empfundenen Erscheinungsformen eine große Rolle in seinem Werk. Aufgrund der Auflagen der Zensur werden jedoch Homosexualität, die in Verbindung mit Schuld und Verderben regelmäßig vorkommt, oder Nekrophilie (in "Vertigo)" nur in einzelnen Gesten oder Schlüsselszenen angedeutet. Auch Fetischismus "(Erpressung", "Vertigo", "Psycho)" und Voyeurismus "(Das Fenster zum Hof", "Psycho)" spielen in seinen Filmen eine gewisse Rolle. In mehreren Filmen wird zudem ein erotischer Bezug der männlichen Hauptfiguren zu ihren Müttern angedeutet, etwa in "Psycho" und "Die Vögel". Zentral in diesem Zusammenhang ist "Berüchtigt". Hier verhalten sich Claude Rains und Leopoldine Konstantin in manchen Schlüsselszenen wie ein Ehepaar. Dieser Eindruck wird durch den geringen Altersunterschied der Schauspieler von nur vier Jahren verstärkt.
Unter den in Hitchcocks Bildsprache verwendeten Symbolen finden sich Vögel als Vorboten des Unglücks (etwa in "Erpressung", später als vorherrschendes Thema in "Die Vögel)", Treppen, die Verlust oder Freiheit bedeuten können "(Berüchtigt", "Psycho", "Vertigo" und andere), sowie Handschellen und andere Fesseln, um Abhängigkeit und Ausgeliefertsein auszudrücken, meist im sexuellen Kontext (Beispiel: "Der Mieter)". Auch Spiegel tauchen bei Hitchcock regelmäßig auf – in Zusammenhang mit dem Verlust oder der Erkenntnis der eigenen Persönlichkeit oder als allgemeines Symbol für Täuschungen (einprägende Beispiele: "Vertigo" und "Psycho").
Die meisten Protagonisten in Hitchcocks Thrillern sind Normalbürger, die zu Beginn der Geschichte in der Regel nichts mit kriminellen Machenschaften zu tun haben. Meist werden sie durch Zufall oder unbekannte Umstände in geheimnisvolle und bedrohliche Vorgänge gezogen. Dem Zuschauer wird so das beunruhigende Gefühl vermittelt, dass auch er jederzeit in derartige Situationen geraten könnte. Professionelle Agenten oder Spione findet man dagegen nur selten unter den Hauptfiguren, obwohl Hitchcock viele Filme drehte, die im Agentenmilieu spielen. Hitchcock drehte bis auf eine Ausnahme ("Erpressung", 1929) auch nie einen Film, in dem die Arbeit der Polizei im Mittelpunkt steht; aktive Polizisten tauchen ansonsten nur als Nebenfiguren und üblicherweise als Hindernis auf.
Der Prototyp des Antihelden bei Hitchcock sind die von James Stewart gespielten Figuren: In "Cocktail für eine Leiche" muss der von Stewart dargestellte Lehrer erkennen, dass zwei seiner Studenten eine seiner Theorien zum Anlass nahmen, einen Mord zu verüben und diesen mit seinen Thesen zu rechtfertigen; am Ende steht er hilflos vor diesem menschlichen Abgrund, in den er nicht nur hineingezogen wurde, sondern den er sogar mit heraufbeschworen hat. In "Das Fenster zum Hof" stellt Stewart eine Figur dar, die bindungsscheu sowie körperlich beeinträchtigt und voyeuristisch veranlagt ist und dadurch in Schwierigkeiten kommt.
Es gibt nur wenige positive, ungebrochene Helden bei Hitchcock. Ein Schauspieler, der diesen seltenen Rollentypus verkörperte, war Cary Grant in "Über den Dächern von Nizza" und in "Der unsichtbare Dritte". Diese Figuren meistern die Herausforderungen zwar mit Charme und Leichtigkeit, doch stehen sie in Verdacht, kriminell zu sein beziehungsweise verlieren sie zeitweise die Kontrolle, womit selbst sie keine gänzlich unantastbaren Helden sein können. Aber sogar Cary Grant spielt in zwei seiner Hitchcock-Filme Figuren, deren Schattenseiten sich zeitweise vor deren positive Merkmale schieben.
Im Laufe der Karriere Hitchcocks gewinnen ambivalente oder gar negativ gezeichnete Hauptfiguren immer stärker an Gewicht. Diese "Antihelden" weisen physische oder psychische Probleme auf, sind Verlierertypen oder unsympathisch. Durch ihr obsessives Fehlverhalten wirken sie schwach und können Schaden anrichten. Diese Figuren dienen zwar kaum als Vorbild, doch soll deren ambivalente Persönlichkeit dazu beitragen, dass sich der Zuschauer in ihnen wiederfinden kann.
In vielen Filmen bedient Hitchcock auf den ersten Blick das klassische Motiv der schwachen, zu beschützenden Frau. Doch während das Klischee verlangt, dass der strahlende Held sie rettet, ist sie bei Hitchcock oft auf sich alleine gestellt. In einigen Fällen ist der vermeintliche Beschützer schwach oder zu sehr mit sich selbst beschäftigt, als dass er der bedrohten Frau helfen könnte, wie zum Beispiel Ingrid Bergman und Cary Grant in "Berüchtigt". In anderen Fällen geht von der männlichen Hauptfigur (in der Regel dem Ehemann) sogar ein tatsächliches oder vermeintliches Bedrohungspotential aus. Klassische Beispiele: Joan Fontaine und Cary Grant in "Verdacht" sowie Grace Kelly und Ray Milland in "Bei Anruf Mord".
Die Rollenverteilung zwischen Mann und Frau kehrt Hitchcock in einigen Filmen gänzlich um: Die Frau ist dem Mann, der zunehmend passiver wird, überlegen und wendet das Geschehen zum Guten. Beispiele sind "Jung und unschuldig" (die Tochter des Polizeichefs verhilft einem Verdächtigen zur Flucht und löst letztendlich den Fall), "Ich kämpfe um dich" (eine Psychologin dringt in das Unterbewusste des Mordverdächtigen ein und rettet ihn vor der sicheren Verurteilung) sowie "Der Mann, der zuviel wußte" (die Ehefrau verhindert zuerst einen geplanten Mord und rettet dann das eigene Kind vor den Verbrechern).
Der Typ, der sich dabei im Laufe der Zeit herauskristallisierte, ist jener der jungen, schönen, kühlen, hintergründigen und undurchsichtigen Blondine. Die oberflächliche Kühle der Hitchcock-Blondine verbirgt jedoch eine stark entwickelte Sexualität. Besonders deutlich wird dies in "Der unsichtbare Dritte", wenn Eva Marie Saint zunächst gegenüber Cary Grant zweideutige Bemerkungen macht, dann plötzlich den völlig überraschten Fremden küsst und ihn ohne Zögern in ihrem Schlafwagenabteil unterbringt. Nicht der Mann, sondern die (blonde) Frau spielt hier den aktiven Part und zeigt damit die Fragilität des männlichen, bürgerlichen Weltbildes.
Hitchcock legt durch seine Gestaltung von Figuren und Dramaturgie dem Zuschauer eine Identifikation mit dem Schurken nahe.
Seine Antagonisten wirken zuweilen auffällig sympathisch und übertreffen mitunter die Ausstrahlung der Hauptfiguren.
Oft konkurrieren Held und Bösewicht um dieselbe Frau; die Liebe des Gegenspielers erscheint dabei tiefer und aufrichtiger als die des Helden. Besonders auffällig ist dies in "Berüchtigt" (Claude Rains gegenüber Cary Grant) und in "Der unsichtbare Dritte" (James Mason wiederum gegenüber Cary Grant). Selbst ein ausgesprochen heimtückischer Schurke wie Ray Milland in "Bei Anruf Mord" wirkt in einzelnen Momenten gegenüber dem unbeholfenen Robert Cummings sympathischer, in jedem Fall jedoch gegenüber der Polizei vertrauenserweckender. Oft sind sie die eigentlichen Hauptfiguren, wie Joseph Cotten als charmanter Witwenmörder in "Im Schatten des Zweifels" oder Anthony Perkins als linkischer, von seiner Mutter gepeinigter Mörder in "Psycho".
In vielen seiner Filme – ab Mitte der 1940er Jahre regelmäßig – tauchen dominante Mütter auf, die einen beunruhigenden Einfluss auf ihre meist erwachsenen Kinder ausüben und zum Teil Auslöser oder Ursache dramatischer Ereignisse sind. Erstmals uneingeschränkt bösartig erscheint die Mutter in "Berüchtigt" (1946), die ihren Sohn zum Mord an der Frau, die er liebt, antreibt.
Der extremste Fall tritt in "Psycho" (1960) zutage, wo die tote Mutter noch von ihrem Sohn Besitz ergreift und ihn zu ihrem mordenden Werkzeug werden lässt. Daneben gibt es eine Vielzahl weniger dämonischer Variationen, wobei Besitzergreifung allen Mutter-Typen gemein ist:
In "Die Vögel" (1963) erträgt es die Mutter von Mitch (Jessica Tandy) nicht, dass ihr erwachsener Sohn (Rod Taylor) sich für eine andere Frau interessiert. In "Marnie" (1964) wird das Leben der Tochter durch einen von der Mutter übertragenen Schuldkomplex beinahe zerstört.
In zwei Filmen variiert Hitchcock dieses Rollenmuster: In "Rebecca" und "Sklavin des Herzens" übernehmen Haushälterinnen die Funktion der dämonischen Mutter.
Üblicherweise positiv besetzte Figuren wie Polizisten, Richter oder andere Vertreter des Staates erscheinen oft zwiespältig: Sie sind nicht in der Lage, die Helden zu beschützen, oder stellen sogar eine Bedrohung für diese dar. Polizisten verdrehen das Recht, sie handeln aus persönlichen Motiven, sie glauben dem ersten Anschein und schützen den tatsächlich Schuldigen aufgrund dessen vordergründig tadellosen Erscheinens, sie sind tollpatschig oder arbeiten schlampig. Dieses Rollenmuster durchzieht Hitchcocks gesamtes Werk, von "Der Mieter" (1927) bis "Frenzy" (1972).
Darüber hinaus finden sich vereinzelt Geheimdienstmitarbeiter unter den Nebenfiguren, die sich als Gegner (das Ehepaar Drayton in "Der Mann, der zuviel wußte" (1956)) oder als Helfer offenbaren, wobei auch letztere Schwierigkeiten bringen können – beispielsweise der „General“ (Peter Lorre) in "Geheimagent" oder Leo G. Carroll als CIA-Mitarbeiter in "Der unsichtbare Dritte". Indem die feste Trennlinie zwischen Gut und Böse verschwimmt, soll beim Zuschauer das Gefühl der Verunsicherung gesteigert werden.
Hitchcocks Credo lautete: „For me, the cinema is not a slice of life, but a piece of cake.“ („Für mich ist das Kino keine Scheibe vom Leben, sondern ein Stück vom Kuchen.“) Film war für ihn eine artifizielle Kunstform. Nur einmal – in "Der falsche Mann" – wich er von diesem Grundsatz ab. Aber auch hier liegt der Akzent auf jenen Elementen, die nicht dokumentarisch sind – etwa der subjektiven Perspektive des unschuldig Verdächtigten und seiner hilflosen Frau. Einigen seiner weiteren Filme liegen zwar auch reale Ereignisse zugrunde ("Der zerrissene Vorhang", "Das Fenster zum Hof", "Der Auslandskorrespondent" oder "Cocktail für eine Leiche"), doch werden diese so weit fiktionalisiert, dass außer dem Grundmotiv kein Bezug zu der ursprünglichen Geschichte übrig bleibt.
Eine nicht verwirklichte Idee für "Der unsichtbare Dritte", die Hitchcock im Interview mit Truffaut erwähnt, verdeutlicht Hitchcocks Vorstellungen davon, die Realität zu transzendieren: Er wollte zeigen, wie unter den Augen Cary Grants auf einem Fließband ein Auto zusammengebaut wird und anschließend aus dem fertiggestellten Auto eine Leiche fällt – nach realistischen Maßstäben unmöglich. Doch Hitchcocks Begründung für das Verwerfen der Idee zeigt, dass er sich in solchen Fragen nicht an der Wahrscheinlichkeit orientierte: „Wir haben die Idee in der Geschichte nicht richtig unterbringen können, und selbst eine willkürliche Szene kann man nicht ohne Motiv ausführen.“ Den Vorwurf, Gesetze der Plausibilität zu missachten, nahm er bewusst in Kauf: „Wenn man alles analysieren wollte und alles nach Erwägungen der Glaubwürdigkeit und Wahrscheinlichkeit konstruieren, dann würde keine Spielfilmhandlung dieser Analyse standhalten, und es bliebe einem nur noch eines übrig: Dokumentarfilme zu drehen.“ Hitchcock vertraute darauf, dass die Zuschauer unwahrscheinliche Details akzeptieren würden, da er diese nur verwendete, um die Handlung zu dramatisieren, voranzutreiben oder zu straffen.
Für bewusste Irritation sorgte auch Hitchcocks Spiel mit filmtypischen Klischees. So vermied er es insbesondere bei den Nebenrollen, Schauspieler nach festgelegtem Typ zu besetzen. Zu früheren Zeiten war es außerdem üblich, dass Schurken Schnurrbärte trugen. Auch bei der Wahl seiner Spielorte entzog sich Hitchcock den Genre-Gesetzen. So ließ er Verbrechen und bedrohliche Szenen häufig nicht in unheimlichen, dunklen Räumen stattfinden, sondern bei hellem Tageslicht und an scheinbar harmlosen Orten wie einem mit Menschen übersäten Marktplatz "(Der Mann, der zuviel wußte" (1956) und "Der Auslandskorrespondent)", in einer menschenleeren Landschaft, auf einer öffentlichen Versteigerung und in einer Hotelhalle "(Der unsichtbare Dritte)", auf einer idyllischen Bergstraße "(Über den Dächern von Nizza)", auf einer Party ("Berüchtigt" und "Jung und unschuldig") in einer voll besetzten Konzerthalle (beide "Der Mann, der zuviel wußte") oder in einem mit lauter freundlichen Menschen besetzten Eisenbahnzug "(Eine Dame verschwindet)".
Die klassische, auf das Überraschungsmoment aufbauende Form des Kriminalfilms ist der Whodunit. Bis auf nur wenige Ausnahmen bediente sich Hitchcock jedoch einer anderen Form des Spannungsaufbaus, dem sogenannten Suspense: Dem Zuschauer sind ab einem gewissen Zeitpunkt Informationen oder Umstände bekannt, von denen die handelnden Personen nichts wissen.
Er fiebert in besonderer Weise mit den Helden, er sieht Ereignisse kommen, möchte den Figuren helfen, kann es aber nicht. In einigen Filmen wird das klassische Suspense dahingehend variiert, dass handelnde Personen die Rolle des Zuschauers übernehmen. Ein Beispiel von vielen: In "Das Fenster zum Hof" dringt Lisa in die Wohnung des verdächtigen Nachbarn ein, um nach Beweisen für einen möglichen Mord zu suchen. Ihr Partner Jeff beobachtet das Geschehen von der gegenüber liegenden Wohnung aus und sieht dabei den Nachbarn vorzeitig zurückkommen. Er vermutet sie in Lebensgefahr, kann ihr aber nicht helfen.
Für einige markante Szenen baute Hitchcock zudem bewusst eine Suspense-Situation auf, um den Zuschauer mit einem umso gewaltigeren Überraschungseffekt schockieren zu können. Ein berühmtes Beispiel findet sich in "Psycho": Zum einen ist Marion Crane mit verschiedenen Insignien einer typischen Hauptfigur eines Hitchcockfilms ausgestattet, so dass kaum jemand erwartet, dass sie bereits in der ersten Hälfte des Films stirbt. Zum anderen schaltet Hitchcock der Duschszene selbst einen Suspense-Moment vor. Norman Bates beobachtet Marion Crane durch ein Loch in der Wand beim Entkleiden. Sie geht unter die Dusche. Der Zuschauer wird nun eben keinen Mord, sondern schlimmstenfalls eine Vergewaltigung durch Norman befürchten. Der bestialische Mord ist somit völlig überraschend und damit ein Grund für die Berühmtheit der Szene.
Ein von Hitchcock in seinen Thrillern sehr häufig verwendetes Mittel war der sogenannte MacGuffin: ein Element, das die Handlung vorantreibt oder sogar initiiert, obwohl es für die Entwicklung der Figuren und für den Zuschauer inhaltlich völlig bedeutungslos, geradezu austauschbar ist. Der MacGuffin in "Der unsichtbare Dritte" sind schlicht „Regierungsgeheimnisse“, über die der Held oder der Zuschauer nie mehr erfährt. In "Psycho" benutzt Hitchcock unterschlagenes Geld, das die Sekretärin zur Flucht treibt und so in „Bates Motel“ führt, um das Publikum anfangs gezielt in die Irre zu führen und für einen Kriminalfall zu interessieren, der mit der eigentlichen Handlung nur am Rande zu tun hat. Die mysteriösen „39 Stufen“ im gleichnamigen Film sind eine Geheimorganisation, über die bis kurz vor Ende des Films überhaupt nichts bekannt ist, außer dass sie gefährlich ist. Ein besonders außergewöhnlicher MacGuffin ist die als Volksliedmelodie getarnte Geheimdienstinformation aus Eine Dame verschwindet.
Beeinflusst vom Stummfilm beruhte Hitchcocks Filmverständnis auf dem Anspruch, alles Wichtige in seinen Filmen visuell und so wenig wie möglich durch Dialoge auszudrücken. Seine typischen Kameraeinstellungen geben im Bild genau das wieder, was für das Verständnis der Szene wesentlich ist – auch um dem Zuschauer nicht die Möglichkeit zu geben, sich durch unwesentliche Details ablenken zu lassen. So wirken beispielsweise Kussszenen bei Hitchcock immer sehr intim, da er gewöhnlich mit der Kamera sehr nahe an die beiden sich Küssenden heranfuhr und den Zuschauer sozusagen zum dritten Umarmenden zu machen. Zu den berühmtesten Beispielen dieser visuellen Erzählweise zählen die Duschszene aus "Psycho", der Flugzeugangriff auf Cary Grant und die Jagd auf Mount Rushmore in "Der unsichtbare Dritte", die Versammlung der Vögel auf dem Klettergerüst in "Die Vögel" oder die zehnminütige Konzertszene in der Royal Albert Hall in "Der Mann, der zuviel wußte" von 1956.
Hitchcocks visueller Arbeitsstil drückt sich unter anderem in den Expositionen vieler seiner Filme aus. Er bringt den Zuschauern die handelnden Figuren und die Umstände der folgenden Handlung ohne die Verwendung von Dialogen nahe. Die Länge dieser Einführungen variierte zwischen wenigen Sekunden und mehreren Minuten. Erstmals verfolgte er diese Technik 1929 in seinem ersten Tonfilm "Erpressung".
Zudem tauchen in Hitchcocks Filmen immer wieder ungewöhnliche filmische Operationen auf, um die Stimmung und Spannung bewusst zu verstärken, beispielsweise eine gegenläufige Zoom-Fahrtbewegung in "Vertigo" (später auch als "Vertigo-Effekt" bezeichnet), lange Kamerafahrten wie die aus einer Totale eines großen Raums bis in die Naheinstellung eines Schlüssels in einer Hand (in "Berüchtigt") oder auf ein zuckendes Auge (in "Jung und unschuldig") sowie die aus ungefähr siebzig Einstellungen bestehende fünfundvierzig Sekunden lange Mordszene unter der Dusche in "Psycho", unmittelbar gefolgt von einer etwa einminütigen Kamerafahrt ohne einen einzigen Schnitt. Der Production Designer Robert Boyle, mit dem Hitchcock bei fünf Filmen zusammenarbeitete, meinte: „Keiner der Regisseure, mit denen ich je zusammengearbeitet habe, wusste soviel über Film wie er. Viele der Regisseure, mit denen ich gearbeitet habe, wussten eine ganze Menge, aber sie besaßen nicht die technischen Fähigkeiten, die er hatte. Er suchte immer nur den visuellen Ausdruck, und so etwas wie eine zufällige Einstellung gab es bei ihm nicht.“
Nur einmal griff Hitchcock aus Experimentierfreude auf einen filmtechnischen Kniff zurück, der sich nicht unmittelbar aus der Dramaturgie ergab. In "Cocktail für eine Leiche" (1948) drehte er bis zu zehn Minuten lange Einstellungen, die er zum großen Teil sogar über unsichtbare Schnitte ineinander übergehen ließ. Er wollte damit bei dieser Theaterverfilmung die Einheit von Zeit und Raum dokumentieren. Später gab er zu, dass es ein Fehler war, damit gleichzeitig den Schnitt als wesentliches gestaltendes Instrument der Dramaturgie aus der Hand gegeben zu haben.
Inspiriert von amerikanischen und deutschen Filmemachern, setzte Hitchcock schon bei seinen ersten Filmen Licht- beziehungsweise Schatteneffekte ein. Typisch für Hitchcock sind Linien und Streifen in Form von Schatten (durch Gitter, Jalousien oder ähnliches verursacht), die vor allem auf Gesichter fallen und die unheilvolle Atmosphäre verstärken sollen. Darüber hinaus verwendet er in einzelnen Szenen sehr starke, zum Teil unnatürlich wirkende Kontraste, um einen äußeren oder inneren Gut-Böse-Gegensatz zu visualisieren.
Dieses Hell-Dunkel-Spiel unterstützte er durch die Kostüme der Figuren. So ließ er Ingrid Bergman am Anfang von "Berüchtigt" gestreifte Kleidung tragen, um ihre Zerrissenheit zu unterstreichen. In "Der Mieter" trug Ivor Novello zu Beginn Schwarz, später, um seine Unschuld auch nach außen hin deutlich zu machen, Weiß. Die Methode, durch die Farbgebung der Kostüme den emotionalen Zustand der Figuren zu unterstreichen, behielt Hitchcock auch für die Farbfilme bei. In "Bei Anruf Mord" wurden die Kostüme von Grace Kelly mit der Dauer des Films immer trister und grauer, entsprechend ihrer inneren Gemütsverfassung.
Zu Hitchcocks Farbwahl von Grace Kellys Kleidern in "Das Fenster zum Hof" sagte die Kostümbildnerin Edith Head: „Für jede Farbe und jeden Stil gab es einen Grund; er war sich seiner ganzen Entscheidung absolut sicher. In einer Szene sah er sie in blassem Grün, in einer anderen in weißem Chiffon, in einer weiteren in Gold. Er stellte im Studio tatsächlich einen Traum zusammen.“ In seinen späteren Filmen, allen voran "Marnie" und "Vertigo", gab es eine ausgefeilte, die Kostüme, die Dekors und die Beleuchtung umfassende Farbdramaturgie.
Nach Hitchcocks Filmverständnis schafft sich der Film seine eigene Realität und soll oder darf kein Abbild des wahren Lebens sein. Die Nutzung sämtlicher Möglichkeiten, genau das wiederzugeben, was der Regisseur sich vorstellt, ist nach diesem Verständnis nicht nur legitim, sondern erforderlich. Hitchcock hat die Entwicklung der Tricktechnik aufmerksam beobachtet und schon sehr früh – gelegentlich zum Missfallen seiner Produzenten – neue Trickverfahren eingesetzt, zum Beispiel das Schüfftan-Verfahren (bei "Erpressung") oder das Matte Painting. In seinen englischen Thrillern, vor allem in "Nummer siebzehn" und "Jung und unschuldig", arbeitete Hitchcock bei Verfolgungsjagden oft und erkennbar mit Modellen. In "Eine Dame verschwindet" sind die Rückprojektionen während der Zugfahrt aber bereits so ausgereift, dass sie noch Jahrzehnte später überzeugen. Ähnliches gilt für die Schlussszene von "Der Fremde im Zug", in der zwei Männer auf einem sich immer schneller drehenden Karussell kämpfen – in einer virtuosen Kombination von Realeinstellungen, Modellen und Rückprojektionen. "Die Vögel" (1963) beinhaltet rund vierhundert Trickeinstellungen, für die Hitchcock auf sämtliche damals verfügbaren Tricktechniken zurückgriff, unter anderem auch auf das ansonsten für Animationsfilme verwendete Rotoskopieverfahren.
Hitchcock hat seit dem Aufkommen des Tonfilms Musik und Toneffekte eingesetzt, um die Dramaturgie bewusst zu unterstützen. Den Umgang Hitchcocks mit dem Medium Ton beschrieb die Schauspielerin Teresa Wright "(Im Schatten des Zweifels)" folgendermaßen: „Wenn ein Schauspieler mit den Fingern trommelte, war das nicht ein zweckloses Trommeln, es hatte einen Rhythmus, ein musikalisches Muster – es war wie ein Geräusch-Refrain. Ob jemand nun ging oder mit Papier raschelte oder einen Umschlag zerriss oder vor sich hin pfiff, ob das Flattern von Vögeln oder ein Geräusch von draußen war, alles wurde sorgfältig von ihm orchestriert. Er komponierte die Toneffekte wie ein Musiker Instrumentenstimmen.“ Gegenüber Truffaut erwähnte Hitchcock, dass er nach dem Endschnitt eines Films seiner Sekretärin ein „Tondrehbuch“ diktiert, das alle von ihm gewünschten Geräusche enthält.
In "Mord – Sir John greift ein!" (1930) hat er, da ein nachträgliches Bearbeiten der Tonspur zu diesem Zeitpunkt technisch noch nicht möglich war, ein komplettes Orchester hinter den Kulissen versteckt, um die entsprechenden Stellen musikalisch zu untermalen. Weitere klassische Beispiele für Hitchcocks dramaturgischen Musikeinsatz sind "Geheimagent" (1936, der Dauerakkord des toten Organisten in der Kirche), "Eine Dame verschwindet" (1938, die Melodie mit dem Geheimcode und der „Volkstanz“), "Im Schatten des Zweifels" (1943, der „Merry-Widow“-Walzer), "Der Fremde im Zug" (1951, die Szenen auf dem Rummelplatz) und "Das Fenster zum Hof" (1954, die im Laufe des Films entstehende Komposition des Klavierspielers). In "Der Mann, der zuviel wußte" (1956) schließlich wird Musik, sowohl orchestral wie auch gesungen, aktiv inszeniert und dramaturgisch eingebunden: Sie spielt eine wesentliche Rolle in der Gesamtdramaturgie des Films.
Die Musik der Filme aus den späten 1950er und frühen 1960er Jahren, der Zeit als Hitchcock mit dem Komponisten Bernard Herrmann zusammenarbeitete, ist tragendes Element der jeweiligen Filme. Kritiker bescheinigen der Musik der Filme "Vertigo", "Der unsichtbare Dritte" und "Psycho" sowie den Toneffekten von Oskar Sala zu "Die Vögel" wesentlich zum jeweiligen Gesamteindruck des Films beizutragen.
Hitchcock war beeindruckt von den Filmen, die er in seiner Jugend und in seinen frühen Jahren im Filmgeschäft sah, etwa jene von D. W. Griffith, Charlie Chaplin, Buster Keaton und Douglas Fairbanks senior. Als Stummfilmregisseur in England übernahm er vom US-Film unter anderem die Technik, mit Hilfe von Beleuchtungseffekten Tiefe zu schaffen und den Vorder- vom Hintergrund abzusetzen, was bis in die 1930er Jahre im britischen Film unüblich war.
Angetan war er auch von den deutschen Stummfilmregisseuren wie Fritz Lang und Ernst Lubitsch.
F. W. Murnaus "Der letzte Mann", dessen Dreharbeiten Hitchcock 1922 in München beobachtete, bezeichnete er später als den fast perfekten Film: „Er erzählte seine Geschichte ohne Titel; Von Anfang bis Ende vertraute er ganz auf seine Bilder. Das hatte damals einen ungeheueren Einfluss auf meine Arbeit.“ Einfluss auf Hitchcocks Arbeit hatte auch "Das Cabinet des Dr. Caligari", den Robert Wiene 1919 drehte. Die Betonung des Visuellen im deutschen Expressionismus prägte seinen eigenen Umgang mit filmischen Mitteln.
Abgesehen von diesen stilistischen Einflüssen vermied es Hitchcock jedoch, Szenen oder Einstellungen bekannter Filme zu zitieren. Als Ausnahme kann "Panzerkreuzer Potemkin" (1925) des sowjetischen Regisseurs Eisenstein angesehen werden. In "Die 39 Stufen", "Über den Dächern von Nizza" und einigen weiteren Filmen erinnern die vor Entsetzen schreienden Frauen an Einstellungen aus der berühmten und oft zitierten Szene an der Hafentreppe in Odessa.
Es gibt außerdem in Hitchcocks Werk aus den 1940er und 1950er Jahren einige motivische und visuelle Überschneidungen mit der Gattung des Film noir, die den amerikanischen Kriminalfilm in jener Zeit bestimmte, etwa in "Im Schatten des Zweifels" und "Berüchtigt", und besonders in "Der falsche Mann", wo das Motiv der allgegenwärtigen Bedrohung der Hauptfiguren eine Rolle spielt. Darüber hinaus bediente er sich gerne einer ähnlichen, kontrastreichen Bildgestaltung, die er sich in den Grundzügen allerdings bereits in den 1920er Jahren angeeignet hatte.
Auch "Vertigo" erinnert in der Grundkonstellation und der alptraumhaften Zwanghaftigkeit der Geschehnisse an einige Filme des Genres, wie zum Beispiel "Frau ohne Gewissen", hebt sich jedoch formal und stilistisch deutlich vom Film noir ab.
Als typischer Vertreter des Genres kann Hitchcock jedenfalls nicht angesehen werden.
Seine Vorliebe für Blondinen erklärte Hitchcock gegenüber Truffaut wie folgt: „Ich finde, die englischen Frauen, die Schwedinnen, die Norddeutschen und die Skandinavierinnen sind interessanter als die romanischen, die Italienerinnen und die Französinnen. Der Sex darf nicht gleich ins Auge stechen. Eine junge Engländerin mag daherkommen wie eine Lehrerin, aber wenn Sie mit ihr in ein Taxi steigen, überrascht sie Sie damit, dass sie Ihnen in den Hosenschlitz greift.“ Ähnlich äußerte er sich 1969 gegenüber "Look" über die Truffaut-Schauspielerin Claude Jade, die bei ihm in "Topaz" gespielt hatte: „Claude Jade ist eine eher ruhige junge Dame, doch für ihr Benehmen auf dem Rücksitz eines Taxis würde ich keine Garantie übernehmen“
Dass Hitchcock zu seinen jungen blonden Schauspielerinnen ein besonderes Verhältnis hatte und ihnen mehr Aufmerksamkeit widmete als allen anderen, war schon früh bekannt.
Die Sorgfalt, mit der Hitchcock bereits in den 1930er und 1940er Jahren Madeleine Carroll, Carole Lombard und insbesondere Ingrid Bergman in Szene setzte, entwickelte sich mit der Zeit zu einer sich steigernden Verquickung privater und beruflicher Interessen, die sich zu einer Obsession ausweitete. Mit Vera Miles probte er die Nervenzusammenbrüche, welche sie in "Der falsche Mann" darstellen sollte, wochenlang jeweils mehrere Stunden täglich. Für sie wie für Kim Novak ließ er von der Kostümbildnerin eine komplette Garderobe schneidern, die für ihr privates Leben gedacht war. Tippi Hedren ("Die Vögel") ließ er sogar von zwei Crew-Mitgliedern beschatten und begann, ihr Vorschriften für ihr Verhalten im Privatleben zu machen.
Diese Vereinnahmung hatte ihren Höhepunkt in sich über Tage hinziehenden Aufnahmen von auf sie einstürzenden, echten Vögeln. Nach einem eindeutigen, erfolglosen Annäherungsversuch während der Arbeiten zu "Marnie" kam es schließlich zum Bruch. Die zuvor offen bekundete Zuneigung schlug ins Gegenteil um, und Hitchcock ließ keine Gelegenheit aus, Tippi Hedren bei anderen herabzusetzen. Sie blieb die letzte typische „Hitchcock-Blondine“. Zwar hielt sich Hitchcock darüber stets äußerst bedeckt, doch es gilt als gesichert, dass der Regisseur sich von diesen Schwierigkeiten lange nicht erholen konnte und in seiner kreativen Schaffenskraft beeinträchtigt war.
Ebenso gelten Filme wie "Vertigo" und "Berüchtigt", aber auch "Marnie" oder "Im Schatten des Zweifels", die von neurotischen Männern handeln, die Frauen manipulieren, als stark autobiografisch.
Auch die Verbindung zwischen Sex und Gewalt faszinierte Hitchcock, was vor allem in seinen späteren Werken immer deutlicher zutage tritt. Mehrfach inszenierte er vollendete oder versuchte Vergewaltigungen (schon früh in "Erpressung", später dann in "Marnie" und "Frenzy"). In drei nicht realisierten Projekten sollten Vergewaltiger oder Vergewaltigungen eine zentrale Rolle spielen. Morde inszenierte er einige Male als Vergewaltigungen, mit dem Messer als Phallus-Symbol.
Doch auch der Tod durch Erwürgen oder Strangulieren übte eine gewisse Faszination auf ihn aus. Einige Würgeszenen gehören zu den bemerkenswertesten Mordszenen seiner Karriere, etwa in "Cocktail für eine Leiche", "Bei Anruf Mord", "Der zerrissene Vorhang" und "Frenzy". Sich selbst ließ er oft in „Würgerposen“ ablichten.
Ähnlich offen kokettierte Hitchcock zeit seines Lebens mit seiner panischen Angst vor der Polizei. Hitchcock erzählte gerne, dass er mit fünf Jahren, nachdem er etwas angestellt hatte, von seinem Vater mit einem Zettel auf das nahegelegene Polizeirevier geschickt worden sei. Der Polizist las den Zettel und sperrte Alfred für fünf oder zehn Minuten in eine Zelle mit dem Kommentar, dies würde die Polizei mit ungezogenen Jungen so machen. In seinen Filmen geht von Polizisten stets eine latente Gefahr aus.
Zu der Frage, inwieweit das von Hitchcock in seinen Filmen transportierte Bild der besitzergreifenden Mütter von der eigenen Mutter geprägt ist, gab es von ihm selbst keinerlei Aussagen. Das wenige, was man aus seiner Kindheit weiß, legt jedoch autobiographische Ursprünge nahe. Hitchcocks Mutter starb nach langer Krankheit im August 1942 – während der Dreharbeiten zu "Im Schatten des Zweifels". Dieser bereits von vornherein stark autobiographisch geprägte Film nimmt eindeutig Bezug auf Hitchcocks Verhältnis zu ihr: Der Name Emma scheint nicht die einzige Gemeinsamkeit zwischen ihr und der dominanten Mutter-Figur im Film zu sein. Zudem ist im Film noch von einer anderen gebieterischen, jedoch kranken Mutter die Rede – jener des Krimi-Besessenen Herb Hawkins, der wiederum als Selbstprojektion Hitchcocks gilt.
Auffallend oft sind Toiletten in Hitchcocks Filmen zu sehen oder zu hören, in denen konspirative Dinge irgendwelcher Art stattfinden. Laut seinem Biographen Donald Spoto hatte er eine „pubertäre Fixierung“, die in seiner viktorianischen Erziehung begründet lag. Hitchcock äußerte sich zwar oft und gerne über menschliche Körperfunktionen, wollte aber den Eindruck erwecken, er selbst habe mit solchen Dingen nichts zu tun.
Bezugnehmend auf seine Körperfülle, deutete Hitchcock hingegen mehrfach an, dass für ihn Essen eine Art Ersatzbefriedigung sei. So gibt es in einigen Hitchcockfilmen eine symbolische Verbindung von Essen, Sex und Tod.
In den USA galt zwischen 1934 und 1967 der "Hays Code", auch "Production Code" genannt, eine Sammlung von Richtlinien über die Einhaltung der gängigen Moralvorstellungen und über die Zulässigkeit der Darstellung von Kriminalität, Gewalt und Sexualität im Film.
So musste Hitchcock zum Beispiel das geplante Ende für "Verdacht" fallen lassen, weil es Anfang der 1940er Jahre nicht möglich war, den Selbstmord einer schwangeren Frau zu zeigen. Noch bis kurz vor Schluss der Dreharbeiten hatte er kein passendes Ende für den Film gefunden.
In "Berüchtigt" musste Hitchcock einen Dialog streichen, in dem sich ein Vertreter der US-Regierung positiv über die Möglichkeit einer Ehescheidung äußerte. Bei "Saboteure" drehte er politisch heikle Textstellen zur Sicherheit alternativ in entschärften Versionen.
Doch in vielen Fällen gelang es ihm, die Beschränkungen durch die Zensur kreativ zu umgehen.
So war es damals unter anderem nicht erlaubt, eine Toilette zu zeigen.
Daher verzerrte Hitchcock in "Mr. und Mrs. Smith" die eindeutigen Geräusche einer Toilette so, dass man sie für eine Dampfheizung halten konnte. In "Psycho" zeigte er eine Toilette, in der ein Papierzettel hinuntergespült wurde.
Indem er das Bild der Toilette mit einer dramaturgischen Funktion versah – das Verschwinden eines Beweisstücks musste erklärt werden – verhinderte er, dass die Szene geschnitten wurde.
Niemals wurde eine Toilette zu Zeiten des Hays Code expliziter gezeigt.
Da auch die Länge von Küssen im Film damals auf drei Sekunden begrenzt war, inszenierte Hitchcock den Kuss zwischen Ingrid Bergman und Cary Grant in "Berüchtigt" als Folge von einzelnen, durch kurze Dialogsätze unterbrochenen Küssen. Hitchcocks größter Sieg gegen die Zensur war die Schlussszene von "Der unsichtbare Dritte". Cary Grant und Eva Marie Saint befinden sich in einem Schlafwagen. Er zieht sie zu sich nach oben in das obere Bett, und sie küssen sich. Es erfolgt ein Umschnitt, und man sieht einen Zug in einen Tunnel rasen – eine der explizitesten Andeutungen des Sexualakts in einem US-Film zu Zeiten des "Production Code".
Einer der wichtigsten Aspekte der Arbeitsweise Alfred Hitchcocks war, dass er im Idealfall von der Stoffauswahl bis zum Endschnitt nichts dem Zufall überließ, sondern die völlige Kontrolle über die Herstellung des Films beanspruchte.
Wenn Hitchcock existierende Vorlagen benutzte, etwa Romane oder Bühnenstücke, übernahm er nur einzelne Grundmotive der Handlung und entwickelte daraus zusammen mit dem jeweiligen Drehbuchautor oft eine völlig neue Geschichte. Hochwertige, komplexe Literatur sperrte sich gegen diesen Umgang und Hitchcock scheute daher deren Verfilmung – auch aus Respekt vor dem Werk.
Hitchcock war meist an der Drehbucherstellung beteiligt, wurde aber nach 1932 bei keinem seiner Filme offiziell als Autor in Vor- oder Abspann erwähnt: „Ich will nie einen Titel als Produzent oder Autor. Ich habe das Design des Films geschrieben. Mit anderen Worten, ich setze mich mit dem Autor zusammen und entwerfe den ganzen Film vom Anfang bis zum Ende.“ Der Autor Samuel A. Taylor: „Mit ihm zu arbeiten, bedeutete auch mit ihm zu schreiben, was auf die wenigsten Regisseure zutrifft. Hitchcock behauptete nie, selbst ein Schriftsteller zu sein, aber in Wahrheit schrieb er doch seine eigenen Drehbücher, denn er sah bereits jede Szene deutlich in seinem Kopf vor sich und hatte eine sehr genaue Vorstellung davon, wie sie ablaufen sollte. Ich merkte, dass ich nur noch die Figuren persönlicher und menschlicher zu gestalten brauchte und sie weiter entwickeln musste.“ Gelegentlich veränderte Hitchcock im Nachhinein noch die Dialoge ganzer Szenen, etwa um die Spannungs-Dramaturgie zu verbessern (Beispiel: "Das Rettungsboot") oder um autobiographische Bezüge einzubauen (Beispiel: "Ich beichte").
Auch wenn ihm geschliffene Dialoge wichtig waren, legte Hitchcock sein Hauptaugenmerk stets auf die Ausdruckskraft der Bilder. So wurde im Idealfall jede einzelne Einstellung des Films vor Drehbeginn in Storyboards festgelegt.
Seit Beginn seiner Regisseurtätigkeit verfolgte er das Ziel, jegliche Improvisation so weit es geht zu vermeiden. Gegenüber Truffaut erklärte er: „Ich habe Angst davor gehabt, im Atelier zu improvisieren, weil, selbst wenn man im Augenblick Ideen hat, bestimmt keine Zeit bleibt nachzuprüfen, was sie taugen. [… Andere Regisseure] lassen ein ganzes Team warten und setzen sich hin, um zu überlegen. Nein, das könnte ich nicht.“
Nach eigenen Aussagen bereitete Hitchcock die Planung eines Projekts mehr Freude als die eigentlichen Dreharbeiten: Durch zu viele Einflüsse – Produzenten, Technik, Schauspieler, Zeitdruck – sah er die angestrebte Kontrolle über sein Werk bedroht. Außerdem sah er im Idealfall die kreative Arbeit am Film mit Beginn der Dreharbeiten als abgeschlossen an: „Ich drehe einen vorgeschnittenen Film. Mit anderen Worten, jedes Stück Film ist entworfen, um eine Funktion zu erfüllen.“
Diese Grundsätze waren jedoch eher eine Idealvorstellung Hitchcocks. Tatsächlich wurde es ihm spätestens ab 1948 zur Gewohnheit, beim Drehen Alternativen auszuprobieren. Doch auch hier bemühte er sich um möglichst exakte Vorausplanung: Ein Beispiel hierfür ist die Belagerung des Hauses durch die Vögel in "Die Vögel". Gegenüber Truffaut beschrieb Hitchcock, wie er die ursprünglich geplante Szene noch unmittelbar am Drehort umschrieb und bis ins kleinste Detail skizzierte, so dass sie kurz darauf entsprechend diesen neuen Entwürfen gedreht werden konnte. Darüber hinaus wurde Hitchcock im Laufe seiner Karriere immer freier, auch kurzfristig vom festgelegten Drehbuch abzuweichen. Entgegen seinen Gewohnheiten ließ er sogar Improvisationen der Schauspieler zu, wenn auch nur bei eher unwichtigen Szenen.
Bill Krohn ging 1999 in "Hitchcock at Work" ausführlich auf Hitchcocks Arbeitsweise ein. Er rekonstruierte auf Basis von Originalunterlagen wie Drehbuchversionen, Skripte, Storyboards, Memos, Produktionsnotizen etc. und mit Hilfe von Beteiligten die Produktionsgeschichte diverser Filme (darunter Hitchcocks berühmteste) und widerlegt Hitchcocks Bekenntnis zum „vorgeschnittenen Films“: So kam es bei vielen Filmen vor, dass Hitchcock entscheidende Schlüsselszenen in verschiedenen Varianten drehte und meist erst im Schneideraum über die endgültige Form einzelner Szenen entschied.
Im Laufe der Jahre entwickelte sich mit verschiedenen Autoren eine besonders kreative Zusammenarbeit. Hervorzuheben sind Eliot Stannard, Angus McPhail, Charles Bennett, Ben Hecht und John Michael Hayes.
Obwohl Samuel A. Taylor "(Vertigo)" und auch Ernest Lehman "(Der unsichtbare Dritte)" nur je zwei Drehbücher zu tatsächlich realisierten Filmen schrieben, gehörten sie zu den wenigen Mitarbeitern, die mit ihm in den letzten Jahren seiner Karriere regelmäßig zusammenarbeiteten und bis kurz vor seinem Tod Kontakt hatten.
Doch auch mit namhaften Theater- oder Romanautoren arbeitete Hitchcock mehrfach bei der Drehbucherstellung zusammen, reibungslos mit Thornton Wilder und George Tabori, konfliktbeladen mit John Steinbeck, Raymond Chandler und Leon Uris.
Der Kult, den Hitchcock gern um seine Person betrieb, und sein manchmal diktatorischer Stil, führte auch zu Konflikten mit befreundeten Autoren. John Michael Hayes, der im Streit von Hitchcock schied: „Ich tat für ihn, was jeder andere Autor für ihn tat – ich schrieb! Wenn man aber Hitchcocks Interviews liest, kann man den Eindruck bekommen, er habe das Drehbuch geschrieben, die Charaktere entwickelt, die Motivation beigesteuert.“
Wenn Hitchcock mit der Arbeit eines Autors nicht zufrieden war, oder wenn er seine Autorität angegriffen fühlte, dann ersetzte er Autoren kurzerhand durch andere.
Cary Grant und James Stewart wurden innerhalb der jeweils vier Filme, die sie für Hitchcock drehten, zu Hitchcocks Alter Ego. Grant wurde zu dem, „was Hitchcock gerne gewesen wäre“, wie es Hitchcocks Biograph Donald Spoto formulierte, während Stewart vieles wäre, „von dem Hitchcock dachte, er sei es selbst“. Mit einigen seiner Schauspieler verband Hitchcock zudem eine langjährige persönliche Freundschaft, allen voran mit Grace Kelly. Darüber hinaus sind die als neurotisch zu bezeichnenden Beziehungen zu seinen blonden Hauptdarstellerinnen – insbesondere mit Tippi Hedren – bekannt.
Am Anfang von Hitchcocks Karriere galt Film in England als Unterhaltung für die Unterschicht. Aus dieser Zeit stammt Hitchcocks oft zitierter Ausspruch „Alle Schauspieler sind Vieh“, der sich auf diejenigen Theaterschauspieler bezog, die nur mit Widerwillen und des Geldes wegen nebenher als Filmschauspieler arbeiteten. Die Aussage verselbständigte sich später und wurde oft als genereller Ausdruck der Geringschätzung Hitchcocks Schauspielern gegenüber angesehen. Tatsächlich hatte er auch später oft Probleme mit Schauspielern, die eigene Vorstellungen durchsetzen wollten, anstatt sich in die vorgefertigte Planung des Regisseurs einzufügen. Anhänger des Method Actings wie Montgomery Clift und Paul Newman waren Hitchcock daher genauso lästig wie Exzentriker oder Egomanen. Große Achtung hatte Hitchcock hingegen vor Schauspielern, die sein Filmverständnis teilten oder sich zumindest seiner Arbeitsweise anpassten, und gewährte etwa Joseph Cotten und Marlene Dietrich große künstlerische Freiheiten. Oft waren es jedoch die Produzenten, die über die Besetzung der Hauptrollen entschieden. Umso mehr nutzte Hitchcock seine größere Freiheit bei den zu besetzenden Nebenrollen, wobei er gerne auf Theaterschauspieler zurückgriff, die er noch aus seiner Zeit in London in bester Erinnerung hatte, zum Beispiel Leo G. Carroll in insgesamt sechs Filmen oder Cedric Hardwicke in "Verdacht" und "Cocktail für eine Leiche".
Die bekannte Kostümbildnerin Edith Head, mit der er ab "Das Fenster zum Hof" bei fast allen Filmen zusammenarbeitete, meinte: „Loyalität war Hitchcock besonders wichtig. Er war Mitarbeitern gegenüber so loyal, wie er es von ihnen erwartete.“
Bei fünf Filmen war Robert F. Boyle für das "Production Design" verantwortlich; er gehörte bis zu Hitchcocks Tod zu dessen engsten Mitarbeitern. Außerdem griff er im Laufe seiner Karriere gern auf Albert Whitlock als Szenenbildner zurück. Äußerst zufrieden war Hitchcock, dem die Ausdruckskraft der Bilder stets wichtig war, auch mit dem Art Director Henry Bumstead. Der Titeldesigner Saul Bass entwarf nicht nur einige Filmtitel für die Vorspanne sowie Plakate, sondern war bereits bei den Arbeiten an vielen Storyboards maßgeblich beteiligt.
Wichtigster Kameramann in seinen frühen Jahren bei den British International Pictures war John J. Cox. Über Hitchcock sagte Kameramann Robert Burks, der mit Ausnahme von "Psycho" an allen Filmen zwischen 1951 und 1964 beteiligt war: „Man hatte nie Ärger mit ihm, solange man etwas von seiner Arbeit verstand und sie ausführte. Hitchcock bestand auf Perfektion.“ Mit Leonard J. South, ehemaliger Assistent Burks’, arbeitete Hitchcock über einen Zeitraum von insgesamt 35 Jahren zusammen.
Von den Komponisten der Filmmusiken ist Louis Levy hervorzuheben, der die Soundtracks für die frühen englischen Filme von "Der Mann, der zuviel wußte" bis "Eine Dame verschwindet" beisteuerte. Als der Hitchcock-Komponist schlechthin gilt Bernard Herrmann, der ab "Immer Ärger mit Harry" bis einschließlich "Marnie" (1964) alle Filmmusiken für Hitchcock komponierte.
Der Filmeditor George Tomasini war bis zu seinem Tod 1964 ein Jahrzehnt lang enger Mitarbeiter Hitchcocks. Zu Beginn seiner Karriere wirkte seine Frau Alma als Editorin bei seinen Filmen mit; sie blieb bis zuletzt eine der einflussreichsten Mitarbeiterinnen.
Schon zu Beginn seiner Karriere war Hitchcock die Bedeutung der Vermarktung der eigenen Person bewusst: Viele seiner späteren Tätigkeiten sind Teil einer Strategie, sich und seinen Namen als Marke zu etablieren. Bereits 1927 führte Hitchcock ein stilisiertes Selbstporträt als Logo, das bis heute bekannt ist. Anfang der 1930er Jahre, als er mit dem Erfolg seiner Filme in England populär wurde, gründete er mit der Hitchcock Baker Productions Ltd. eine Gesellschaft, die bis zu seiner Übersiedlung nach Amerika ausschließlich dafür zuständig war, für ihn und mit seiner Person Öffentlichkeitsarbeit zu betreiben. Anschließend wurden diese Aufgaben von der Künstleragentur Selznick-Joyce, danach von der Music Corporation of America (MCA) wahrgenommen, wobei der Präsident der MCA, Lew Wasserman, zu seinem persönlichen Agenten wurde. 1962 wurde unter Herman Citron eine neue Gesellschaft gegründet, die Hitchcocks Interessen vertrat und seinen Namen vermarktete. Diese Selbstvermarktung diente auch dazu, eine Machtposition im Produktionsprozess seiner Filme zu erlangen, und war somit Teil seines Kampfes um künstlerische Unabhängigkeit.
Aus Mangel an Statisten in seinen ersten britischen Filmen sah man Hitchcock immer wieder im Hintergrund auftauchen. Daraus entwickelte er eines seiner bekanntesten Markenzeichen: Hitchcocks obligatorischer Cameo-Auftritt. Da das Publikum mit der Zeit immer weniger auf die Handlung achtete, als vielmehr auf Hitchcock lauerte, legte er in späteren Filmen diesen Running Gag möglichst weit an den Filmanfang.
In drei Filmen hatte Hitchcock keinen eigentlichen Cameo-Auftritt. In zwei von diesen Filmen trat er auf Fotos in Erscheinung: "Das Rettungsboot" spielt ausschließlich in einem kleinen Rettungsboot auf dem Meer. Er ist daher in einer zufällig im Boot liegenden Zeitung in einer Werbeanzeige für eine Diät auf einem „Vorher-Nachher-Foto“ zu sehen. Auch in "Bei Anruf Mord" war kein Auftritt möglich. Stattdessen taucht Hitchcock auf einem an der Wand hängenden Foto einer Wiedersehensfeier von College-Absolventen auf. In "Der falsche Mann" schließlich tritt er am Anfang des Films persönlich auf und spricht den Prolog. Dies ist gleichzeitig seine einzige Sprechrolle in einem seiner Kinofilme.
Während von den Filmgesellschaften üblicherweise für die Vermarktung eigene Abteilungen oder externe Agenturen beauftragt werden, trugen bei Hitchcocks Filmen die Werbekampagnen deutlich die Handschrift des Regisseurs. Seine Kino-Trailer waren häufig nicht nur Zusammenschnitte des angekündigten Films: Mit steigendem Wiedererkennungswert seiner Person stellte Hitchcock in der Rolle eines „Master of Ceremony“ seine eigenen Filme vor und führte den Zuschauer humorvoll durch die Kulissen.
Auf den Rat seines Agenten Lew Wasserman hin stieg Hitchcock 1955 in das Fernsehgeschäft ein. Hitchcock gründete die Fernsehproduktionsfirma "Shamley Productions" und produzierte bis 1965 seine eigene wöchentliche Fernsehserie. Am Anfang vieler Folgen begrüßte Hitchcock das Publikum, indem er mit ungerührter Miene makabre Ansagetexte sprach. Die Moderationen, die ihn zu einer nationalen Berühmtheit machten, wurden von dem Bühnenautor James D. Allardice verfasst, der fortan bis zu seinem Tod 1966 für Hitchcock auch als Redenschreiber arbeitete. Als Titelmusik für die Serie "Alfred Hitchcock Presents" verwendete Hitchcock das Hauptthema von Charles Gounods "Marche funèbre d’une marionette" (Trauermarsch einer Marionette), das sich im Weiteren zu einer Erkennungsmelodie für Hitchcocks Öffentlichkeitsarbeit entwickelte.
1956 schloss Hitchcock einen Lizenzvertrag mit HSD Publications ab, der die Überlassung seines Namens für das Krimi-Magazin "Alfred Hitchcock’s Mystery Magazine" zum Inhalt hatte. Die Zeitschrift enthält Mystery- und Kriminalgeschichten, Buchrezensionen und Rätsel und erscheint noch heute. Einführungen und Vorworte, die mit seinem Namen unterschrieben waren, wurden stets von Ghostwritern verfasst.
Von 1964 bis 1987 erschien in den USA die Jugend-Krimi-Reihe „The Three Investigators“, auf Deutsch seit 1968 "Die drei ???". Der Journalist und Autor Robert Arthur kannte Alfred Hitchcock persönlich und bat ihn, seinen Namen zur Vermarktung dieser geplanten Buchreihe verwenden zu dürfen. Schließlich baute er die Figur „Alfred Hitchcock“ in die Handlung ein. Anders als in Europa hielt sich der Erfolg der Bücher in den USA in Grenzen. In Deutschland, wo die Bücher besonders populär waren, entstand die gleichnamige Hörspielreihe. Durch diese bis heute erfolgreichste Hörspielproduktion der Welt wurde der Name Hitchcock auch bei vielen bekannt, die mit seinem filmischen Schaffen nicht vertraut waren.
Viele Elemente aus seinem Werk sind inzwischen in das Standardrepertoire des Kinos eingegangen, ohne dass sie noch bewusst oder direkt mit Hitchcock in Verbindung gebracht werden, insbesondere der Einsatz von Suspense als spannungserzeugendem Mittel oder die Verwendung von MacGuffins als handlungsvorantreibendes Element. Darüber hinaus gibt es seit den 1940er Jahren unzählige Beispiele für Thriller oder Dramen, teils von sehr namhaften Regisseuren, in denen typische Motive Hitchcocks oder seine Stilelemente bewusst kopiert oder variiert werden. Manche dieser Filme sind als Hommage des jeweiligen Regisseurs an Hitchcock zu verstehen, in anderen Fällen wurde Hitchcocks Stil übernommen, da er sich als erfolgreich und wirksam erwiesen hat.
Insbesondere seine Erfolgsfilme aus den 1950er bis Anfang der 1960er Jahre inspirierten in den Folgejahren Hollywood-Produktionen, die inhaltlich oder stilistisch oft mit Hitchcock in Verbindung gebracht werden. 
Zu den vielen Hollywood-Regisseuren, die Alfred Hitchcock mehr oder weniger direkt geprägt hat, zählt Brian De Palma, der mit vielen Verweisen und Zitaten auf Hitchcocks Werk arbeitet. Überdies übernahm er in einigen Filmen Grundstrukturen aus dessen Filmen. So entwickelt er in "Dressed to Kill" (1980) das Grundmotiv aus "Psycho" weiter und zitiert aus weiteren Hitchcock-Filmen. 1976 lehnte sich "Schwarzer Engel" stark an "Vertigo" an. 1984 spielt de Palma in "Der Tod kommt zweimal" mit eindeutigen Bezügen auf "Das Fenster zum Hof" und "Vertigo".
Auch wenn Steven Spielberg selten direkt stilistische Motive kopiert oder adaptiert oder nur wenige seiner Filme thematische Parallelen aufzeigen, erinnert "Der weiße Hai" (1975) in Spannungsaufbau und Dramaturgie an "Die Vögel" und ist die "Indiana-Jones-Filmreihe" (1981–1989) stark von "Der unsichtbare Dritte" (1959) beeinflusst. Auch ein Film wie "Schindlers Liste" (1993) wäre in dieser Form ohne den Einfluss Hitchcocks nicht möglich gewesen. Der von Hitchcocks Kameramann Irmin Roberts entwickelte Vertigo-Effekt wird bisweilen auch als „Jaws Effect“ bezeichnet, da Spielberg diese relativ schwierig umzusetzende Kameraeinstellung im "Weißen Hai" (Originaltitel: "Jaws") als einer der ersten prominenten Regisseure 16 Jahre nach "Vertigo" einsetzte. Inzwischen gehört dieser emotional sehr wirkungsvolle Kameratrick zum Standardrepertoire des Hollywood-Kinos.
Weitere Beispiele für zeitgenössische amerikanische Regisseure, die erkennbar von Hitchcock beeinflusst wurden oder sich auf dessen Werk berufen, sind John Carpenter, David Fincher, David Mamet, Quentin Tarantino, Martin Scorsese, David Lynch und M. Night Shyamalan.
Bereits seit Mitte der 1950er Jahre war Hitchcock insbesondere in Frankreich bei den Vertretern der Nouvelle Vague hoch angesehen. 1957 veröffentlichen die damaligen Filmkritiker und späteren Regisseure Éric Rohmer und Claude Chabrol das erste Buch über ihn. 1956 erschien ein Sonderheft der "Cahiers du cinéma," das maßgeblich zu Hitchcocks Popularität in Frankreich beitrug. Als er im Mai 1960 zu einem Filmfestival reiste, das die Cinémathèque française ihm zu Ehren in Paris abhielt, wurde er von Dutzenden jungen Filmemachern frenetisch gefeiert. Die internationale Ausgabe der "Herald Tribune" schrieb, dass Hitchcock in dieser Woche „das Idol der französischen Avantgarde geworden“ sei.
Im August 1962 gab Hitchcock dem damals dreißigjährigen französischen Filmkritiker und Regisseur François Truffaut ein fünfzigstündiges Interview. Truffaut befragte Hitchcock chronologisch zu dessen bisherigen achtundvierzig Filmen. Das Interview erschien 1966 als "Mr. Hitchcock, wie haben Sie das gemacht?" in Buchform und gilt als Standardwerk der Filmliteratur. Einzelne Filme Truffauts zeigen den Einfluss Hitchcocks deutlich, etwa "Die Braut trug schwarz" (1968) oder "Das Geheimnis der falschen Braut" (1969), die Geschichte eines Mannes, der einer Betrügerin und Mörderin verfällt und auch nicht von ihr lassen kann, als sie ihn zu töten versucht. Der Film ist stark von verschiedenen inhaltlichen und stilistischen Motiven aus "Vertigo", "Marnie" und "Verdacht" beeinflusst. Sein letzter Film "Auf Liebe und Tod" (1983), in dem ein unschuldiger Mann eines Mordes beschuldigt wird, ist voll von hitchcockschen Motiven und Anspielungen auf dessen Werk. Weitere Filme, die Truffaut selbst in der Tradition Hitchcocks sah, waren "Die süße Haut" und "Fahrenheit 451". 1968/69 besetzte Hitchcock die bevorzugte Schauspielerin Truffauts, Claude Jade, für seinen Film "Topas".
In vielen Filmen von Claude Chabrol wird eine scheinbar heile bürgerliche Welt angegriffen und durcheinandergebracht. Die hitchcockschen Hauptmotive der Schuldübertragung sowie der doppelten oder der gespaltenen Persönlichkeit tauchen bei Chabrol immer wieder auf. Einige Beispiele sind "Schrei, wenn du kannst" (1959), "Das Auge des Bösen" (1962), "Der Schlachter" (1970) und "Masken" (1987). Neben Chabrol und Truffaut haben sich in Frankreich unter anderen auch Henri-Georges Clouzot und René Clément des hitchcockschen Repertoires bedient.
Außerhalb Frankreichs war in Europa der unmittelbare Einfluss Hitchcocks auf andere Filmemacher deutlich geringer. Einige europäische oder europäischstämmige Regisseure haben jedoch einzelne Filme gedreht, denen eine Stilverwandtschaft anzuerkennen ist oder die unmittelbar als Hommage an Hitchcock gedacht sind, zum Beispiel "Ministerium der Angst" von Fritz Lang (1943), "Der dritte Mann" von Carol Reed (1949), "Zeugin der Anklage" von Billy Wilder (1957), "Frantic" von Roman Polański (1988) und "Schatten der Vergangenheit" von Kenneth Branagh (1991).
Alle Filme, an denen Hitchcock beteiligt war, in der Reihenfolge ihrer Produktion:
"Stummfilme, Schwarzweiß"
"bis einschließlich Nr. 9 Stummfilme, ab Nr. 10 Tonfilme; alle Filme in Schwarzweiß"
"alle Filme in Schwarzweiß"
In diese Phase fällt auch Hitchcocks einzige Mitarbeit an einem längeren Dokumentarfilm "(German Concentration Camps Factual Survey)" im Mai — Juli 1945 in London. Er hat dies später im Interview als seinen Beitrag zum Krieg bezeichnet. Der Film wurde aus anderen Gründen nicht fertiggestellt.
"Filme Nr. 36, 37, 38 und 44 in Schwarzweiß, alle anderen in Farbe"
Außerdem trat er zwischen 1955 und 1965 in insgesamt 360 Folgen der Fernsehserien "Alfred Hitchcock Presents" und "The Alfred Hitchcock Hour" in der Rolle des Gastgebers auf.
"Film Nr. 47 in Schwarzweiß, alle anderen in Farbe"
"alle Filme in Farbe"
Hitchcock wurde sechsmal für den Oscar nominiert: fünfmal für die Beste Regie, einmal für den Besten Film (als Produzent). Alle sechs Mal ging er leer aus, was ihn zu dem Kommentar veranlasste: „Immer nur Brautjungfer, nie die Braut“. Dennoch blieb er nicht oscarlos, denn 1968 gewann er den Irving G. Thalberg Memorial Award als Spezialoscar für besonders kreative Filmproduzenten.
Er wurde mit zwei Sternen auf dem Hollywood Walk of Fame geehrt. Den einen in der Kategorie Film findet man bei der Adresse 6506 Hollywood Blvd, den anderen in der Kategorie Fernsehen am 7013 Hollywood Blvd.
Regie: Julian Jarrold; Besetzung: Toby Jones (Alfred Hitchcock), Sienna Miller (Tippi Hedren), Imelda Staunton (Alma Reville Hitchcock), Conrad Kemp (Evan Hunter), Penelope Wilton (Peggy Robertson)
Regie: Sacha Gervasi; Besetzung: Anthony Hopkins (Alfred Hitchcock), Helen Mirren (Alma Reville Hitchcock), Scarlett Johansson (Janet Leigh), Danny Huston (Whitfield Cook), Toni Collette (Peggy Robertson), Michael Stuhlbarg (Lew Wasserman), Michael Wincott (Ed Gein), Jessica Biel (Vera Miles), James D’Arcy (Anthony Perkins)
Sortiert in der chronologischen Reihenfolge der jeweiligen Originalausgabe.
Hauptquellen sind die beiden Biografien von Taylor und Spoto sowie die Bücher von Truffaut und Krohn.

</doc>
<doc id="78" url="https://de.wikipedia.org/wiki?curid=78" title="Auteur-Theorie">
Auteur-Theorie

Die Auteur-Theorie (von frz. „Auteur“ = Autor) ist eine Filmtheorie und die theoretische Grundlage für den Autorenfilm – insbesondere den französischen – in den 1950er Jahren, der sich vom „Produzenten-Kino“ abgrenzte. Auch heute noch wird die Definition des "Auteur"-Begriffs ständig weiterentwickelt. Im Zentrum des Films steht für die Auteur-Theorie der Regisseur oder Filmemacher als geistiger Urheber und zentraler Gestalter des Kunstwerks.
Ende der 1940er Jahre wurde eine erste Auteur-Theorie von dem französischen Filmkritiker Alexandre Astruc formuliert, indem er die Frage nach dem geistigen Besitz eines Films aufwarf.
Im traditionellen Schaffensprozess lassen sich die Anteile von Drehbuchautor, Kameramann und Filmregisseur am Gesamtwerk nur schwer zuordnen. Durch die Zuteilung der Teilaufgaben als Honorartätigkeit durch die Filmgesellschaften leide die Kreativität, so die These. Im Umkehrschluss fordert diese Theorie die Zusammenführung der Tätigkeiten zu einer kreativen Einheit. Er formulierte seinen Entwurf in dem Aufsatz „"La caméra-stylo"“. Die Kamera sollte wie ein Stift verwendet werden. Er war sich sicher, dass bedeutende Schriften in Zukunft nicht mehr als Text, sondern mit der „Kamera geschrieben“ würden.
Doch durchgesetzt haben sich solche und ähnliche Ideen der Auteur-Theorie erst in den 1950er Jahren. Deren gängiger Begriff als Wegbereiter für die heutige Auteur-Theorie lautete zunächst "politique des auteurs" (Autoren-Politik), was erst im Laufe der Zeit zur "Theorie" umgeformt wurde. Das Wort "politique" bzw. Politik stand hier also eher für Parteilichkeit, welche für filmwissenschaftliche Diskussionen eher hinderlich ist (siehe unten).
Die "politique des auteurs" wurde zu dieser Zeit von einer Gruppe von jungen Filmkritikern um André Bazin entwickelt, die für die Filmzeitschrift "Cahiers du cinéma" schrieben. Eine wesentliche Rolle spielte dabei François Truffaut: Im Januar 1954 veröffentlichte er seinen Aufsehen erregenden Aufsatz "Eine gewisse Tendenz im französischen Film" ("Une certaine tendance du cinéma français"), in dem er sich mit scharfer Polemik gegen den etablierten französischen „Qualitätsfilm“ wandte. Bei diesem trat der Regisseur gegenüber dem Drehbuchautor und dem Autor der literarischen Vorlage oft in den Hintergrund. Truffaut plädierte dagegen für einen Film, bei dem Form und Inhalt vollständig vom Regisseur selbst als dem eigentlichen „auteur“ des Films bestimmt werden. Er fand das bei traditionell als Autoren ihrer Filme betrachteten europäischen Regisseuren wie Luis Buñuel, Jean Renoir und Roberto Rossellini, außerdem aber auch und vor allem bei Regisseuren wie Alfred Hitchcock, Howard Hawks, Fritz Lang und Vincente Minnelli, die (zum großen Teil als Vertragsregisseure) im Studiosystem Hollywoods arbeiteten, deren Filme aber trotzdem einen persönlichen Stil aufweisen.
Das Konzept des Regisseurs als "auteur" seiner Filme wurde für die Filmkritik der "Cahiers du cinéma" bestimmend, und damit für die Regisseure der Nouvelle Vague, die daraus hervorgingen, neben Truffaut etwa Jean-Luc Godard, Jacques Rivette oder Claude Chabrol – Filmemacher, die sich zur Umsetzung ihrer künstlerischen Ziele einer jeweils ganz eigenen filmischen Form bedienten.
1968 veröffentlichte Roland Barthes, einer der Auteur-Theoretiker, den Aufsatz "La mort de l'auteur" ("Der Tod des Autors"). Barthes ersetzte den „Auteur-Dieu“ („Autoren-Gott“) durch den „écrivain“ (den Schriftsteller) und führte damit eine Kritik fort, die bereits 1967 durch Julia Kristeva in ihrem Aufsatz "Bakhtine, le mot, le dialogue et le roman" aufgebracht wurde.
Bis in die 1970er blieb die Auteur-Theorie aber noch prägend für den europäischen Film. Danach setzte auch hier eine Abkehr von der „verhängnisvollen Macht der Regisseure“ (Günter Rohrbach) ein. Wirtschaftlicher Druck zwang zur Rückkehr zu einer arbeitsteiligen Produktionsweise, wie sie für den Produzenten-Film charakteristisch ist. Damit einher ging notwendigerweise auch wieder die Einigung aller Beteiligten auf einen kleinsten gemeinsamen Nenner und somit auch häufig eine gewisse Banalisierung der Filminhalte, die umso stärker zu Tage tritt, je weniger der Produzent als Projektverantwortlicher in den eigentlichen schöpferischen Prozess eingebunden ist.
In der Filmwissenschaft wurden auch immer neue Autorschaften von Teammitgliedern entdeckt. In der Realität ist Film Teamarbeit und es ist dem Film nicht anzusehen, ob zum Beispiel die Idee für eine Einstellung nun vom Regisseur oder vom Kameramann stammt. Im Dogma-Film ist der Kameramann nicht weisungsgebunden. Die „Polnische Schule“ bindet den Kameramann bereits in den Prozess des Drehbuchschreibens ein. Unerfahrene Regisseure sind meist sehr auf die Kreativität des Kameramanns oder der Kamerafrau und anderer Teammitglieder angewiesen.
Durch das Aufkommen digitaler Aufnahmetechniken wie Digital Video seit Ende der 1990er Jahre sehen viele Filmemacher, wie etwa Wim Wenders, wieder günstigere Bedingungen für individuelle, subjektive Produktionen gegeben.
Die von François Truffaut und Jean-Luc Godard proklamierte „politique des auteurs“ (Autorenpolitik) der fünfziger Jahre war ursprünglich ein Versuch, bestimmte Regisseure wie Alfred Hitchcock als Künstler anzuerkennen, die ihre völlig eigene Bildsprache entwickelten oder, wie Truffaut selber, sämtliche Aspekte ihrer Filme selbst bestimmten. Ein Autorenfilmer ist demnach ein Regisseur, der einen Film – möglichst ohne Kompromisse – so gestaltet, wie er ihn selbst haben möchte.
Die „politique des auteurs“ geriet schnell in die Kritik. Kritiker wie Andrew Sarris und Peter Wollen wiesen auf ein empirisches Problem hin: Niemand kann beweisen, wie viel Einfluss der Regisseur wirklich auf seine Filme hatte bzw. welchen Einfluss Form und Inhalt wirklich auf das haben, was wir als Autorschaft wahrnehmen.
Als Beispiel hierfür gilt der Vorspann von "Vertigo – Aus dem Reich der Toten" (1958), den Alfred Hitchcock nicht selbst angefertigt hat, oder die Tatsache, dass viele seiner Filme auf einer Buchvorlage fremder Autoren basieren und selbst die Drehbücher selten von ihm selbst stammten. Gerade Hitchcock aber ist eine zentrale Figur in der „politique des auteurs“.
Wie der Name „politique des auteurs“ sagt, handelte es sich um eine Politik, einen gezielten polemischen Eingriff. Der Village-Voice-Kritiker Andrew Sarris übersetzte „politique des auteurs“ jedoch 1962 mit „auteur theory“, wobei unklar blieb, in welchem Sinne es sich hier tatsächlich um eine "Theorie" handelt. Sarris popularisierte diese „Theorie“ im englischen Sprachraum und benutzte sie vor allem, um die absolute Überlegenheit des Hollywood-Kinos darzulegen, war er doch davon überzeugt, es sei „the only cinema in the world worth exploring in depth beneath the frosting of a few great directors at the top“. Nun war die Frage: Wo ist die Grenze? "Wen" oder vielmehr "was" nehmen wir als Autor wahr?
Soziologisch gesehen war die Autorentheorie eine Distinktionsstrategie junger Kritiker, die auf sich aufmerksam machen wollten. Godard hat dies später offen zugegeben: „Wir sagten von Preminger und den anderen Regisseuren, die für Studios arbeiteten, wie man heute fürs Fernsehen arbeitet: ‚Sie sind Lohnempfänger, aber gleichzeitig mehr als das, denn sie haben Talent, einige sogar Genie …‘, aber das war total falsch. Wir haben das gesagt, weil wir es glaubten, aber in Wirklichkeit steckt dahinter, dass wir auf uns aufmerksam machen wollten, weil niemand auf uns hörte. Die Türen waren zu. Deshalb mussten wir sagen: Hitchcock ist ein größeres Genie als Chateaubriand.“
In den siebziger Jahren folgte dann die stärkste Kritik an der „politique des auteurs“. Roland Barthes proklamierte bereits 1968 vor einem poststrukturalistischen Hintergrund den „Tod des Autors“. Der Autor wurde nun aufgrund des empirischen Dilemmas der Beweisbarkeit von Autorschaften als Image-Figur erkannt, die sich aus ihrer Umwelt formt und in die Werke einschreibt. Auch von feministischer Seite wurde die „politique des auteurs“ scharf angegriffen, diene sie doch dazu, den kollektiven Charakter des Filmemachens zu verdecken und in der Tradition patriarchaler Heldenverehrung Männer zu Superstars zu stilisieren. Claire Johnston verteidigte den Ansatz insofern, als dieser einer zu monolithischen Sicht des Hollywood-Kinos entgegenwirke.
In den neunziger Jahren schließlich ging die Tendenz zu der Annahme, dass Autorschaften zum Großteil (z. T. kommerziell) konstruiert sind. Timothy Corrigan nennt dies den „commercial auteur“. Es wird damit gerechnet, dass das Publikum den Film eines als Autor bekannten Regisseurs als z. B. „Der neue Woody Allen!“ wahrnimmt, ohne wirklich zu wissen, wie viel Einfluss Woody Allen tatsächlich auf den Film hatte.
Dana Polan verfolgte einen weiteren interessanten Ansatz: Er sieht den „auteurist“ als Hauptverantwortlichen für konstruierte Autorenbilder. Das sind Kritiker, die den Autor als höchste Instanz suchen und damit – wie François Truffaut – auf einen Filmemacher als Künstler hinweisen wollen und nebenbei ihre eigene Erkenntniskraft zelebrieren. Der Begriff dafür lautet „Auteur Desire“. Dieser Ansatz zeigt noch einmal den größten Vorwurf gegenüber der „politique des auteurs“ auf. Doch trotzdem ist die Nennung eines Regisseurs parallel zu – beispielsweise – einem Buchautor als Schöpfergeist auch unter reflektierenden Filmkritikern und -wissenschaftlern weiterhin außerordentlich beliebt. Steckt also doch mehr dahinter?
Ein neuerer Ansatz, die kontextorientierte Werkanalyse von Jan Distelmeyer, versucht diese Frage zu klären. Als Grundlage dienen Publikums- und Kritikerrezeption auf der einen Seite und die Konstruktion des Autors aus Biografie, Filmindustrie und kulturellem Umfeld auf der anderen Seite.
Diese zweiseitige Annäherung erkennt das empirische Dilemma der Definition von „auteur“ an und maßt sich auch keine Bestimmung dessen an, was jetzt eigentlich das Werk von Autor XYZ ist. Viele andere Filmtheoretiker verfolgen heutzutage ähnliche Konzepte. Doch auch eine solch freie Handhabung kann das Problem nicht vollständig lösen, da die wichtigsten Elemente variabel sind und sich so einer eindeutigen Aussage verschließen.
Der Schwerpunkt kritischer Tendenzen liegt also zum Großteil in der Empirie. Einen Filmemacher als „auteur“ anzuerkennen fordert uneingeschränktes Vertrauen in seine Aussagen, wie viel Einfluss er auf seine eigenen Filme hatte. Da dies in Zeiten einer sehr starken Vermarktung aller möglichen mehr oder weniger (un)abhängigen Regisseure seitens von Filmindustrie und Verleih ein fast aussichtsloses Unterfangen ist, ist ein Restzweifel und das stete Hinterfragen der „auteur“-Definition angebracht.

</doc>
<doc id="79" url="https://de.wikipedia.org/wiki?curid=79" title="Aki Kaurismäki">
Aki Kaurismäki

Aki Olavi Kaurismäki (* 4. April 1957 in Orimattila) ist ein vielfach preisgekrönter finnischer Filmregisseur.
Aki Kaurismäki studierte an der Universität Tampere Literatur- und Kommunikationswissenschaften. Neben diversen Aushilfsjobs, etwa als Briefträger oder in der Gastronomie, war er Herausgeber eines universitären Filmmagazins. Darüber hinaus schrieb er von 1979 bis 1984 Filmkritiken für das Magazin "Filmihullu". Das erste Drehbuch folgte 1980 für den mittellangen Film "Der Lügner (Valehtelija)", bei dem sein Bruder Mika Regie führte.
Kaurismäkis Filme thematisieren häufig Schicksale von gesellschaftlichen Außenseitern in städtischen Zentren wie Helsinki. Sie sind nicht nur für ihre sparsamen Dialoge, sondern auch für einen skurril-lakonischen Humor bekannt. Kaurismäki arbeitet regelmäßig mit einem festen Stamm befreundeter Schauspieler und Musiker, die seine Filme auch stilistisch prägen bzw. prägten: Matti Pellonpää, Kati Outinen, Kari Väänänen und Sakke Järvenpää. Als Reminiszenz an Alfred Hitchcock hat er in seinen Filmen gelegentlich Cameo-Auftritte, wie sie auch Hitchcock pflegte. 
In Deutschland wurden seine Filme zum ersten Mal 1986 auf dem Filmfestival "Grenzland-Filmtage" in Selb gezeigt. Aki Kaurismäki führte dabei die Filme "Der Lügner", "Calamari Union" und "Schuld und Sühne" persönlich vor. Während des Festivals schrieb er das Drehbuch für seinen Film "Schatten im Paradies," den er 1988 erneut persönlich bei den Grenzland-Filmtagen in Selb präsentierte. Dieser Film brachte ihm den internationalen Durchbruch. Ein Großteil der Filmmusik kam von der Band "Nardis" aus Erlangen, die Kaurismäki 1986 auf den Grenzland-Filmtagen kennengelernt hatte.
Dem breiten deutschen Publikum bekannt wurde der finnische Regisseur durch seine Teilnahme an der Berlinale 1988. Für großes Aufsehen sorgte Kaurismäki im Herbst 2006, als er sich weigerte, seinen Film "Lichter der Vorstadt" als offiziellen finnischen Beitrag für eine Oscar-Nominierung in der Kategorie Bester fremdsprachiger Film zuzulassen, obwohl das Drama von der finnischen Filmkammer einstimmig ausgewählt worden war. Kaurismäki begründete seine Ablehnung mit seiner seit Jahren vertretenen kritischen Haltung gegen den Irak-Krieg der USA.
Zusammen mit seinem Bruder Mika Kaurismäki gründete er das Midnight Sun Film Festival im lappischen Sodankylä sowie die Verleihfirma Villealfa. Dieser Name geht zurück auf die Figur Ville Alfa, den Protagonisten im Film "Der Lügner". Gleichzeitig handelt es sich um ein Anagramm von "Alphaville", einem Film von Jean-Luc Godard.
Im Jahr 1989 emigrierte Kaurismäki zusammen mit seiner Frau nach Portugal, weil „es in ganz Helsinki keinen Platz mehr gebe, wo er seine Kamera noch postieren könne“.
Rainer Gansera, der für die Zeitschrift epd Film mit dem „Chef-Melancholiker des europäischen Autorenkinos“ 2006 in Hof gesprochen hat, zeigte sich auch von seinem Auftreten persönlich beeindruckt und beschrieb atmosphärisch:
Als persönliche Leitbilder sehe Kaurismäki Bresson, Ozu und Godard. Der Ausbildung an den Filmhochschulen seines Landes konnte er dagegen nicht viel Positives abgewinnen.
Bei "Pandora" sind Ende 2006 als „Aki Kaurismäki DVD-Collection“ 14 Spielfilme und fünf Kurzfilme (mit digital restaurierten Bildern) in vier Boxen erschienen.
2011 stellte Kaurismäki nach fünf Jahren mit "Le Havre" wieder einen Spielfilm fertig, der ihm eine Einladung in den Wettbewerb der 64. Internationalen Filmfestspiele von Cannes einbrachte. Der in Frankreich gedrehte Film handelt von einem Schuhputzer aus der gleichnamigen Hafenstadt, der sich eines illegalen Flüchtlingskindes aus Afrika annimmt. "Le Havre" gewann in Cannes den FIPRESCI-Preis.
Für den Spielfilm "Toivon tuolla puolen" (Englischsprachiger Festivaltitel: "The Other Side of Hope") erhielt Kaurismäki 2017 eine Einladung in den Wettbewerb der 67. Internationalen Filmfestspiele Berlin. Der Film spielt während des Herbstes in Helsinki und erzählt von der Begegnung eines älteren finnischen Handelsvertreters (dargestellt von Sakari Kuosmanen) mit einem jungen syrischen Flüchtling (Sherwan Haji).

</doc>
<doc id="81" url="https://de.wikipedia.org/wiki?curid=81" title="Anime">
Anime

Anime (jap. , [], im Deutschen häufig []) ist eine Verkürzung des japanischen Lehnwortes "animēshon" (, von ) und bezeichnet in Japan produzierte Zeichentrickfilme. In Japan selbst steht "Anime" für alle Arten von Animationsfilmen, für die im eigenen Land produzierten ebenso wie für importierte. Er bildet das Pendant zum Manga, dem japanischen Comic. Japan besitzt die umfangreichste Trickfilmkultur weltweit.
Animes decken ein breitgefächertes Themenspektrum für alle Altersstufen ab. Von Literaturverfilmungen (z. B. "Das Tagebuch der Anne Frank" oder "Heidi") über Horror bis hin zu Science Fiction werden nahezu alle Bereiche und Altersklassen abgedeckt. Auch gibt es Genres bei Anime, die ausschließlich in diesen und Mangas vorkommen (z. B. Mecha-Serien über überdimensional große Roboter). In Japan können für Kinder geeignete Produktionen ernsthafte Themen haben oder realistischere Gewaltdarstellungen enthalten, als das in westlichen Produktionen der Fall ist. Aber auch sexuelle Anspielungen, bei denen sich die Charaktere zuweilen sehr freizügig geben, werden geduldet. Dies führt dazu das sie in anderen Ländern vor der Ausstrahlung bearbeitet werden.
Pornographische Animes, sogenannte Hentai, machen nur einen kleinen Teil des japanischen Kaufvideo-Marktes aus; im Fernsehen und im Kino werden diese in Japan überhaupt nicht gezeigt. Viele Animes beinhalten jedoch erotische Ansätze, ohne dem Hentai-Genre zugeordnet werden zu können, insbesondere die des Genres Etchi.
Anime- und Manga-spezifische Genre:
Anime – dort auch unter den veraltenden Bezeichnungen "dōga" (, „bewegte Bilder“) und "manga eiga" (, „Manga-Film“) bekannt – sind ein fester Bestandteil des japanischen Kulturgutes. Zu den erfolgreichsten Kinofilmen in Japan zählen viele Animes, so "Prinzessin Mononoke", "Pokémon: Der Film" und "Chihiros Reise ins Zauberland". Nach einer Umfrage sind die 100 beliebtesten Zeichentrickserien in Japan, alle Anime, mit Ausnahme von "Tom und Jerry". Zudem ist die Unterhaltungsindustrie in Japan, die Animes wie Mangas produziert, mit 80 Milliarden Euro Umsatz im Jahr wirtschaftlich bedeutend. Pro Jahr kommen bis zu 200 neue Serien auf den Markt.
Neben Fernsehserien und Kinofilmen werden Animes als Original Video Animation, kurz "OVA", für den Kaufvideo- und DVD-Markt produziert, die seit den frühen 80er erschienen. Die Zielgruppe sind meist junge Erwachsene, daher sind die Inhalte in der Regel mit viel Fanservice versehen. Diese wurden aber weitestgehend durch Mitternachtsanime ersetzt. Seit 2000 gibt es auch Serien direkt für das Internet, und sind Original Net Animation, kurz ONA, genannt.
Anime-Fernsehserien haben für gewöhnlich 12–13, 24–26, sowie seltener 52 oder mehr Folgen, so dass bei wöchentlicher Ausstrahlung eine Laufzeit von einem viertel, halben oder ganzen Jahr erreicht wird. Ein solches Vierteljahresintervall wird als "cours" (, "kūru") bezeichnet. Die "cours" sind dabei saisonhaft, d. h., es gibt Winter-, Frühlings-, Sommer- und Herbst-Cours, die im Januar, April, Juli bzw. Oktober beginnen. Die meisten Anime-Serien sind nicht als Endlosserien ausgelegt, obwohl insbesondere Verfilmungen langer Manga-Serien auf weit mehr als 100 Folgen kommen können.
Im Jahr 1963 wurden 7 Serien gesendet, und wird generell als der Beginn von Anime TV Serien angesehen. 1978 wurde die 50er Grenze, mit 52 Serien gebrochen. 1998 wurde die 100er Grenze mit 132 Serien erreicht. Mit 233 Serien wurde die 200er Grenze im Jahr 2004 erreicht. Seitdem hat sich die Anzahl der Serien mehr oder weniger etabliert, jedoch gab es Jahre wie 2006 und 2014, wo die 300er Grenze erreicht wurde.
Der Anstieg der Anime-Anzahl in den 90ern ist darauf zurückzuführen, dass seit 1996 die Mitternachtsprogrammplätze für Anime verwendet werden; aber auch, dass durch den großen Erfolg (und Kontroverse) von "Neon Genesis Evangelion" immer mehr Studios, Videounternehmen und Verlage Werke produzieren ließen. Diese schließen sich dann oft mit Merchandising-Partnern zu Produktionskomitees (, "seisaku iinkai") zusammen und kaufen einen Mitternachtsprogrammplatz – daher auch als Mitternachts-Animes (, "shin’ya anime") bezeichnet – bei mehreren Sendern, üblicherweise für ein bis zwei "cours". Der größte Teil dieser Programmierungen geschieht auf Regionalsendern, die keinem der großen Networks angeschlossen sind. Da diese auf UHF-Band ausstrahlen, werden derartige Anime auch UHF-Anime (UHF) genannt. Mitternachtsanimes erreichen durchschnittliche Einschaltquoten von etwa 2 %, während 4 bis 5 % schon außergewöhnlich hoch sind. Einschaltquoten spielen bei Mitternachtsanimes und damit den meisten Animes seit den späten 90ern kaum eine Rolle, sondern die Ausstrahlung dient dabei der Werbung für die DVD- oder Blu-Ray-Veröffentlichungen, mit denen und den Merchandise-Artikeln der Gewinn gemacht wird. Abhängig von deren Verkaufszahlen entscheidet sich dann, ob weitere Staffeln produziert werden. Viele dieser Anime, die ein bestehendes Werk adaptieren, dienen letztendlich aber auch der Bewerbung der Vorlage, so dass für das auftraggebende Produktionsunternehmen auch die Animeverkäufe zweitrangig sein können, sofern die Vorlagenverkäufe anziehen, was sich daran äußert das teilweise auch nur wenig erfolgreiche Anime Fortsetzungen bekommen können.
Anders sieht dies bei am Tage ausgestrahlten Animes aus, die meist langläufig sind (über zwei "cours") und sich zudem auch entweder an ein junges oder ein Familienpublikum richten. Seit der Einführung der Mitternachtsanime haben sich die Anzahl der Serien mit hohen Einschaltquoten verringert, und auch die Art der Serien im Tagesprogramm hat sich verändert.
Anime mit den höchsten Einschaltquoten:
Anime mit den höchsten Heimvideoverkäufen:
Durch die sich erholende Wirtschaft während der 90er und die starke Berichterstattung über die steigende Beliebtheit von Animes im Ausland und dem „Moe-Boom“ investierten aber auch branchenfremde Unternehmen wie Finanz- und neue IT-Unternehmen in diesen früheren Nischenmarkt. Der Rückgang seit 2006 wird auf die sinkenden Geburtenraten und die wirtschaftliche Rezession zurückgeführt.
Japanische Fernsehsender gehen aber auch dazu über, den ausländischen Markt direkt zu beliefern. In den USA wird der Rückgang der Marktgröße für Animes von 4,8 Mrd. Dollar im Jahr 2003 auf 2,8 Mrd. Dollar für 2007 hauptsächlich mit der Fansubbing-Szene in Zusammenhang gesetzt, die Serien bereits kurz nach deren Erstausstrahlung im japanischen Fernsehen untertitelt über Filesharing verbreitet. Im Januar 2009 begann TV Tokyo als erster größerer Fernsehsender, seine Animes nur Stunden nach deren Ausstrahlung im japanischen Fernsehen englisch untertitelt auf einer abopflichtigen Website zu veröffentlichen. Heute wird ein großer Teil der Neuerscheinungen gleichzeitig zur japanischen Ausstrahlung (Simulcast) auf Websites mit englischen (Funimation und Crunchyroll), aber auch deutschen (siehe Abschnitt Anime auf Videoportalen) Untertiteln gestreamt.
Viele Animes beruhen auf erfolgreichen Mangas, sowie, vor allem in jüngerer Zeit, auf Light Novels. Es wird aber auch aufgrund eines erfolgreichen Animes ein entsprechender Manga gezeichnet. Vergleichsweise selten sind „Anime-Comics“ bei denen der Manga nicht neu gezeichnet, sondern aus Einzelbildern des Anime und eingefügten Sprechblasen zusammengesetzt wird.
Oft ist auch die Computerspiel-Industrie an der Anime-Produktion beteiligt, die auf Grundlage der Animes Computer- und Konsolenspiele produziert. Da den Produktionskomitees Unternehmen unterschiedlicher Branchen angehören können, neben Buch-, Spieleverlagen, Studios, auch Lebensmittelfirmen, die Kapital einbringen und sich die Rechte am Werk aufteilen, können zum Anime zeitgleich auch Manga, Romane und weitere Artikel erscheinen. Teilweise werden diese Franchises dann gezielt zur Werbung für ein Produkt oder einer Produktgruppe eingesetzt.
Wie in Kinofilmen wird im Anime die Musik als wichtiges künstlerisches Mittel benutzt. Mit Anime-Soundtracks wird in Japan sehr viel Geld gemacht, da diese sich häufig ebenso gut verkaufen wie Chartstürmer-Alben. Aus diesem Grund wird Animemusik häufig von erstklassigen Musikern komponiert und aufgeführt. Fähige Komponisten für die Hintergrundmusik sind bei den Fans hochangesehen. Zu den bekannteren Komponisten zählen z. B. Joe Hisaishi, Yuki Kajiura, Yōko Kanno und Kenji Kawai.
Am häufigsten wird Musik in Animes genutzt, um als Hintergrundmusik die Stimmung einer Szene wiederzugeben, oder als Thema für einen Charakter. Serien haben ein Vorspannlied als Einleitung. Dieses Thema passt für gewöhnlich zum Gesamtton der Sendung und dient dazu, den Zuschauer für das anschließende Programm zu begeistern. Zwischen- und Abspannlieder kommentieren oft die Handlung oder die Sendung als Ganzes und dienen häufig dazu, eine besondere wichtige Szene hervorzuheben. Diese Lieder werden häufig von bekannten Musikern oder japanischen Idolen gesungen, aber auch von den Synchronsprechern (Seiyū), die dadurch wiederum zu Idolen werden. Somit sind sie ein sehr wichtiger Bestandteil des Programms.
Zusätzlich zu diesen Musikthemen veröffentlichen die Sprecher eines bestimmten Animes auch CDs für ihren Charakter, "Image Album" genannt. Trotz dem Wort "image" beinhalten sie nur Musik und/oder Textpassagen, in denen der Sprecher zu dem Zuhörer oder über sich singt bzw. redet, wodurch der Zuhörer glaubt, dass der Charakter selber singt oder redet. Eine weitere Variante von Anime-CD-Veröffentlichungen sind "Drama-CDs": Hörspiele, in denen die Sprecher eine Geschichte erzählen, die häufig im Anime nicht vorkommt.
Eines der bekanntesten japanischen Anime-Studios ist Studio Ghibli, das seit 1985 unter der Leitung von Hayao Miyazaki Filme produziert, z. B. "Prinzessin Mononoke" 1997, "Chihiros Reise ins Zauberland" 2001, "Das wandelnde Schloss" 2004. Seinen bisher größten weltweiten Erfolg hatte Studio Ghibli mit "Chihiros Reise ins Zauberland". Der Film erhielt neben zahlreichen internationalen Zuschauer- und Kritikerpreisen im Jahr 2002 den Goldenen Bären auf der Berlinale und im Jahr 2003 den Oscar als bester Animationsfilm, was ihn zum meistausgezeichneten Zeichentrickfilm aller Zeiten macht.
Weitere bekannte Anime-Studios:
Laut einer im Jahr 2013 durchgeführten Studie arbeiten japanische Anime-Zeichner im Durchschnitt 10 bis 11 Stunden pro Arbeitstag bzw. 263 Stunden pro Monat bzw. 4,6 freie Tage/Monat. Animatoren verdienen pro Jahr durchschnittlich (Mittelwert) 3,3 Millionen Yen (ca. 23.000 €) bzw. am häufigsten (Modalwert) 4,0 Mio. Yen (28.000 €) angefangen bei Einstiegspositionen wie Zwischenzeichnern mit 1,1 Mio. Yen (8000 €), über Schlüsselzeichner mit 2,8 Mio. Yen (20.000 €), Storyboardern/3D-Animatoren mit 3,8 Mio. Yen (26.000 €) bis zu Regisseuren mit 6,5 Mio. Yen (45.000 €). Zeichner werden häufig nach einem Schema bezahlt bei dem sie zusätzlich zu einem festen Lohn noch nach fertiggestellten Einzelbildern bzw. -Szenen bezahlt werden.
Als erster Anime in Deutschland wurde ab dem 16. März 1961 der Film "Der Zauberer und die Banditen" von Toei Animation aus dem Jahr 1959 in den Kinos gezeigt. Seither sind im deutschen Kino mehr als 30 Anime-Filme gezeigt worden, darunter "Akira" (1991), "Ghost in the Shell" (1997), "Perfect Blue" (2000) und einige Produktionen von Studio Ghibli wie "Prinzessin Mononoke" (2001) und "Chihiros Reise ins Zauberland" (2003). Die bisher höchsten Zuschauerzahlen hatten die drei im Kino gezeigten Filme zur "Pokémon"-Serie.
Die erste Anime-Serie im deutschen Fernsehen war "Speed Racer" (Tatsunoko Productions, 1967), von der von November 1971 bis Dezember 1971 in der ARD aber nur drei von ursprünglich acht geplanten Folgen gezeigt wurden. Wegen Protesten von Eltern, Pädagogen und Medien wurde die bereits angekündigte vierte Folge kurzfristig gestrichen. Im Frühjahr 1973 wurden zwei weitere Folgen ohne Vorankündigung als Ersatz für ausgefallene Asterix-Sendungen ins Programm genommen, danach wurde die Serie nach erneuten Protesten vollständig abgesetzt.
Die ARD und ZDF hatte die in Italien und Frankreich erfolgreiche Serie "UFO Robot Grendizer" ("Goldorak") wegen zu hoher Brutalität abgelehnt.
Mit Ausnahme von "Captain Future", gegen das es ebenfalls zahlreiche Proteste von Eltern gab und vor dem in den 1980er-Jahren sogar in einigen Schulbüchern gewarnt wurde, umfassten Animes im öffentlichen-rechtlichen Fernsehen nur Serien für jüngere Kinder. Auch waren diese teilweise Koproduktionen, wie "Wickie und die starken Männer," "Die Biene Maja" und "Nils Holgerson". Dabei wurde stets darauf geachtet, dass die Serien im zeichnerischen Stil wie in der Handlung dem westlichen Empfinden und gesellschaftlichen Vorstellungen entsprachen. Die Programme waren dennoch Gegenstand teils heftiger Kritik, die Sendungen würden den Geschmack des jungen Publikums verderben und der Verantwortliche wurde nach Erstausstrahlung der "Biene Maja" als „Insekten-Jupp“ oder gar kriminell genannt.
Mit den Aufkommen des Privatfernsehens in den späten 80ern und 90ern wurden eine Vielzahl von Anime Serien im deutschen Fernsehen ausgestrahlt. Jedoch waren die meisten dieser Serien keine bewussten Lizenzen seitens der Fernsehsender, sondern kamen durch italienische/europäische Programmpakete, wo neben westlichen Zeichentrickserien, auch vereinzelt Anime beinhaltet waren. Viele davon sind in anderen europäischen Ländern und Japan schon weit der deutschen Erstausstrahlung gelaufen.
Die Liste alter Serien im deutschen Fernsehen bevor des Moon Toon Zone und Anime@RTL2 Blocks auf RTL2.
Im August 1999 sind Anime ein durchschlagender Erfolg geworden, durch den Programmblock "Moon Toon Zone" von RTL2. Dieser Block bestand aus "Sailor Moon, Dragon Ball" und "Pokemon". Der Block wurde konsequent ausgebaut mit "Anime@RTL2" im Jahr 2001, und "PokitoTV" im Jahr 2004. Durch den Erfolg von RTL2 begann das bewusste lizenzieren von Anime von RTL2 und auch anderen TV Sendern. Der Pay TV Sender K-Toon, MTV, VIVA und VOX sendeten Anime für ein älteres Publikum. Bei den VOX gezeigten Animes stammten die nicht vom Sender selbst, sondern wurden den an dctp verkauften Programmblöcken von Fremdanbietern zur Verfügung gestellt. Auch gab es wieder vereinzelte Serien in den Zeichentrickprogrammen der Pro7Sat1 Gruppe.
Seit 2007 hat RTL2 nur drei neue Anime-Serien ins Programm genommen, bei denen es sich nicht um Fortsetzungen handelte. 2013 wurde das Programm bei RTL2 vollständig abgesetzt. Auch haben alle anderen Sender seit 2007 keine Anime mehr ausgestrahlt. Heute strahlen der Pay TV Sender Animax (seit 2007), Prosieben MAXX (seit 2013) und Nickelodeon, Anime aus.
Erste Nachbearbeitungen von Anime-Serien gab es in Deutschland mit der Ausstrahlung von "Captain Future" beim ZDF. Bis ca. 2001 wurden italienische ("Mila Superstar") oder französische ("Dragon Ball") Fassungen übernommen. Amerikanische Fassungen ("Saber Rider und die Starsheriffs", "Samurai Pizza Cats", "Pokemon") werden seit Anbeginn und bis heute international vermarktet. In Deutschland gibt es seit 1993 die FSF (Freiwillige Selbstkontrolle Fernsehen) die Fernsehinhalte prüft und ob sie für ein Tagesprogramm geeignet sind und eventuell Schnittauflagen nach deutschen Jugendschutz aufgibt. RTL2 hat mehrere FSK12 Inhalte (z. B. Folgen in "Dragon Ball", "Lady Oscar", "Ein Supertrio") während des vermarkteten Kinderprogramm unbearbeitet am Nachmittag gezeigt. Es ist nicht bekannt ob RTL2 damals die Inhalte von der FSF geprüft lassen hat. Laut eines dementierten Interviews im Jahre 2005 gab es keine Beschwerden bezüglich Gewaltdarstellungen von Serien im RTL2 Programm, jedoch subjektive Ansichten. RTL2 nahm zunehmend Schnitte vor, die aber nicht dem Muster der FSF entsprechten. Negativ aufgefallen ist RTL2 vor allem als man Folgen geschnitten hat die auf Heimvideo FSK0 oder FSK6 bekamen. Bei "Naruto" und "Dragon Ball GT" hat man, nicht nur Schnitte sondern auch Dialogverharmlosungen vorgenommen, und weitere Bearbeitungen angewandt. Die von Fans oft kritisierte Fassung von Naruto wird von RTL2, wegen ihrer hinter den Erwartungen hinterbliebenen Quoten, als Grund genannt, dass das „Anime-Fieber“ im TV vorbei ist. Von der FSF geprüften Ausstrahlungen von Tele 5 und Pro7Maxx legten auf, das RTL2 nicht nach Muster der FSF arbeitete.
Die ersten deutschen Kauf-Animes gab es im Jahr 1975 auf sogenannten TED-Bildplatten (Abkürzung von '), analogen Vinyl-Bildplatten, die eine Spieldauer von ca. 10 Minuten pro Seite hatten und nur von einem einzigen Abspielgerät der Firma Telefunken gelesen werden konnten. Sie verschwanden bereits im folgenden Jahr wieder vom Markt. Bei den darauf angebotenen Anime handelte es sich um einzelne Folgen der Serien ', "Hotte Hummel" und '. Der erste Anime-Spielfilm, den es in Deutschland zu kaufen gab, war der Film "Perix der Kater und die 3 Mausketiere" (jap. , ') von Toei Animation aus dem Jahr 1969, der Ende der 1970er-Jahre von der Firma "piccolo film" stark gekürzt auf Super-8-mm-Film angeboten wurde. In der DDR lief er als "Der gestiefelte Kater" im Fernsehen und Kino.
In den 1980er-Jahren erschienen zahlreiche Animes auf VHS-Kassetten, wie im Fernsehen kamen die Veröffentlichungen nicht durch bewusstes lizenzieren Japanischer Lizenzgeber, sondern waren von französischer, amerikanischer und italienischer Abstammung. Die ersten VHS-Kassetten, mit denen gezielt Fans japanischer Animationen angesprochen werden sollten und bei denen ausdrücklich Japan als Produktionsland genannt wurde, stammten aus dem Jahr 1986. Damals wurden unter dem Label „“ einzelne Folgen der Serien "Die Abenteuer der Honigbiene Hutch", "Demetan der Froschjunge" und "Macross" veröffentlicht, wobei darauf geachtet wurde, das Originalmaterial möglichst unverändert zu lassen – der japanische Vor- und Abspann blieben erhalten, und vorkommende Songs wurden japanisch belassen. „“ verschwand jedoch bereits 1987 wieder vom Markt.
Liste der Anime Videos bevor des ersten Anime Labels, OVA Films:
Der erfolgreiche Versuch eines eigenen deutschen Anime-Labels begann mit OVA Films. OVA Films veröffentlichte 1995 den Film ', der in einer offiziellen Auflage von 2500 VHS-Kassetten erschien und zugleich der erste deutsche Kauf-Anime im japanischen Original mit deutschen Untertiteln war. In der Folgezeit wurden immer mehr Anime-Kaufvideos von OVA Films veröffentlicht (z. B. ', ', '). Das dabei auftretende Problem, dass die Fans Originalfassungen mit Untertiteln bevorzugten, während für den Massenmarkt eher synchronisierte Fassungen erforderlich waren, löste sich mit dem Aufkommen der DVD, auf der beide Formate gleichzeitig angeboten werden konnten. Zu OVA Films schlossen sich immer mehr Labels an, wie Anime Virtual (jetzt Kaze, nach Übernahme des Shogakukan Konzern), Anime House, Panini Video, ADV Films, BeezEntertainment, Universum Anime, Tokyopop und Red Planet. Jedoch sind diese Unternehmen außer Universum Anime, Kaze und Anime House insolvent gegangen. Danach neu hinzugekommene Labels sind Nipponart, Peppermint Anime, KSM Anime, Universal Pictures und Film Confect.
Im September 2007 startete das Video-on-Demand-Portal "Anime-on-Demand".
Seit dem Jahr 2011 bietet das Video-Portal MyVideo auch offiziell Anime-Serien an. Das Angebot umfasst überwiegend ältere Titel sowie kürzlich bei ProSieben Maxx ausgestrahlte TV-Folgen.
Im April 2013 startete der Anime-Publisher Peppermint auf der Videoplattform Vimeo den Kanal Peppermint TV, auf dem die Serie "Valvrave The Librator" als deutscher Simulcast angeboten wurde, d. h., die Folgen direkt nach ihrer Erstausstrahlung in Japan mit deutschen Untertiteln angeboten wurde. Das US-Portal Crunchyroll kündigte im April 2013 ebenfalls an deutsche Untertitel anzubieten und begann im Dezember 2013 mit deutschen Simulcasts für mehrere Serien.
Von April 2013 bis Juni 2015 bot auch RTL II Animes im Internet an. Das Angebot sollte als Ersatz zum eingestellten Programmblock im TV dienen. Auch andere TV-Sender haben Animes nach Ausstrahlung zeitlich befristet auf ihren Homepages angeboten.
Im Juli 2013 begann der Publisher Kazé mit "Space Dandy" auf Anime-on-Demand mit Simulcasts, sowie Nipponart mit "Chaika, die Sargprinzessin" auf Clipfish. Letztere Website richtete im selben Zeitraum einen separaten Animebereich ein. Das von japanischen Animationsstudios und Publishern betriebene Portal Daisuki.net, bot im Dezember 2013 den Fernsehfilm "" als deutschen Simulcaststream an. Beim deutschen Netflix-Start zum September 2014 war neben anderen Animeserien auch "Knights of Sidonia" im Katalog das als „Netflix-Original“ beworben wird, da die Website die exklusiven Vertriebsrechte in all ihren Territorien besitzt und mit dem sie im Dezember 2013 in den USA ihr Anime-Angebot startete. Viewster begann im Dezember 2014 mit regulären deutschen Simulcasts von Anime der aktuellen Saison.
Das einzige derzeitige professionelle deutschsprachige Anime-Fachmagazin ist die "AnimaniA", die seit September 1994 erscheint. Dazu kommen Jugendmagazine mit eigenen Anime-Bereichen, wie "Mega Hiro", "Koneko" und "Kids Zone". Der Verein Anime no Tomodachi gibt seit 1997 die Zeitschrift "Funime" heraus.
Von August 1995 bis Dezember 1996 erschien das Magazin "A.M.I." (die Erstausgabe noch unter dem Namen "Project A-nime") im Schaefers-Verlag. Das Magazin wurde nach fünf Ausgaben eingestellt. Weiterhin erschien zwischen Januar 2001 und September 2007 das Magazin "MangasZene".
Außerhalb Asiens sind hauptsächlich die USA, Frankreich und Italien für die Verbreitung von Anime in Nachbarländern wie Spanien, Portugal, Arabien, Lateinamerika und auch Deutschland verantwortlich.
Im Westen sind Anime Serien zum ersten Mal im US Fernsehen aufgetaucht. Dort sind in den 60er Jahren, unter anderem, "Astro Boy"", Kimba, der weiße Löwe", "Gigantor" und "Speed Racer" gelaufen. Danach waren Anime Serien weniger präsent als es in Europa der Fall war. Die populärsten Serien waren Serien im Science Fiction Bereich wie "Star Blazer", "Voltron" und "Robotech". In den späten 90ern sind wie in Deutschland die internationale Vermarktung von den Serien "Sailor Moon", "Pokemon" und "Dragon Ball Z" für die Wahrnehmung von Anime in speziellen verantwortlich gewesen. Auch gab es eine Anzahl von Koproduktionen zwischen USA und Japan, darunter zählen "Das Letzte Einhorn" und "Transformers". Erfolgreiche Ausstrahlungen von Anime Serien hatten Einfluss auf die Cartoon Industrie in den USA selbst. Serien wie "Galaxy Rangers" in den 80ern, sowie "Avatar – Der Herr der Elemente", "Monsuno" und "Teen Titans" in den 2000ern, waren von der Anime Äesthetik beeinflusst.
Im US Fernsehen werden für Anime die im Kinderprogramm laufen, die umfangreichsten Bearbeitungsmaßnahmen unternommen. Diese Fassungen werden dann oft international vermarktet. Der Jugendschutz ist im Vergleich zum europäischen Umfeld in Deutschland, Frankreich etc. weitaus strenger. In den USA stehen den Unternehmen umfangreiche Mittel zur Verfügung um Bilder zu retuschieren, Namen zu ändern, Folgen auszulassen, zusammen zuschneiden und somit die Handlung zu verändern. Auch die Musik wurde teilweise verändert. Auch freizügige, gewalttätige, religiöse, japanisch kulturelle Inhalte und auch Bezüge zu Alkohol, Waffen und Drogen werden entfernt. Auch werden ernsthafte Themen wie Tod umschrieben oder ausgelassen. Diese Maßnahmen unterscheiden sich von Serie zu Serie und auch von Firma zu Firma. Die konsequentesten und umfangreichsten Bearbeitungen finden bei 4Kids ("One Piece", "Yu-Gi-Oh"), Harmony Gold ("Robotech"), Saban Brands ("Digimon", "Glitter Force") und DiC (Sailor Moon) statt.
Weitgehend unbearbeitete Serien haben Popularität durch Videokassetten oder durch Nachtprogramme von Sendern wie Cartoon Network oder SyFy gewonnen. Speziell im Nachtprogrammblock von Cartoon Network sind "Cowboy Bebop" und "Big O" sehr populär geworden. "Space Dandy" (vom Regisseur von "Cowboy Bebop") "" und auch eine zweite Staffel von "Big O" wurde von amerikanischen Geld mitfinanziert. Netflix plant mehrere Serien mitzufinanzieren die dann als Netflix Original beworben werden.
In Frankreich sind Anime zum ersten Mal 1974 aufgetaucht, mit den Serien "Choppy und die. Prinzessin" (Erstausstrahlung folgte in Deutschland in 1996) und "Kimba, der weiße Löwe" (vier Jahre vor der Deutschen Ausstrahlung). Ähnlich wie mit "Wickie und die starken Männer" und "Die Biene Maja", gab es französisch – japanische Koproduktionen wie "Barbapapa", "Odysseus 31" und "Die geheimnisvollen Städte des Goldes""." Mit der Toei Produktion "Grendizer" (auch genannt "Goldorak") wurde 1978 eine Serie ausgestrahlt die maßgeblich dafür verantwortlich war das im Kinderprogramm vermehrt auf Anime gesetzt wurde. Die Serie erreichte hohe Einschaltquoten, löste aber auch große Anfeindungen und Proteste gegenüber gewalthaltige Japanische Produktionen aus. TF1 ist der größte Privatsender Frankreichs und im Kinderprogramm wurde stark auf Anime gesetzt, viele verschiedene Serien waren verantwortlich für die große Fanszene in Frankreich. Hat RTL2 während seiner Laufbahn etwa 60 Serien gezeigt, waren es auf TF1 weit über 100 Serien. AB Productions hat die Serien jedoch als billiges Kinderprogramm angesehen, die diese Massen an Serien dann so im Schnitt und Dialog zusammengestutzt haben. 1997 wurde das Programm auf TF1 nach Protesten und einen Konflikt über Anime der über 15 Jahre anhielt, vollständig abgesetzt. Danach haben sich verschiedene Spartensender gefunden die ein Anime Programm sendeten, während im Kinderprogramm der großen Sender ausschließlich auf sehr kindgerechte Anime gesetzt wurde.
Space Adventure Cobra gilt als der Anime mit den höchsten Kultstatus in Frankreich, Realverfilmungen und Fortsetzungen als eine Koproduktion sind geplant. 2004 wurde Ghost in the Shell 2: Innocence bei den Internationale Filmfestspielen von Cannes 2004 nominiert. Wie in den USA, hatten Anime einen Einfluss auf die heimische Zeichentrickindustrie in Frankreich. "Totally Spies!" und "Wakfu" sind von der Anime Äesthetik beeinflusst.
In Italien war die Resonanz von Anime durchwegs positiv. Seit "Goldorak", wurden beinahe jedes Genre und Format von Japan übernommen. In Italien wurden die meisten Anime außerhalb Japans im Fernsehen und Kino gezeigt. Während in Deutschland nur knapp 20 Serien in den 70ern und 80er gezeigt wurden, waren es in Italien bereits über 200. Der Grund für diese Massenimporte war das Italien bereits 1976 das Fernsehen privatisierte und eine Vielfalt an Sendern hervorgingen. Auch waren Anime die preiswertesten Zeichentrickproduktionen. Koproduktionen mit Japan wie "Calimero", "Z wie Zorro" und "Die Abenteuer des Sherlock Holmes" sind auch entstanden. Eine Vielzahl der Sendungen die in Kinderprogrammen der großen Sender liefen (Rai und Mediaset) wurden konsequent bearbeitet. So hat man Gewalt und freizügige Szenen geschnitten – aber auch Zensur und Veränderungen im Dialog wurden vorgenommen. Thematiken wie der Tod, sexuelle Anspielungen, japanische kulturelle Inhalte, sowie drastische Bilder und Zustände wurden sehr kindgerecht und abgeflacht aufbereitet. Durch die Thematik der Serie "Detektiv Conan" haben sich aber solche Dialogumschreibungen aber wieder gelegt, und wird auch in anderen Serien nicht mehr verwendet. In den 70er, 80er und 90er sind verschiedene Serien unverändert auf verschiedenen Lokalsendern gelaufen, jedoch kam "Fist of the North Star" unter starker Kritik, weshalb auf diesen kleineren Sendern auf Anime verzichtet wurde. 1999 begann mit MTV Italy die erste bewusste Ausstrahlung von Anime für ein älteres Publikum zu einer passenden Sendezeit.
In Italien sind speziell Anime für jüngere Mädchen beliebter als in vielen anderen Ländern, speziell die "Rock ’n’ Roll Kids" ist für 4 Staffeln als Realserie umgesetzt worden. Der mitunter populärste Anime ist "Lupin III", Italien war Mitfinanzierer der vierten Serie des Franchises.
Verschiedene Anime Serien sind in Spanien zunächst auf den öffentlich-rechtlichen Sender Televisión Español gelaufen, jedoch kam "Saint Seiya" unter Kritik und wurde abgesetzt. Auch Koproduktionen wie "Um die Welt mit Willy Fog", "D’Artagnan und die drei MuskeTiere" und "Roy, the Little Cid" sind entstanden. Mit dem Aufkommen des Privatfernsehens im Jahre 1990, ist der Sender Telecinco gestartet der "Saint Seiya" fortgesetzt hat und viele weitere Anime importierte, wo die Anzahl bis an die 100 Serien reichte. Genau wie in Frankreich und Italien hat sich die Wahrnehmung für Anime entwickelt weit bevor Deutschland und den USA. Jedoch kamen viele dieser Serien unter Kritik wegen der Gewaltdarstellung oder auch wegen freizügigeren Szenen wie die kurzen Röcke bei "Sailor Moon" oder Nacktheit bei "Ranma 1/2", und wurden 1999 zeitweilig mit Disney Cartoons ersetzt.
Die zwei kulturell bedeutendsten Anime in Spanien sind "Mazinger Z" und "Shin Chan". Zu "Mazinger Z" gibt es eine Statue in der Stadt Tarragona, und "Shin Chan" hatte mitunter eine größere Beliebtheit in Fernsehen als manche Hauptnachrichtensendungen.
Japanische Animationsfilme haben weltweit eine Fangemeinde, die sich in großen Teilen mit der von Mangas überschneidet. Viele Veranstaltungen widmen sich daher beiden Medien. Eine Fanszene entwickelte sich zunächst in Japan und ab den 1980er Jahren nur in kleinerem Maße in den USA und Frankreich. Mit zunehmender Verbreitung und Popularität von Animes wie auch Mangas nach der Veröffentlichung von "Akira" im Westen und umso mehr nach dem Erfolg von Fernsehserien, darunter "Sailor Moon" und "Dragon Ball", entwickelte sich in Nordamerika und Europa eine größere Fangemeinde. Als die deutsche Fanszene um das Jahr 2000 herum wuchs, war sie noch sehr jung. Einer Umfrage von Sozioland aus dem Jahr 2005 sowie einer Untersuchung des französischen Centre d'Études et de Recherches Internationales zufolge waren die meisten zwischen 14 und 25 Jahren alt. Nur wenige waren über 25, spielten jedoch in der Fanszene eine wichtige Rolle, gründeten die ersten Magazine und Veranstaltungen. 70 bis 85 Prozent der Befragten waren weiblich.
Bedeutende Veranstaltungen, auf denen sich Fans vorrangig treffen, sind Anime- und Manga-Conventions. Diese Conventions bieten Verkaufsstände, Workshops, Autogrammstunden, Konzerte oder Videoabende, Fans verkleiden sich oft als Figur aus einem Anime (Cosplay). Eine der weltweit größten Conventions ist die Japan Expo in Frankreich mit über 230.000 Besuchern. Darüber hinaus finden viele weitere in Europa und Nordamerika statt. In Deutschland war lange Zeit die AnimagiC die von der Besucherzahl größte Anime-Convention. Im Jahre 2011 wurde sie von der Connichi mit fast 25.000 Besuchern abgelöst. Daneben ist Anime auch bei Veranstaltungen zu Japan, Animationsfilm oder Comic ein Thema, so finden sich in Deutschland beim Japantag oder der Frankfurter Buchmesse Veranstaltungen zum japanischen Animationsfilm.

</doc>
<doc id="82" url="https://de.wikipedia.org/wiki?curid=82" title="Actionfilm">
Actionfilm

Der Actionfilm (von engl. "action": Tat, Handlung, Bewegung) ist ein Filmgenre des Unterhaltungskinos, in welchem der Fortgang der äußeren Handlung von zumeist spektakulär inszenierten Kampf- und Gewaltszenen vorangetrieben und illustriert wird. Es geht eher um stimulierende Aktionen als um inhaltliche Zusammenhänge, empathisches Miterleben der Gefühlswelt der Protagonisten oder künstlerisch-ästhetische Bildwelten. Hauptbestandteile von Actionfilmen sind daher meist aufwendig gedrehte Stunts, Schlägereien, Schießereien, Explosionen und Verfolgungsjagden.
Der Actionfilm ist seit den 1960er-Jahren ein eigenständiges Filmgenre, doch seine Konventionen sind bereits seit dem Beginn der Filmgeschichte bekannt. Künstler aus dem Vaudeville wie Buster Keaton ließen ihr Können in artistischer Bewegung in Verbindung mit Tricktechnik in ihr Filmschaffen einfließen. 
Der Actionfilm als eigenes Genre hat seinen Ursprung im Kriminalfilm, in dem in den 1950er-Jahren Aktion und explizite Darstellung von physischer Gewalt zunehmend an Bedeutung gewann, etwa in Stanley Kubricks "Die Rechnung ging nicht auf" (1956). Alfred Hitchcock präsentierte in "Der unsichtbare Dritte" (1959) erstmals eine geschlossene filmische Welt, die ausschließlich als Herausforderung für die physische Aktion der Hauptfigur dient. Dieses Konzept der geschlossenen Actionwelt, die rein zum Ausleben von Körperakrobatik und zur Demonstration spektakulärer Gewaltanwendungstechniken existiert, fand seine Fortsetzung in den Filmen der James-Bond-Reihe und in Fernsehserien wie "Kobra, übernehmen Sie". 
Dieser von realistischer Darstellung und moralischer Wertung weit entfernten Illusionstendenz stehen die Regisseure der Bewegung des New Hollywood gegenüber, die in offener Form Aktion und Gewaltanwendung inszenierten. Sie reagierten auf gesellschaftliche und politische Entwicklungen wie die Protestbewegung und den Vietnamkrieg und suchten den Kontext der Darstellung zu Fragen der Moral, etwa zu den Folgen von Gewaltanwendung auf den menschlichen Körper. Beispiele für diese realistischere und ernüchternde Herangehensweise sind Arthur Penns "Bonnie und Clyde" (1967) und Sam Peckinpahs "The Wild Bunch – Sie kannten kein Gesetz" (1969). 
Mit den Bruce-Lee-Filmen fand eine Ära der Überbetonung physischer Kräfte und des Körperkultes im Actionfilm ihren Anfang. Stilmittel wie Zeitlupe und Tonverfremdungen führten zur Entwicklung und Definition des Subgenres des Martial-Arts-Films. In den 1980er Jahren beherrschte der Actionfilm das Mainstreamkino mit Stars wie Arnold Schwarzenegger und Sylvester Stallone, die durch Bodybuilding den Körperkult auf einen Höhepunkt führten. Reaktionäre Themen wie Rachephantasien und das stereotype Aufbauen von Feindbildern beherrschten das Actionkino. 
In den 1990er Jahren wurde das Genre zunehmend ironisiert und spiegelte sich selbst, etwa in Filmen wie "Last Action Hero" (John McTiernan, 1993) und "True Lies" (James Cameron, 1994). McTiernans "Stirb-langsam"-Reihe (1988 bis 2013) brach ebenfalls ironisch mit dem Heldenbild des Actionfilms und ließ ihren Protagonisten, dargestellt von Bruce Willis, entmystifiziert als leidensfähigen Jedermann gegen das Böse siegen. Stars wie Jackie Chan vereinnahmten den Stunt als Teil der künstlerischen Darstellung und zogen einen Teil ihrer Popularität aus der Tatsache, auch gefährliche Action grundsätzlich selbst zu bewerkstelligen.
Die Bewegung, Grundmotiv des Films, dient im Actionfilm in erster Linie Schauzwecken und hat weniger erzählerische Funktionen. Oft werden im Actionfilm in der Art einer Nummernrevue geschlossene Sequenzeinheiten aneinandergereiht, die der Zurschaustellung unterschiedlichster bewegungsgetriebener Konflikt- oder Duellsituationen dienen, etwa Shootouts, Verfolgungsjagden, Körperkämpfe oder Explosionen. Subjekte der Aktion sind speziell im US-amerikanischen Actionfilm häufig sich verfolgende Fahrzeuge, etwa in "Brennpunkt Brooklyn" (William Friedkin, 1971), "Bullitt" (Peter Yates, 1968) oder "Dirty Harry" (Don Siegel, 1971). Der dargestellten Gewalt wird häufig in wirklichkeitsfremder Weise der Realitätsbezug genommen. Filmische Mittel wie Konvergenzmontage und Parallelmontage strukturieren diese Nummern, etwa um einen Spannungsbogen in einer Last Minute Rescue aufzulösen. 
In den Plots geht es meist um den Kampf zwischen Gut und Böse, die Identifikationsfigur ist häufig ein physisch starker männlicher Held (oder eine weibliche Heldin, siehe beispielsweise ""), der/die in der Regel eindeutige moralische Prinzipien vertritt, die den ethischen und weltanschaulichen Grundlagen der westlichen Kultur entsprechen (Gut gegen Böse, Beschützen der Schwachen, Gerechtigkeit, Sühne für erlittenes Unrecht, Verteidigung und Bewahrung der vertrauten Lebensweise usw.). Häufig fließen erzählerische Elemente aus verwandten Genres in den Actionfilm ein, unter anderem aus dem Abenteuerfilm, dem Kriegsfilm, dem Kriminalfilm, dem Psychothriller, dem Horrorfilm und dem Science-Fiction-Film.

</doc>
<doc id="83" url="https://de.wikipedia.org/wiki?curid=83" title="Al Pacino">
Al Pacino

Alfredo James „Al“ Pacino (* 25. April 1940 in New York) ist ein US-amerikanischer Schauspieler, Filmregisseur, -produzent und Oscar-Preisträger. Er gilt als einer der herausragenden Charakterdarsteller im zeitgenössischen US-amerikanischen Film und Theater.
Al Pacino, geboren in Manhattan, ist der Sohn von Salvatore Pacino, geboren in der sizilianischen Stadt Corleone, und von Rose Gerard, der Tochter eines italienischen Einwanderers und einer italienisch-amerikanischen Mutter, die in New York geboren wurde. Seine Eltern ließen sich scheiden, als er zwei Jahre alt war. Nach der Scheidung zogen Al und seine Mutter in die Bronx, und Pacino wuchs bei seinen sizilianischen Großeltern, die aus der Heimatstadt seines Vaters eingewandert waren, in der New Yorker South Bronx auf.
Sein Vater Salvatore, der nach Covina zog, arbeitete als Versicherungsagent und besaß ein Restaurant, „Pacino’s Lounge“. Das "Pacino’s" wurde in wirtschaftlich schweren Zeiten in den frühen 90er Jahren geschlossen; heute trägt es den Namen "Citrus Grill". Salvatore Pacino starb am 1. Januar 2005 im Alter von 82 Jahren.
Al Pacino ist der Stiefsohn der Schauspielerin und Maskenbildnerin Katherin Kovin-Pacino und hat vier Schwestern: Josette, eine Lehrerin, die Zwillinge Roberta und Paula sowie Desiree, die sein Vater in seiner vierten Ehe adoptierte.
Mit 17 Jahren wurde Pacino der Schule verwiesen und er ging fortan auf die "Manhattan School of Performing Arts". Nebenher arbeitete er in kleineren Theatern als Platzanweiser und Kartenabreißer. Nach seinem Abschluss am renommierten Lee Strasberg Theatre and Film Institute bei dem angesehenen Schauspiellehrer Herbert Berghof begann er eine Schauspielkarriere.
Pacino interessiert sich schon als Kind für die Schauspielerei. Er verfeinerte sein Talent an zwei renommierten New Yorker Schauspielschulen, in den Studios Herbert Berghoff und im Actors Studio. Dort spielte er in mehreren erfolgreichen Theaterstücken wie in seinem Debütstück "The Connection" und in "The Indian Wants the Bronx," für das er mit einem Obie-Award ausgezeichnet wurde.
1969 wirkte er in seiner ersten Hollywood-Produktion "Ich, Natalie" mit. 1971 erhielt er neben Kitty Winn eine Rolle in dem Film "Panik im Needle Park," die ihm den Weg für die Rolle des Michael Corleone in Francis Ford Coppolas "Der Pate" (1972) ebnete und ihm 1973 seine erste Oscar-Nominierung einbrachte.
Nach "Hundstage" wurde es stiller um Pacino. Erst in den 1980er Jahren brachte er sich durch Filme wie Brian De Palmas "Scarface" (1983) und "Sea of Love – Melodie des Todes" (1989) wieder ins Gespräch. Nach einer erneuten Zusammenarbeit mit Coppola in "Der Pate III" (1990) folgte der Thriller "Heat" (1995) mit Schauspielkollege Robert De Niro. Die männliche Hauptrolle in dem Film "Pretty Woman" lehnte er ab.
Seine Darstellung des AIDS-kranken Schwulenhassers Roy Cohn in der Miniserie "Engel in Amerika" (2003) brachte ihm zahlreiche Preise ein und wurde von der Kritik hoch gelobt.
Pacino ist dafür bekannt, seine Rollen bis zum Extrem auszufüllen. Während sein Spiel in den 1970er Jahren - insbesondere in der "Der Pate" - dabei zumeist minimalistisch war, änderte es sich mit seinem Comeback in den 1980er Jahren radikal. Pacinos exaltierte Darstellungen in Filmen wie "Scarface, Im Auftrag des Teufels, An jedem verdammten Sonntag" oder auch "Der Duft der Frauen" wurden von Kritikern oftmals als Overacting bezeichnet. Für einen Großteil des Publikums zeichnet ihn allerdings genau diese Art und Weise als einen der größten Charakterdarsteller der Gegenwart aus. Viele seiner Filme, darunter auch "Glengarry Glen Ross," der zunächst floppte, zählen heute zu den Besten ihres Genres.
Neben seiner Karriere als Filmschauspieler arbeitet er weiterhin regelmäßig an verschiedenen Theatern – sowohl als Darsteller wie auch als Regisseur und Produzent. Für seine Rollen in den Bühneninszenierungen von "The Basic Training Of Pavlo Hummel" von David Rabe und "Does A Tiger Wear A Necktie?" von Don Petersen erhielt er jeweils einen Tony Award.
Als langjähriges Mitglied von David Wheelers "Experimental Theatre Company" in Boston stand er unter anderem in "Richard III." und Bertolt Brechts "Der aufhaltsame Aufstieg des Arturo Ui" auf der Bühne. In New York und London spielte Pacino in David Mamets "American Buffalo," in New York war er der Titelheld in "Richard III." und spielte den Mark Anton in "Julius Cäsar". Außerdem stand er im "Square Theatre" in New York in Oscar Wildes "Salome" auf der Bühne und wirkte in der Uraufführung von Ira Levins Theaterstück "Chinese Coffee" mit.
In der Theatersaison 2010/2011 spielte Pacino in der "Shakespeare in the Park"-Produktion "Der Kaufmann von Venedig" den "Shylock". Mit der Inszenierung gewann Heather Lind, die als Shylocks Tochter "Jessica" auftrat, einen Theatre World Award für ein herausragendes Broadway-Debüt.
Pacinos erstes eigenständiges Projekt war 1996 "Looking for Richard," eine dokumentarische und künstlerische Filmstudie über den Charakter von Shakespeares "Richard III.," bei dem er Regie führte, die Produktion übernahm, das Drehbuch schrieb und die Hauptrolle spielte.
Pacino war auch Produzent, Hauptdarsteller und Koautor des Independent-Kurzfilms "The Local Stigmatic," einer Verfilmung des gleichnamigen Theaterstücks von Heathcote Williams, das sowohl im New Yorker Museum of Modern Art als auch im "Public Theatre" aufgeführt wurde.
Al Pacino war nie verheiratet. Er hat drei Kinder, eine Tochter mit Jan Tarrant und Zwillinge (Tochter und Sohn, * 2001) mit Beverly D’Angelo.
Al Pacino wurde im Lauf der Jahrzehnte von verschiedenen deutschen Sprechern synchronisiert. Nachdem er im ersten Jahrzehnt seiner Karriere in der Regel von Lutz Mackensy gesprochen wurde (u.a. "Der Pate I und II, Serpico, Hundstage)," übernahm mit "Scarface" (1983) Frank Glaubrecht die Synchronisation des Schauspielers. Seit 1995 ist Glaubrecht alleiniger Sprecher "(Heat, City Hall, Insomnia" etc.) und er kann mittlerweile als Pacinos Standardstimme bezeichnet werden.
In den frühen 1990er Jahren wurde Pacino auch von Joachim Kammer "(Dick Tracy)," Gottfried Kramer "(Der Pate 3)" und Klaus Kindler "(Carlito's Way)" gesprochen.

</doc>
<doc id="88" url="https://de.wikipedia.org/wiki?curid=88" title="Alkohole">
Alkohole

Alkohole (, eigentlich: feines Antimonpulver) sind chemische Verbindungen, die eine oder mehrere an aliphatische Kohlenstoffatome gebundene Hydroxygruppen (–O–H) haben. Alkohole, die sich von den Alkanen ableiten, werden Alkanole genannt. Um eine klare Abgrenzung der Alkohole von Halbacetalen oder Carbonsäuren sicherzustellen, kann man ergänzen, dass das Kohlenstoffatom (sp-hybridisiert, siehe auch Enole) mit der Hydroxygruppe nur noch mit Kohlenstoff- oder Wasserstoffatomen gebunden sein darf.
Ist die Hydroxygruppe an ein Kohlenstoffatom gebunden, das Teil eines aromatischen Ringes ist, so werden die Verbindungen als Phenole bezeichnet. Sie zählen nicht zu den Alkoholen, da diese Hydroxygruppen analog einer Carboxygruppe sauer regieren.
Der Name einfacher Alkohole ergibt sich als Zusammensetzung aus dem Namen des ursprünglichen Alkans und der Endung . Zusätzlich wird die Position der OH-Gruppe durch eine vorangestellte Zahl verdeutlicht, zum Beispiel . Eine veraltete, bis 1957 gültige Bezeichnung für Alkohole ist – nach einem Vorschlag von Hermann Kolbe – "Carbinole".
Die Gruppe wird nach verschiedenen Kriterien (Zahl der Nichtwasserstoffnachbarn, Wertigkeit, Vorhandensein von Doppel-/Dreifachbindungen und Kettenlänge) eingeteilt.
Man unterscheidet Alkohole nach der Zahl der Nichtwasserstoffnachbarn des Kohlenstoffatoms, an welchem sich die Hydroxygruppe befindet. Bei primären Alkoholen trägt es zwei, bei sekundären ein und bei tertiären kein Wasserstoffatom. Ein Sonderfall ist das Methanol, das neben der Hydroxygruppe drei Wasserstoffatome am Kohlenstoffatom trägt.
Ist mehr als eine Hydroxygruppe in einem Alkoholmolekül vorhanden, wird deren Anzahl durch Einfügen einer der Anzahl der Hydroxygruppen entsprechenden griechischen Silbe (-di-, -tri- usw.) vor der Endung angegeben und man spricht von mehrwertigen Alkoholen. Ein ist das (Trivialname Ethylenglycol), ein das (Trivialname Glycerin). Die Zahl vor der Endung gibt die Position der funktionellen Gruppe(n) an. Dies gilt auch für einwertige Alkohole, zum Beispiel (Trivialname Isopropanol).
In Bezug auf das Vorhandensein von Doppel- bzw. Dreifachbindungen unterscheidet man Alkanole, Alkenole und Alkinole sowie den Spezialfall der meist instabilen Enole.
Über die Kettenlänge werden Alkohole ebenfalls unterschieden. Die Bezeichnung "Fettalkohole" verwendet man für Alkohole mit endständiger primärer mit gerader Kette und einer Länge von sechs (Hexanol) bis hin zu 22 (Behenylalkohol) Kohlenstoffatomen. Sie werden meist durch Reduktion der aus Fettsäuren gewonnen. Die höheren primären Alkohole mit 24 bis 36 Kohlenstoffatome bezeichnet man als Wachsalkohole.
In allen aliphatischen Alkoholen ist die Hydroxygruppe an ein sp-hybridisiertes Kohlenstoffatom (C-Atom mit 4 Substituenten) gebunden. Ein Spezialfall sind die meist instabilen Enole, bei denen die Hydroxygruppe an ein sp-hybridisiertes Kohlenstoffatom gebunden ist.
Niedrigmolekulare Alkohole sind Flüssigkeiten, die einen charakteristischen Geruch und einen brennenden Geschmack besitzen. Höhere Alkohole sind meist feste Verbindungen mit nur schwach ausgeprägtem Geruch. Aufgrund von intermolekularen Wasserstoffbrückenbindungen besitzen die Alkohole im Vergleich zu Kohlenwasserstoffen gleicher Molekülmasse relativ hohe Schmelz- und Siedepunkte. Wichtigstes gemeinsames Merkmal der Alkohole ist die Hydrophilie. Diese Eigenschaft nimmt mit zunehmender Länge des Alkylrestes ab und mit der Anzahl der Hydroxygruppen zu. Besonders die kurzkettigen Alkohole werden aufgrund ihres amphiphilen Charakters oft als Lösungsmittel verwendet.
Sauerstoff ist elektronegativer als Wasserstoff und Kohlenstoff, d. h., er zieht Elektronen stärker an als diese. Das führt zu einer unsymmetrischen Verteilung der Elektronen entlang der , man spricht von einer polaren Bindung, es bildet sich ein molekularer Dipol aus. Diese Dipole können untereinander Wasserstoffbrückenbindungen ausbilden, die die Anziehung der einzelnen Moleküle untereinander drastisch verstärken.
Dies führt für Alkohole zu relativ hohen Siedepunkten gegenüber den um eine Methyleneinheit verlängerten Homologen ihrer Stammverbindung, die eine annähernd gleiche molarer Masse besitzen. So hat beispielsweise das unpolare Ethan (CH) (M = 30) einen Siedepunkt von −89 °C, während Methanol (CHOH) (M = 32) diesen erst bei 65 °C erreicht.
Zusammenfassend:
Die OH-Gruppe ist ebenfalls in der Lage, Wasserstoffbrückenbindungen mit Wasser einzugehen. Sie erhöht damit die Hydrophilie, die Wasserlöslichkeit, der Verbindung. Organische Alkylreste selbst sind nicht wasserlöslich, also hydrophob. Die Wasserlöslichkeit sinkt daher mit der Größe des organischen Anteils und steigt mit der Zahl der Hydroxygruppen. Die Propanole und "tert"-Butanol sind bei Raumtemperatur noch in jedem Verhältnis mit Wasser mischbar, alle langkettigeren Alkohole lösen sich nur noch in zunehmend kleinen Mengen. Größere Mengen gelöster anorganischer Salze können auch bei den kurzkettigen Alkoholen eine Phasentrennung bewirken („Salzfracht“).
Zusammenfassend:
Der pK-Wert (Säurestärke) von Alkoholen liegt bei etwa 15. Sie reagieren somit in wässriger Lösung näherungsweise neutral. Es ist möglich, sie mit einer starken Base zu deprotonieren. Die deprotonierte Form eines Alkohols heißt "Alkoholat". Die Acidität von Alkoholen nimmt in der Reihe von Methanol über primäre, sekundäre und tertiäre
Alkohole ab. Ebenso ist es möglich, sie in gewissem Umfang mit starken Säuren zu protonieren:
Industriell werden Alkoholate durch Umsetzung der entsprechenden Alkohole mit elementarem Natrium hergestellt:
Im IR-Spektrum von Alkoholen ist deutlich die breite Bande der O–H-Valenzschwingung im Bereich von 3200–3650 cm zu erkennen. Die Breite des Peaks wird durch Wasserstoffbrückenbindungen mit Wassermolekülen verursacht und ist in Spektren von wasserfreien Alkoholen in einem engeren Bereich von 3620–3650 cm zu finden.
Unterhalb von 140 °C bildet sich der Ester der Schwefelsäure.
Bei etwa 140 °C findet die Kondensationsreaktion zu einem Ether statt.
Oberhalb von 170 °C werden primäre Alkohole zu Alkenen dehydratisiert. (Eliminierung)
Die Selenoxid-Eliminierung ist eine milde Variante der Eliminierung.
Mit Carbonsäuren reagieren Alkohole unter Wasserabgabe zu Estern, diese Reaktion wird auch Veresterung genannt. Diese Reaktion wird durch Säuren katalysiert.
Primäre Alkohole lassen sich zu Aldehyden und Carbonsäuren, sekundäre Alkohole zu Ketonen oxidieren. Tertiäre Alkohole lassen sich nicht weiter oxidieren, es sei denn unter Zerstörung des Kohlenstoffgerüsts.
Zur Oxidation von primären Alkoholen zur Carbonsäure können Chrom(VI)-haltige Oxidationsmittel eingesetzt werden, wie sie z. B. bei der Jones-Oxidation Anwendung finden. Als chromfreies, weniger giftiges Reagenz steht wässriges Rutheniumtetroxid zur Verfügung.
Die Oxidation eines primären Alkohols kann unter Verwendung bestimmter Chrom(VI)-Verbindungen wie dem Collins-Reagenz auch nur bis zur Stufe des Aldehyds erfolgen. Entscheidend ist, dass wasserfreie Lösungsmittel eingesetzt werden. Ist kein Wasser anwesend, kann keine Hydratisierung zum geminalen Diol des Aldehyds (Aldehydhydrate) stattfinden.
Da lösliche Chromate sehr giftig sind, sowie karzinogene und mutagene Eigenschaften besitzen, wurden alternative Methoden zur Oxidation von Alkoholen entwickelt. Eine häufig zur Anwendung kommende Methode ist die Swern-Oxidation mit aktiviertem Dimethylsulfoxid. Fast alle Methoden eignen sich ebenfalls für die Oxidation sekundärer Alkohole zu Ketonen. Die folgende Aufzählung liefert eine Übersicht der wichtigsten Methoden.
Oxidation zur Carbonsäure/zum Keton:
Oxidation zum Aldehyd/zum Keton:
Mit Aldehyden reagieren Alkohole in Gegenwart saurer Katalysatoren zu Halbacetalen bzw. Acetalen.
Viele Alkohole sind wichtige Lösungsmittel, die sowohl in der Industrie, als auch im Haushalt eingesetzt werden; die mengenmäßig wichtigsten sind Methanol, Ethanol, 2-Propanol und n-Butanol. Im Jahr 2011 wurden weltweit etwa 6,4 Mio. Tonnen dieser alkoholischen Lösungsmittel nachgefragt.
Wenn man zu einer flüssigen Alkoholprobe Natrium hinzufügt, so entsteht Wasserstoff, welches man mit der Knallgasprobe nachweisen kann. Diese Methode gilt zwar als Alkoholnachweis, ist jedoch nicht eindeutig, da alle ausreichend protischen Substanzen, zum Beispiel Carbonsäuren, anorganische Säuren oder Wasser, die gleiche Reaktion eingehen.
Der Umsatz von Alkoholen mit Dichromaten in schwefelsaurer Lösung ist geeignet, um Alkohole quantitativ nachzuweisen und wurde früher in den Alcotest-Röhrchen eingesetzt:
Das Nachweisprinzip beruht auf dem Farbumschlag von gelb-orange (saure Dichromatlösung) nach grün (Chrom(III)-Ionen) und kann spektralphotometrisch gemessen werden.
Eine weitere Möglichkeit besteht in der Umsetzung mit Ammoniumcer(IV)-nitrat. Hierbei wird eine konzentrierte Lösung von Ammoniumcer(IV)-nitrat mit einer verdünnten Lösung der unbekannten Substanz versetzt. Enthält die unbekannte Substanz Alkohol-Gruppen, färbt sich das Gemisch rot (manchmal auch grün). Enthält die Substanz Phenole, fällt ein brauner Niederschlag aus.
Der Grund für diese Farbreaktion ist eine Komplexbildung, genauer gesagt eine Ligandensubstitution, bei der ein Alkohol/Phenol mit dem Sauerstoffatom am Cer(IV) koordiniert. Durch die Veränderung der Ligandensphäre verändert sich die Farbe des Cer(IV) von hellgelb zu rot/grün/braun. Leicht oxidierbare Alkohole/Phenole können einen negativen Nachweis ergeben, indem sie das Cer(IV) zu Cer(III) reduzieren.
Der Nachweis des Substitutionsgrades eines Alkohols, also ob es sich dabei um einen primären, sekundären oder tertiären Alkohol handelt, erfolgt über nucleophile Substitution der OH-Gruppe gegen Chlorid durch die Lucas-Probe. Die Substitution hat zur Folge, dass sich die entstehende Substanz nicht mehr in Wasser löst und damit eine eigene Phase ausbildet.
Dabei ist die Geschwindigkeit dieser Phasenbildung entscheidend:
Voraussetzung für diesen Test ist, dass sich der ursprüngliche Alkohol in Wasser löst. Auch darf keine andere unter den Reaktionsbedingungen substituierbare Gruppe vorliegen.
Die eindeutige Identifizierung eines unbekannten Alkohols erfolgt entweder spektroskopisch oder durch Synthese eines charakteristischen Derivates, das einen Schmelzpunkt hat, der von den Schmelzpunkten gleicher Derivate ähnlicher Alkohole gut zu unterscheiden ist. Oftmals werden sie über Ester der oder der identifiziert. Hierzu wird die zu analysierende Substanz in Gegenwart geringer Mengen Schwefelsäure umgesetzt. Die Schmelzpunkte dieser Derivate sind in der Regel scharf.
Die Derivate der besitzen in der Regel höhere Schmelzpunkte als die der . Sie werden dann bevorzugt gewählt, wenn der Schmelzpunkt mit der zu niedrig ist und keine genaue Bestimmung mehr möglich wird.

</doc>
<doc id="89" url="https://de.wikipedia.org/wiki?curid=89" title="The Visitors">
The Visitors

The Visitors ist das achte und letzte Studioalbum der schwedischen Popgruppe ABBA. Es wurde erstmals am 30. November 1981 in Schweden veröffentlicht. Die Aufnahmen im Studio dauerten von März bis November 1981. Etwa zeitgleich mit der LP wurde "One of Us" als Single herausgegeben und zum letzten internationalen Charterfolg der Gruppe, während die nachfolgende Singleauskopplung "Head over Heels" floppte. "The Visitors" war zudem die erste veröffentlichte Audio-CD der Musikgeschichte und wurde am 17. August 1982 vorgestellt.
Im Vergleich zu den Vorgängeralben von ABBA weist dieses einige musikalische Eigenheiten auf, die zuvor in den Produktionen der Gruppe kaum Anwendung fanden. So wird kein einziges Stück auf der gesamten LP von den beiden ABBA-Sängerinnen Agnetha Fältskog und Anni-Frid Lyngstad durchgehend gemeinsam gesungen. Die Lead Vocals wurden in jedem der neun Lieder auf eine der beiden aufgeteilt. Zudem wurden viele Elemente, die bis dato charakteristisch für die Musik der Gruppe gewesen war, durch neue ersetzt. So handeln die Texte von Trennung, Abschiedschmerz und Kriegsangst. 
Benny Andersson und Björn Ulvaeus begannen im Februar 1981 mit dem Komponieren für ein neues Studioalbum. Die am 12. Februar 1981 öffentlich bekannt gegebene Scheidung der beiden ABBA-Mitglieder Andersson und Anni-Frid Lyngstad überschattete dabei die Arbeit innerhalb der Band. Die Aufnahmen für drei neue Stücke begannen am 16. März 1981 in den Polar Music Studios in Stockholm. Darunter befand sich mit "When All Is Said And Done" eine Pop-Ballade, deren Text von einer zerbrochenen Beziehung handelt und sich auf die Trennung von Lyngstad und Andersson beziehen sollte. Dementsprechend übernahm Lyngstad die Lead Vocals und ließ in ihre stimmliche Darbietung auch die emotionalen Aspekte mit einfließen, die angesichts ihrer damaligen Privatsituation gegeben waren. 
Auch "Slipping Through My Fingers" gehörte zu den ersten drei eingespielten Songs. Ulvaeus hatte die Idee für den Text, nachdem er seine Tochter Linda dabei beobachtete wie sie zur Schule ging. Er handelt von den gemischten Gefühlen der Eltern beim Heranwachsen ihrer Kinder und der damit verbundenen Einsamkeit und Nostalgie. Hier übernahm Agnetha Fältskog den Leadgesang und erzählte später, der Song habe sich „sehr echt angefühlt“. Eine symbolische Kulisse der Küche und des Frühstückstisches, die im Text des Songs vorkommen, befindet sich heute im ABBA-Museum auf Djurgården in Stockholm.
Ein weiterer Song, dessen Grundspuren im März 1981 eingespielt wurde, war "Two For The Price Of One", bei dem zur Abwechslung Ulvaeus den Hauptgesang übernahm. Dieser meinte später, der Song hätte trotz des trivialen Textes ein Hit werden können, hätten Fältskog und Lyngstad die Lead Vocals gesungen. Etwa zur selben Zeit wurde das bisherige analoge Aufnahmegerät im Tonstudio durch ein digitales mit 32 Spuren ersetzt. Zwischen 27. und 29. April 1981 wurden "Slipping Through My Fingers" und "Two For The Price Of One" für das Fernsehspecial "Dick Cavett Meets ABBA" live aufgezeichnet, neben Darbietungen einiger älterer ABBA-Songs. 
Am 26. Mai wurde ein erster Versuch unternommen, "Like An Angel Passing Through My Room" aufzunehmen. Diese erste Demoversion trug den Arbeitstitel "Twinkle Twinkle Little Star" und wurde von Ulvaeus gesungen. Eine weitere Alternative zur selben Melodie entstand mit "Another Morning Without You", bei der Lyngstad solo sang. Bis zum Ende der Aufnahmesessions wurden weitere Varianten mit verschiedenen Instrumentierungen und Lead Vocals ausprobiert. Eine Version, die beide ABBA-Sängerinnen beinhaltete, wurde später verworfen und durch eine Variante ersetzt, in der Lyngstad vollständig alleine singt. Für die Deluxe Edition des Albums wurde 2012 ein neun-minütiges Medley zusammengestellt, das die Entstehung von "Like An Angel Passing Through My Room" über mehrere Alternativversionen dokumentiert.
Am 2. September 1981 war Aufnahmebeginn für "I Let The Music Speak" und "Head over Heels". Besonders bei letzterem hatten die Musiker Probleme. Da die neue digitale Studiotechnik sehr präzise arbeitete, konnte Toningenieur Michael B. Tretow nur noch mäßig mit Verzögerung und Verschiebung der Spuren experimentieren, was die Aufnahmen „trockener“ und „kälter“ wirken ließ. So wirkte "Head over Heels" trotz eingängiger Melodie und ausgefeiltem Arrangement „wie eingefroren“, zählt aber dennoch zu den schwungvolleren und fröhlicheren Songs des Albums. Währenddessen zeugte "I Let The Music Speak" einmal mehr von den Ambitionen der beiden Komponisten, ein Musical zu schreiben. Parallel wurde "Should I Laugh Or Cry" produziert, das später nicht auf der Titelliste des Albums erschien, weil es als „nicht stimmig genug“ angesehen wurde. Stattdessen wurde es im Dezember 1981 als B-Seite veröffentlicht.
Am 15. Oktober 1981 gingen die Aufnahmen mit "Soldiers", "One of Us" und das für das Album namensgebende Lied "The Visitors" zu Ende. Der Text des ersteren handelte in Zusammenhang mit dem Kalten Krieg von Kriegsangst und der Furcht vor einer möglichen neuen faschistischen Bewegung. "One of Us" handelte wiederum von den Folgen einer zerbrochenen Beziehung und wurde am 21. Oktober begonnen. Das Stück wurde später als erste Single aus dem Album ausgekoppelt und avancierte zum letzten internationalen Hit der Gruppe. Der Titelsong wurde ab 22. Oktober 1981 aufgenommen. Der Text von "The Visitors" handelt von Überwachung und Besuchen der Sicherheitspolizei bei Dissidenten in der Sowjetunion. Mitte November waren die Aufnahmen und Nachbearbeitungen von allen Songs abgeschlossen.
Das Album-Cover wurde von Rune Söderqvist zusammen mit dem Fotografen Lasse Larsson entworfen. Söderqvist stellte die Gruppe im kalten, ungeheizten Atelier des Malers Julius Kronberg im Freilichtmuseum Skansen, Stockholm, in einer düsteren Stimmung dar. Im Hintergrund hing ein großes Gemälde von Julius Kronberg, auf dem ein Engel dargestellt war. Das Cover zeigte die Gruppe zum ersten Mal nicht mehr als Gemeinschaft, sondern jedes Mitglied für sich allein. Diese düstere Stimmung innerhalb der Gruppe spiegelte sich sowohl in den Melodien als auch in den Texten vieler Songs wider. Das Cover ist laut "The Making of The Visitors" auch das Resultat einer gewissen Erschöpfung aufgrund des jahrelangen Zusammenseins als Gruppe.

</doc>
<doc id="93" url="https://de.wikipedia.org/wiki?curid=93" title="Aluminium">
Aluminium

Aluminium ist ein chemisches Element mit dem Elementsymbol "Al" und der Ordnungszahl 13. Im Periodensystem gehört Aluminium zur dritten Hauptgruppe und zur 13. IUPAC-Gruppe, der Borgruppe, die früher auch als "Gruppe der Erdmetalle" bezeichnet wurde.
Aluminium ist ein silbrig-weißes Leichtmetall. In der Erdhülle ist es, auf den Massenanteil (ppmw) bezogen, nach Sauerstoff und Silicium das dritthäufigste Element und in der Erdkruste das häufigste Metall.
Weltweit wurden 2014 insgesamt 108 Mio. Tonnen Aluminium hergestellt (davon 53 Mio. Tonnen Primäraluminium). In Deutschland wurden 2014 etwa 1,1 Mio Tonnen hergestellt (davon 531.000 Tonnen Primäraluminium). Das Metall ist sehr unedel und reagiert an frisch angeschnittenen Stellen bei Raumtemperatur mit Luft und Wasser zu Aluminiumoxid. Dies bildet aber sofort eine dünne, für Luft und Wasser undurchlässige Schicht (Passivierung) und schützt so das Aluminium vor Korrosion.
Im Vergleich zu Gold, Silber, Kupfer, Zinn, Blei, Eisen und Quecksilber, die schon in der Antike bekannt waren, ist Aluminium ein junges Metall. Der englische Chemiker Humphry Davy beschrieb es im Jahre 1808 als „Aluminum“ und versuchte sich an seiner Darstellung. Diese gelang jedoch erst 1825 dem Dänen Hans Christian Ørsted durch Reaktion von Aluminiumchlorid (AlCl) mit Kaliumamalgam, wobei Kalium als Reduktionsmittel diente:
Friedrich Wöhler verwendete 1827 die gleiche Methode, verwendete zur Reduktion jedoch metallisches Kalium und erhielt damit ein reineres Aluminium. Zu jener Zeit kostete Aluminium mehr als Gold.
Henri Étienne Sainte-Claire Deville verfeinerte den Wöhler-Prozess im Jahr 1846 und publizierte ihn 1859 in einem Buch. Durch den verbesserten Prozess stieg die Ausbeute bei der Aluminiumgewinnung, damit fiel der Aluminiumpreis innerhalb von zehn Jahren auf ein Zehntel.
1886 wurde unabhängig voneinander durch Charles Martin Hall und Paul Héroult das nach ihnen benannte Elektrolyseverfahren zur Herstellung von Aluminium entwickelt: der Hall-Héroult-Prozess. 1889 entwickelte Carl Josef Bayer das nach ihm benannte Bayer-Verfahren zur Isolierung von reinem Aluminiumoxid aus Bauxiten. Aluminium wird noch nach diesem Prinzip großtechnisch hergestellt.
Am Ende des 19. Jahrhunderts stand das Metall in solchem Ansehen, dass man daraus gefertigte Metallschiffe auf den Namen Aluminia taufte.
Der Elementname leitet sich vom lateinischen Wort für Alaun ab. Zwei Namen für das Element sind in Gebrauch: "Aluminium" und "Aluminum". Weltweit dominiert das erstere, während in den Vereinigten Staaten und im kanadischen Englisch der zweite Name geläufiger ist. Die International Union of Pure and Applied Chemistry (IUPAC) beschloss im Jahr 1990, dass der Elementname "Aluminium" laute, akzeptierte drei Jahre später aber "Aluminum" als mögliche Variante.
Aluminium ist mit einem Anteil von 7,57 Gewichtsprozent nach Sauerstoff und Silicium das dritthäufigste Element der Erdkruste und damit das häufigste Metall. Allerdings kommt es aufgrund seines unedlen Charakters praktisch ausschließlich in gebundener Form vor. Die größte Menge befindet sich chemisch gebunden in Form von Alumosilicaten, in denen es in der Kristallstruktur die Position von Silicium in Sauerstoff-Tetraedern einnimmt. Diese Silicate sind zum Beispiel Bestandteil von Ton, Gneis und Granit.
Seltener wird Aluminiumoxid in Form des Minerals Korund und seiner Varietäten Rubin (rot) und Saphir (farblos, verschiedenfarbig) gefunden. Die Farben dieser Kristalle beruhen auf Beimengungen anderer Metalloxide. Korund hat mit fast 53 Prozent den höchsten Aluminiumanteil einer Verbindung. Einen ähnlich hohen Aluminiumanteil haben die noch selteneren Minerale Akdalait (etwa 51 Prozent) und Diaoyudaoit (etwa 50 Prozent). Insgesamt sind bisher (Stand: 2010) 1156 aluminiumhaltige Minerale bekannt.
Das einzige wirtschaftlich wichtige Ausgangsmaterial für die Aluminiumproduktion ist Bauxit. Vorkommen befinden sich in Südfrankreich (Les Baux), Guinea, Bosnien und Herzegowina, Ungarn, Russland, Indien, Jamaika, Australien, Brasilien und den Vereinigten Staaten. Bauxit enthält ungefähr 60 Prozent Aluminiumhydroxid (Al(OH) und AlO(OH)), etwa 30 Prozent Eisenoxid (FeO) und Siliciumdioxid (SiO).
Bei der Herstellung unterscheidet man "Primäraluminium", auch "Hüttenaluminium" genannt, das aus Bauxit gewonnen wird, und "Sekundäraluminium" aus Aluminiumschrott. Die Wiederverwertung benötigt nur etwa 5 Prozent der Energie der Primärgewinnung.
Infolge der Passivierung kommt Aluminium in der Natur auch sehr selten elementar (gediegen) vor, meist in Form von körnigen bis massigen Aggregaten, kann in seltenen Fällen aber auch tafelige Kristalle bis etwa einen Zentimeter Größe entwickeln. Von der International Mineralogical Association (IMA) ist es daher in der Mineralsystematik nach Strunz unter der System-Nummer 1.AA.05 beziehungsweise veraltet unter I/A.03-05 als Mineral anerkannt. Gediegenes Aluminium konnte bisher (Stand: 2010) an rund 20 Fundorten nachgewiesen werden: In Aserbaidschan, Bulgarien, der Volksrepublik China (Guangdong, Guizhou und Tibet), Italien, Russland (Ostsibirien, Ural) und in Usbekistan. Auch auf dem Mond ist gediegenes Aluminium gefunden worden. Aufgrund der extremen Seltenheit hat gediegenes Aluminium keine Bedeutung als Rohstoffquelle.
Aluminiummetall wird elektrolytisch aus einer Aluminiumoxidschmelze hergestellt. Da dieses aus den auf der Erde allgegenwärtigen Alumosilicaten nur schwer isoliert werden kann, erfolgt die großtechnische Gewinnung aus dem relativ seltenen, silikatärmeren Bauxit. Zur Gewinnung von reinem Aluminiumoxid aus Silikaten gibt es seit langem Vorschläge, deren Umsetzung allerdings bis heute nicht wirtschaftlich möglich ist.
Das im Erz enthaltene Aluminiumoxid/-hydroxid-Gemisch wird zunächst mit Natronlauge aufgeschlossen (Bayer-Verfahren, Rohrreaktor- oder Autoklaven-Aufschluss), um es von Fremdbestandteilen wie Eisen- und Siliciumoxid zu befreien und wird dann überwiegend in Wirbelschichtanlagen (aber auch in Drehrohröfen) zu Aluminiumoxid (AlO) gebrannt.
Der trockene Aufschluss (Deville-Verfahren) hat dagegen keine Bedeutung mehr. Dabei wurde feinstgemahlenes, ungereinigtes Bauxit zusammen mit Soda und Koks in Drehrohröfen bei rund 1200 °C kalziniert und das entstehende Natriumaluminat anschließend mit Natronlauge gelöst.
Die Herstellung des Metalls erfolgt in Aluminiumhütten durch Schmelzflusselektrolyse von Aluminiumoxid nach dem Kryolith-Tonerde-Verfahren (Hall-Héroult-Prozess). Zur Herabsetzung des Schmelzpunktes wird das Aluminiumoxid zusammen mit Kryolith geschmolzen (Eutektikum bei 963 °C). Bei der Elektrolyse entsteht an der den Boden des Gefäßes bildenden Kathode Aluminium und an der Anode Sauerstoff, der mit dem Graphit (Kohlenstoff) der Anode zu Kohlenstoffdioxid und Kohlenstoffmonoxid reagiert. Die Graphitblöcke, welche die Anode bilden, brennen so langsam ab und werden von Zeit zu Zeit ersetzt. Die Graphitkathode (Gefäßboden) ist gegenüber Aluminium inert. Das sich am Boden sammelnde flüssige Aluminium wird mit einem Saugrohr abgesaugt.
Aufgrund der hohen Bindungsenergie durch die Dreiwertigkeit des Aluminiums ist der Prozess recht energieaufwändig. Pro produziertem Kilogramm Rohaluminium müssen 12,9 bis 17,7 Kilowattstunden an elektrischer Energie eingesetzt werden. Eine Reduzierung des Strombedarfs ist nur noch in geringem Ausmaß möglich, weil die Potentiale für energetische Optimierungen weitgehend erschlossen sind. Aluminiumherstellung ist daher nur in der Nähe preiswert zur Verfügung stehender Elektroenergie wirtschaftlich, beispielsweise neben Wasserkraftwerken, wie in Rheinfelden.
Die nachfolgende Tabelle zeigt die Aluminiumproduktion und die maximal mögliche Produktionsleistung der Hüttenwerke nach Ländern.
Um Aluminium zu recyceln, werden Aluminiumschrotte und „Krätzen“ in Trommelöfen eingeschmolzen. „Krätze“ ist ein Abfallprodukt bei der Verarbeitung von Aluminium und bei der Herstellung von Sekundäraluminium. Krätze ist ein Gemisch aus Aluminiummetall und feinkörnigen Oxidpartikeln und wird beim Schmelzen von Aluminium bei 800 °C aus dem Aluminiumoxid der normalen Aluminiumkorrosion und als Oxidationsprodukt (Oxidhaut) beim Kontakt von flüssigem Aluminium mit Luftsauerstoff gebildet. Damit beim Aluminiumgießen keine Aluminiumoxidpartikel in das Gußteil gelangen, wird die Krätze durch "Kratz"vorrichtungen von der Oberfläche des Metallbads abgeschöpft.
Um die Bildung von Krätze zu verhindern, wird die Oberfläche der Schmelze mit Halogenidsalzen (rund zwei Drittel NaCl, ein Drittel KCl und geringe Mengen Calciumfluorid CaF) abgedeckt (siehe dazu Aluminiumrecycling). Dabei entsteht als Nebenprodukt Salzschlacke, die noch ca. 10 Prozent Aluminium enthält, die, entsprechend aufbereitet, als Rohstoff für mineralische Glasfasern dient.
Allerdings steht das Sekundäraluminium im Ruf, dass beim Recycling pro Tonne jeweils 300 bis 500 Kilogramm Salzschlacke, verunreinigt mit Dioxinen und Metallen, entsteht, deren mögliche Wiederverwertung aber Stand der Technik ist.
Aluminium ist ein relativ weiches und zähes Metall. Die Zugfestigkeit von absolut reinem Aluminium liegt bei 45 N/mm², die Streckgrenze bei 17 N/mm² und die Bruchdehnung bei 60 %, während bei handelsüblich reinem Aluminium die Zugfestigkeit bei 90 N/mm² liegt, die Streckgrenze bei 34 N/mm² und die Bruchdehnung bei 45 %. Die Zugfestigkeit seiner Legierungen liegt dagegen bei bis zu 710 N/mm² (Legierung 7068). Sein Elastizitätsmodul liegt bei etwa 70.000 MPa.
Der Schmelzpunkt liegt bei 660,4 °C und der Siedepunkt bei 2470 °C. Die Schmelztemperatur ist deutlich niedriger als die von Kupfer (1084,6 °C), Gusseisen (1147 °C) und Eisen (1538 °C).
Mit einer Dichte von 2,70 g/cm³ (etwa ein Drittel von Stahl) ist Aluminium ein typisches Leichtmetall.
Aluminium besitzt mit 237 W/(m·K) eine hohe Wärmeleitfähigkeit; als Metalle liegen nur Silber mit 429 W/(m·K), Kupfer mit 401 W/(m·K) und Gold mit 314 W/(m·K) darüber. Der Wärmeausdehnungskoeffizient ist durch den recht niedrigen Schmelzpunkt mit 23,1 μm·m·K recht hoch.
Bei einer Sprungtemperatur von 1,2 K wird reines Aluminium supraleitend.
Das reine Leichtmetall Aluminium hat aufgrund einer sich sehr schnell an der Luft bildenden dünnen Oxidschicht ein stumpfes, silbergraues Aussehen. Diese passivierende Oxidschicht macht reines Aluminium bei pH-Werten von 4 bis 9 sehr korrosionsbeständig, sie erreicht eine Dicke von etwa 0,05 µm.
Diese Oxidschicht schützt auch vor weiterer Oxidation, ist aber bei der elektrischen Kontaktierung und beim Löten hinderlich. Sie kann durch elektrische Oxidation (Eloxieren) oder auf chemischem Weg verstärkt werden.
Die Oxidschicht kann mittels Komplexbildungsreaktionen aufgelöst werden. Einen außerordentlich stabilen und wasserlöslichen Neutralkomplex geht Aluminium in neutraler chloridischer Lösung ein. Folgende Reaktionsgleichung veranschaulicht den Vorgang:
Dies geschieht vorzugsweise an Stellen, wo die Oxidschicht des Aluminiums bereits geschädigt ist. Es kommt dort durch Bildung von Löchern zur Lochfraßkorrosion. Kann die chloridische Lösung dann an die freie Metalloberfläche treten, so laufen andere Reaktionen ab. Aluminium-Atome können unter Komplexierung oxidiert werden:
Liegen in der Lösung Ionen edlerer Metalle vor, so werden sie reduziert und am Aluminium abgeschieden. Auf diesem Prinzip beruht die Reduktion von Silberionen, die auf der Oberfläche von angelaufenem Silber als Silbersulfid vorliegen, hin zu Silber.
Aluminium reagiert heftig mit wässriger Natriumhydroxidlösung (NaOH) unter Bildung von Wasserstoff. Diese Reaktion wird in chemischen Rohrreinigungsmitteln ausgenutzt.
Die Reaktion von Aluminium mit NaOH läuft in zwei Schritten ab: der Reaktion mit Wasser und die Komplexierung des Hydroxids zu Natriumaluminat.
Bei der Reaktion mit Wasser
entsteht zunächst Aluminiumhydroxid.
In der Regel wird anschließend die Oberfläche getrocknet, dabei wird das Hydroxid in das Oxid umgewandelt:
Dies passiert jedoch nicht bei der Reaktion von Aluminium in wässriger Natronlauge.
Nun folgt der 2. Schritt, die Komplexierung des Hydroxids zu Natriumaluminat:
Durch die Komplexierung wird das gallertartige Hydroxid wasserlöslich und kann von der Metalloberfläche abtransportiert werden. Dadurch ist die Aluminiumoberfläche nicht mehr vor dem weiteren Angriff des Wassers geschützt und Schritt 1 läuft wieder ab.
Mit dieser Methode lassen sich – ebenso wie bei der Reaktion von Aluminium mit Säuren – pro zwei Mol Aluminium drei Mol Wasserstoffgas herstellen.
Mit Brom reagiert Aluminium bei Zimmertemperatur unter Flammenerscheinung. Hierbei ist zu beachten, dass das entstehende Aluminiumbromid mit Wasser unter Bildung von Aluminiumhydroxid und Bromwasserstoffsäure reagiert.
Mit Quecksilber bildet Aluminium ein Amalgam. Wenn Quecksilber direkt mit Aluminium zusammenkommt, d. h., wenn die Aluminiumoxidschicht an dieser Stelle mechanisch zerstört wird, frisst Quecksilber Löcher in das Aluminium; unter Wasser wächst dann darüber Aluminiumoxid in Gestalt eines kleinen Blumenkohls. Daher wird Quecksilber in der Luftfahrt als Gefahrgut und „ätzende Flüssigkeit“ gegenüber Aluminiumwerkstoffen eingestuft.
Auch mit Salzsäure reagiert Aluminium sehr heftig unter Wasserstoffentwicklung, von Schwefelsäure wird es langsam aufgelöst. In Salpetersäure wird es passiviert.
In Pulverform (Partikelgröße kleiner 500 µm) ist Aluminium vor allem dann, wenn es nicht phlegmatisiert ist, aufgrund seiner großen Oberfläche sehr reaktiv. Aluminium reagiert dann mit Wasser unter Abgabe von Wasserstoff zu Aluminiumhydroxid. Feinstes, nicht phlegmatisiertes Aluminiumpulver wird auch als Pyroschliff bezeichnet. Nicht phlegmatisierter Aluminiumstaub ist sehr gefährlich und entzündet sich bei Luftkontakt explosionsartig von selbst.
Aluminium kann im schmelzflüssigen Zustand mit Kupfer, Magnesium, Mangan, Silicium, Eisen, Titan, Beryllium, Lithium, Chrom, Zink, Zirconium und Molybdän legiert werden, um bestimmte Eigenschaften zu fördern oder andere, ungewünschte Eigenschaften zu unterdrücken. Es geht mit einigen Elementen intermetallische Phasen ein, insbesondere NiAl, NiAl und TiAl.
Bei den meisten Legierungen ist jedoch die Bildung der schützenden Oxidschicht (Passivierung) stark gestört, wodurch die daraus gefertigten Bauteile teils hochgradig korrosionsgefährdet sind. Nahezu alle hochfesten Aluminiumlegierungen sind von dem Problem betroffen.
Es gibt Aluminiumknetlegierungen (AW, ), zum Beispiel AlMgMn, und Aluminiumgusslegierungen (AC, ). Aluminiumgusslegierungen werden zum Beispiel für Leichtmetallfelgen verwendet.
Im Allgemeinen werden Aluminiumlegierungen nach dem System der AA (Aluminum Association) bezeichnet:
Der Aluminiumpreis bewegte sich am Weltmarkt seit 1980 um den Wert von 2000 Dollar pro Tonne (Reinheit von 99,7 %).
Aluminium weist eine hohe spezifische Festigkeit auf. Verglichen mit Stahl sind Bauteile aus Aluminium bei gleicher Festigkeit etwa halb so schwer, weisen jedoch ein größeres Volumen auf. Deshalb wird es gern dort verwendet, wo es auf geringe Masse ankommt, die zum Beispiel bei Transportmitteln zum geringeren Treibstoffverbrauch beiträgt, vor allem in der Luft- und Raumfahrt. Auch im Fahrzeugbau gewann es aus diesem Grund an Bedeutung; hier standen früher der hohe Materialpreis, die schlechtere Schweißbarkeit sowie die problematische Dauerbruchfestigkeit und die Verformungseigenschaften bei Unfällen (geringes Energieaufnahmevermögen in der sogenannten Knautschzone) im Wege. Die Haube des Washington-Denkmals, ein 3 kg schweres Gussstück, galt bis 1884 als eines der größten Aluminiumwerkstücke. In den 1930er Jahren verwendete ein amerikanisches Unternehmen Aluminium, um ein militärisches Amphibienfahrzeug leichter zu machen. Beim Bau von kleinen und mittleren Yachten wird die Korrosionsbeständigkeit von speziell legiertem Aluminium gegenüber Salzwasser geschätzt, die sich nach Bildung einer dünnen, schützenden Oxidschicht an der Oberfläche ergibt. Der Fahrzeugbau machte 2010 mit ca. 35 Prozent den größten Anteil an der Verwendung von Aluminium aus.
In Legierungen mit Magnesium, Silicium und anderen Metallen werden Festigkeiten erreicht, die denen von Stahl nur wenig nachstehen. Daher ist die Verwendung von Aluminium zur Gewichtsreduzierung überall dort angebracht, wo Materialkosten eine untergeordnete Rolle spielen. Insbesondere im Flugzeugbau und in der Weltraumtechnik sind Aluminium und Duraluminium weit verbreitet. Der größte Teil der Struktur heutiger Verkehrsflugzeuge wird aus Aluminiumblechen verschiedener Stärken und Legierungen genietet. Bei den neusten Modellen (Boeing 787, Airbus A350) wird Aluminium größtenteils durch den noch leichteren kohlenstofffaserverstärkten Kunststoff (CFK) verdrängt. Ganz ähnliche Entwicklungen – zeitlich und hin zu leichteren Ausführungen – von Holz über Stahl zu Alu und weiter zu Kohlenstoff- oder Glasfaserverbund sind an Rahmen und Felgen des Fahrrads, sowie an Zeltstangen und Stöcken etwa zum Schifahren zu beobachten. In einigen Anwendungsfällen etablieren sich Aluminiumwerkstoffe als dominierend: Fahrradfelgen aus Strangprofil, Motorrad- und modisch gestaltete Autofelgen aus Aluguss, biegsames gefädeltes Gestänge von Kuppelzelten, teleskopierbare Stöcke zum Schitourengehen, beides aus Rohr, Kamerastativbeine jedoch daneben auch aus Profil.
Aluminium ist ein guter elektrischer Leiter. Es weist nach Silber, Kupfer und Gold die vierthöchste elektrische Leitfähigkeit aller Metalle auf. Ein Leiter aus Aluminium hat bei gegebenem elektrischen Widerstand eine kleinere Masse, aber ein größeres Volumen als ein Leiter aus Kupfer. Daher wird meistens dann Kupfer als elektrischer Leiter verwendet, wenn das Volumen eine dominante Rolle spielt, wie beispielsweise bei den Wicklungen in Transformatoren. Aluminium hat dann als elektrischer Leiter Vorteile, wenn das Gewicht eine wesentliche Rolle spielt, beispielsweise bei den Leiterseilen von Freileitungen. Aus dem Grund der Gewichtsreduktion werden auch in Flugzeugen wie dem Airbus A380 Aluminiumkabel verwendet.
Aluminium wird unter anderem auch zu Stromschienen in Umspannwerken und zu stromführenden Gussteilen verarbeitet. Für Elektroinstallationen gibt es kupferkaschierte Aluminiumkabel, der Kupferüberzug ist zur Verbesserung der Kontaktgabe. In diesem Anwendungsbereichen sind primär Rohstoffpreise entscheidend, da Aluminium preisgünstiger als Kupfer ist. Für Oberleitungen bei elektrischen Bahnen ist es dagegen aufgrund seiner schlechten Kontakt- und Gleiteigenschaften ungeeignet, in diesem Bereich wird trotz des höheren Gewichts primär Kupfer eingesetzt.
Beim Kontaktieren unter Druck ist Aluminium problematisch, da es zum Kriechen neigt. Außerdem überzieht es sich an Luft mit einer Oxidschicht. Nach längerer Lagerung oder Kontakt mit Wasser ist diese isolierende Schicht so dick, dass sie vor der Kontaktierung beseitigt werden muss. Vor allem im Kontakt mit Kupfer kommt es zu Bimetallkorrosion. Bei ungeeigneten Kontaktierungen in Klemmen kann es bei Aluminiumleitern in Folge zu Ausfällen und Kabelbränden aufgrund sich lösender Kontakte kommen. Crimpverbindungen mit passenden Hülsen und Werkzeugen sind jedoch sicher. Als Zwischenlage zwischen Kupfer und Aluminium können Verbindungsstücke aus Cupal die Kontaktprobleme vermeiden.
Hervorzuheben ist das geringe Absinken der spezifischen elektrischen Leitfähigkeit von Aluminium bei Zusatz von Legierungsbestandteilen, wohingegen Kupfer bei Verunreinigungen eine deutliche Verringerung der Leitfähigkeit zeigt.
Die Elektronikindustrie setzt Aluminium aufgrund der guten Verarbeitbarkeit und der guten elektrischen und Wärme-Leitfähigkeit ein.
In integrierten Schaltkreisen wurde bis in die 2000er Jahre ausschließlich Aluminium als Leiterbahnmaterial eingesetzt. Bis in die 1980er-Jahre wurde es auch als Material für die Steuerelektrode (Gate) von Feldeffekttransistoren mit Metall-Isolator-Halbleiter-Struktur (MOSFET bzw. MOS-FET) verwendet. Neben dem geringen spezifischen Widerstand sind für die Verwendung die gute Haftung auf und geringe Diffusion in Siliciumoxiden (Isolationsmaterial zwischen den Leiterbahnen) sowie die einfache Strukturierbarkeit mithilfe von Trockenätzen ausschlaggebend. Seit Anfang der 2000er-Jahre wird Aluminium jedoch zunehmend durch Kupfer als Leiterbahnmaterial ersetzt, auch wenn dafür aufwendigere Strukturierungsverfahren (vgl. Damascene- und Dual-Damascene-Prozess) und Diffusionsbarrieren notwendig sind. Der höheren Fertigungsaufwand wird durch den geringeren spezifischen Widerstand, der im Fall von kleinen Strukturen bei Aluminium viel früher signifikant ansteigt und anderen Eigenschaften (z. B. Elektromigrationverhalten) überwogen und die Aluminium-Prozesse konnte die gestiegenen Anforderungen (Taktfrequenz, Verlustleistung, etc.) in mit hohen Frequenzen arbeitenden Schaltkreisen nicht mehr genügen (siehe auch RC-Glied).
Aluminium wird jedoch weiterhin auch in mikroelektronischen Produkten verwendet, so wird wegen seiner guten Kontaktierbarkeit durch andere Metalle in den letzten Leiterbahnebenen eingesetzt, um den elektrischen Kontakt zu den bei der Flip-Chip-Montage eingesetzten Lotkügelchen herzustellen. Ähnlich verhält es sich bei Leistungshalbleitern, bei denen in der Regel alle Leiterbahnebenen aus Aluminium bestehen. Allgemein und insbesondere bei Leistungshalbleitern wird das Material für Bonddrähte (Verbindungsdrähte zwischen Chip und Gehäuseanschluss) verwendet.
Mit der Einführung der High-k+Metal-Gate-Technik hat Aluminium nach gut 25 Jahren Abstinenz auch im Bereich des Gates an Bedeutung gewonnen und wird neben anderen als Material zur Einstellung der Austrittsarbeit eingesetzt.
Wegen seiner hohen Wärmeleitfähigkeit wird Aluminium als Werkstoff für stranggepresste Kühlkörper und wärmeableitende Grundplatten verwendet. Aluminium-Elektrolytkondensatoren verbauen Aluminium als Elektrodenmaterial und Gehäusewerkstoff, weiters wird es zur Herstellung von Antennen und Hohlleitern verwendet.
In der Verpackungsindustrie wird Aluminium zu Getränke- und Konservendosen sowie zu Aluminiumfolie verarbeitet. Dabei macht man sich die Eigenschaft der absoluten Barrierewirkung gegenüber Sauerstoff, Licht und anderen Umwelteinflüssen bei gleichzeitig relativ geringem Gewicht zunutze, um Lebensmittel zu schützen. Dünne Folien werden in Stärken von 6 Mikrometer hergestellt und dann zumeist in Verbundsystemen eingesetzt, beispielsweise in Tetra Paks. Kunststofffolien können durch Bedampfen mit Aluminium mit einer dünnen Schicht versehen werden, welche dann eine hohe (aber nicht vollständige) Barrierefunktion aufweist. Grund dieser Barrierewirkung ist nicht das reine Aluminium, sondern die Passivschicht aus Böhmit. Wird diese verletzt, so kann Gas ungehindert durch den Werkstoff Aluminium strömen.
Aus Aluminium werden auch Kochtöpfe und andere Küchengeräte, wie die klassische italienische Espressokanne, sowie Reise- und Militär-Geschirr hergestellt.
Die Aufbewahrung und Zubereitung von säurehaltigen Lebensmitteln in Aluminiumbehältern beziehungsweise -folie ist problematisch, da es dabei lösliche Aluminiumsalze bildet, die mit der Nahrung aufgenommen werden. Aluminiumschichten in Verpackungsmitteln werden daher häufig durch eine Kunststoffschicht geschützt.
Aluminium wird für eine Vielzahl von Behältern und Gehäusen verarbeitet, da es sich gut durch Umformen bearbeiten lässt. Gegenstände aus Aluminium werden häufig durch eine Eloxalschicht vor Oxidation und Abrieb geschützt.
Aluminium wird aufgrund seines hohen Reflexionsgrades als Spiegelbeschichtung von Oberflächenspiegeln, unter anderem in Scannern, Kraftfahrzeug-Scheinwerfern und Spiegelreflexkameras aber auch in der Infrarotmesstechnik eingesetzt. Es reflektiert im Gegensatz zu Silber auch Ultraviolettstrahlung.
Aluminium-Spiegelschichten werden meist durch eine Schutzschicht vor Korrosion und Kratzern geschützt.
Aluminiumpulver und Aluminiumpasten werden zur Herstellung von Porenbeton eingesetzt. Man verwendet Verbindungen wie Aluminiumhydroxysulfat, Aluminiumdihydroxyformiat oder amorphes Aluminiumhydroxid als alkalifreie Spritzbetonbeschleuniger.
In der Raketentechnik besteht der Treibstoff von Feststoffraketen zu maximal 30 Prozent aus Aluminiumpulver, das bei seiner Verbrennung viel Energie freisetzt. Aluminium wird in Feuerwerken (s. a. Pyrotechnik) verwendet, wo es je nach Körnung und Mischung für farbige Effekte sorgt. Auch in Knallsätzen findet es oft Verwendung.
Bei der Aluminothermie wird Aluminium zur Gewinnung anderer Metalle und Halbmetalle verwendet, indem das Aluminium zur Reduktion der Oxide genutzt wird. Ein wichtiges Verfahren der Aluminothermie ist die Thermitreaktion, bei der Aluminium mit Eisen(III)-oxid umgesetzt wird. Bei dieser stark exothermen Reaktion entstehen Temperaturen bis zu 2500 °C und flüssiges Eisen, das zum aluminothermischen Schweißen genutzt wird, z. B. zum Fügen von Bahngleisen. Weitere Anwendungen der Reduktionswirkung von Aluminium werden für Laborzwecke ermöglicht, indem Aluminiumamalgam verwendet wird.
Aluminium dient als Pigment für Farben (Silber- oder Goldbronze). Farbig eloxiert ist es Bestandteil vieler Dekorationsmaterialien wie Flitter, Geschenkbänder und Lametta. Zur Beschichtung von Oberflächen wird es beim Aluminieren verwendet.
Mit Aluminium werden Heizelemente von Bügeleisen und Kaffeemaschinen umpresst.
Bevor es gelang, Zinkblech durch Titanzusatz als so genanntes Titanzink verarbeitbar zu machen, wurde Aluminiumblech für Fassaden- und Dachelemente (siehe Leichtdach) sowie Dachrinnen eingesetzt.
Aluminium gehört aufgrund seines vergleichsweise geringen Schmelzpunktes von 660 °C (Gusseisen etwa 1150 °C, Stahl 1400 °C bis 1500 °C) und seiner guten Gießbarkeit zu den häufig in der Gießerei verwendeten Werkstoffen. Spezielle Gusslegierungen mit Silicium haben sogar Schmelzpunkte um 577 °C. Aus dem niedrigen Schmelzpunkt folgt ein geringer Energieeinsatz beim Schmelzvorgang sowie eine geringere Temperaturbelastung der Formen.
Aluminium eignet sich grundsätzlich für alle Gussverfahren, insbesondere für Druckguss bzw. Aluminiumdruckguss, mit denen auch kompliziert geformte Teile gefertigt werden können.
Etwa 74 Prozent des Aluminiums wird durch Umformen bearbeitet. Aluminium lässt sich durch Strangpressen in komplizierte Konstruktionsprofile formen; hierin liegt ein großer Vorteil bei der Fertigung von Hohlprofilen (Automatisierungstechnik, Messebau), Kühlkörperprofilen oder in der Antennentechnik. Die Herstellung von Halbzeug oder Bauteilen geschieht aus Vormaterial wie etwa Walzbarren, Blech oder Zylindern durch Umformen. Mittels Walzen lassen sich sehr dünne Folien herstellen und mit dem Tiefziehen Hohlkörper wie etwa Getränkedosen. Ein Mischverfahren aus Gießen und Schmieden ist Cobapress, welches speziell für Aluminium ausgelegt ist und häufig in der Automobilbranche genutzt wird.
Rein- und Reinstaluminium lässt sich wegen der niedrigen Festigkeit gut umformen und verfestigt sich bei Kaltumformung, wobei große Formänderungen möglich sind. Die Verfestigung lässt sich durch Rekristallisationsglühen beseitigen. Knetlegierungen mit AlMg und AlMn erreichen ihre höhere Festigkeit durch die Legierungselemente und durch Kaltverformung. Die aushärtbaren Legierungen AlMgSi, AlZnMg, AlCuMg und AlZnMgCu scheiden bei Umformung festigkeitssteigernde Phasen aus; sie lassen sich relativ schwierig umformen.
Aluminiumwerkstoffe sind gut spanbar. Ihre genauen Eigenschaften hängen jedoch von der Legierung und Gefügezustand ab. Zu beachten ist, dass die bei der Bearbeitung auftretenden Temperaturen schnell im Bereich des Schmelzpunktes liegen können. Bei gleichen Schnittparametern wie bei Stahl resultiert bei Aluminium allerdings eine geringere mechanische und thermische Belastung. Als Schneidstoff wird oft Hartmetall für untereutektische oder Diamant für die stark abrasiven übereutektischen Legierungen verwendet.
Insbesondere die Bearbeitung von eloxierten Werkstücken erfordert harte Werkzeuge, um Verschleiß durch die harte Eloxalschicht zu vermeiden. Die beim Schleifen von Aluminium entstehenden Schleifstäube können zu einem erhöhten Explosionsrisiko führen.
Grundsätzlich sind alle Aluminium-Werkstoffe zum Schweißen geeignet, wobei jedoch reines Aluminium zu Poren in der Schweißnaht neigt. Außerdem neigt die Aluminiumschmelze zu Reaktionen mit der Atmosphäre, weshalb fast immer unter Schutzgas geschweißt wird. Gut geeignet sind das MIG- und Plasmaschweißen sowie das WIG-Schweißen. Bei letzterem wird bei Nutzung von Wechselstrom Argon als Schutzgas verwendet und bei Gleichstrom Helium. Für das Laserschweißen eignen sich sowohl Kohlendioxid- als auch Festkörperlaser allerdings nicht für alle Legierungen. Wegen der hohen Wärmeleitfähigkeit erstarrt die Schmelze sehr schnell, sodass die Schweißnaht zu Poren und Rissen neigt. Das Widerstandspunktschweißen erfordert verglichen mit Stahl, höhere Ströme und kürzere Schweißzeiten sowie teilweise spezielle Geräte, da die handelsüblichen nicht geeignet sind. Für das Elektronenstrahlschweißen eignen sich alle Legierungen, jedoch neigen Magnesium und Zinn zum Verdampfen während des Schweißvorgangs. Lichtbogenhandschweißen wird nur noch selten verwendet, meist zur Gussnachbesserung. Löten gestaltet sich wegen der sich bildenden Oxidschicht an Luft schwierig. Genutzt werden sowohl Hart- als auch Weichlöten mit speziellen Flussmittel. Alternativ kann Aluminium auch ohne Flussmittel mit Ultraschall gelötet werden, dabei wird die Oxidschicht mechanisch während des Lötvorganges aufgebrochen.
Aluminium ist kein essenzielles Spurenelement und gilt für die menschliche Ernährung als entbehrlich. Im menschlichen Körper befinden sich durchschnittlich etwa 50 bis 150 Milligramm Aluminium. Diese verteilen sich zu ungefähr 50 Prozent auf das Lungengewebe, zu 25 Prozent auf die Weichteile und zu weiteren 25 Prozent auf die Knochen. Aluminium ist damit ein natürlicher Bestandteil des menschlichen Körpers.
99 bis 99,9 Prozent der üblicherweise in Lebensmitteln aufgenommenen Menge von Aluminium (10 bis 40 mg pro Tag) werden unresorbiert über den Kot wieder ausgeschieden. Chelatbildner "(Komplexbildner)" wie Citronensäure können die Resorption auf 2 bis 3 Prozent steigern. Auch die Aufnahme von Aluminiumsalzen über den Magen-Darm-Trakt ist gering; sie variiert aber in Abhängigkeit von der chemischen Verbindung und ihrer Löslichkeit, dem pH-Wert und der Anwesenheit von Komplexbildnern. Die Eliminierung von in den Organismus gelangten wasserlöslichen Aluminiumsalzen erfolgt vorwiegend über den Urin, weniger über den Kot. Bei Dialysepatienten mit einer eingeschränkten Nierenfunktion besteht daher ein erhöhtes Risiko einer Akkumulation im Körper mit toxischen Effekten, etwa Knochenerweichungen und Schäden des Zentralnervensystems; zusätzlich sind Dialysepatienten aufgrund für sie notwendiger pharmazeutischer Produkte (Phosphatbinder) einer höheren Aluminiumzufuhr ausgesetzt.
Im Blut ist Al überwiegend (zu etwa 80 %) an Transferrin gebunden. 16 Prozent liegen als [Al(PO)(OH)], 1,9 Prozent als Citrat-Komplex, 0,8 Prozent als Al(OH) und 0,6 Prozent als [Al(OH)] vor.
Aluminium in Form verschiedener Salze (Phosphate, Silikate) ist Bestandteil vieler Pflanzen und Früchte, denn gelöste Al-Verbindungen werden durch Regen aus den Böden von den Pflanzen aufgenommen, bei Säurebelastung der Böden infolge sauren Regens ist dies vermehrt der Fall (siehe dazu auch Waldschäden).
Ein großer Teil des Bodens auf der Welt ist chemisch sauer. Liegt der pH-Wert unter 5,0, werden Al-Ionen von den Wurzeln der Pflanzen aufgenommen. Dies ist bei der Hälfte des bebaubaren Lands auf der Welt der Fall. Die Ionen schädigen insbesondere das Wurzelwachstum der Feinwurzeln. Die Pflanze, wenn sie nicht Aluminium-tolerant ist, steht dann unter Stress. Zahlreiche Enzyme und signalübertragende Proteine sind betroffen; die Folgen der Vergiftung sind noch nicht vollständig bekannt. In sauren metallhaltigen Böden ist Al das Ion mit dem größten Potenzial zur Schädigung. Von der Modellpflanze "Arabidopsis" sind Transgene bekannt, die deren Aluminium-Toleranz heraufsetzen und auch bei Kulturpflanzen sind tolerante Sorten bekannt.
Der saure Regen hat beispielsweise in Schweden in den Sechzigerjahren des letzten Jahrhunderts die Seen übersäuert, wodurch mehr Al-Ionen in Lösung gingen und empfindliche Fische verendeten.
Bei pH-Werten über 5,0 ist Aluminium als polymeres Hydroxykation an der Oberfläche von Silicaten gebunden. Bei pH-Werten von 4,2 bis 5 steigt Anteil von mobilen Kationen.
Bei Erhöhung der Schwefelsäurekonzentration durch sauren Regen bildet sich Aluminiumhydroxysulfat:
Die meisten Lebensmittel enthalten in Spurenmengen auch Aluminium. Unverarbeitete pflanzliche Lebensmittel enthalten durchschnittlich weniger als 5 mg/kg in der Frischmasse. Dabei streuen die Werte aufgrund unterschiedlicher Sorten, Anbaubedingungen und Herkunft in erheblichem Maße. So weisen beispielsweise Salat und Kakao deutlich höhere Durchschnittswerte auf. Schwarzer Tee kann Gehalte von bis zu 1042 mg/kg in der Trockenmasse aufweisen.
Beim Kochen oder Aufbewahren in Aluminiumgeschirr oder in Alufolie kann es (außer bei sauren Lebensmitteln) nach einer Schätzung zu einer maximalen zusätzlichen Aufnahme von 3,5 mg/Tag/Person kommen. Bei sauren Lebensmitteln wie Sauerkraut oder auch Tomaten können aufgrund der Säurelöslichkeit wesentlich höhere Werte erreicht werden.
Trink- und Mineralwässer weisen mit durchschnittlich 0,2–0,4 mg/l im Gegensatz zur Nahrung geringe Gehalte auf und stellen somit nur einen kleinen Beitrag zur täglichen Aluminium-Aufnahme. Die Trinkwasserverordnung legt einen Grenzwert von 0,2 mg/l fest. Trinkwasser darf in Deutschland, Österreich und der Schweiz keine höheren Werte aufweisen.
Nach einer Schätzung nimmt der erwachsene Europäer im Durchschnitt zwischen 1,6 und 13 mg Aluminium pro Tag über die Nahrung auf. Dies entspricht einer wöchentlichen Aufnahme von 0,2 bis 1,5 mg Aluminium pro kg Körpergewicht bei einem 60 kg schweren Erwachsenen. Die großen Unsicherheiten beruhen auf den unterschiedlichen Ernährungsgewohnheiten und der variablen Gehalte an Aluminium in den Lebensmitteln.
Die Europäische Behörde für Lebensmittelsicherheit (Efsa) legt eine tolerierbare wöchentliche Aufnahme (TWI) von 1 mg Aluminium pro kg Körpergewicht fest.
Aluminium ist als Lebensmittelzusatzstoff unter der Bezeichnung E 173 ausschließlich als Farbstoff für Überzüge von Zuckerwaren und als Dekoration von Kuchen und Keksen erlaubt. Weiterhin ist Aluminium zum Färben von Arzneimitteln und Kosmetika zugelassen.
Bei der Untersuchung von Laugengebäck (Brezeln, Stangen, Brötchen) aus Bäckereien wurde Aluminium nachgewiesen, das in das Lebensmittel gelangt, wenn bei der Herstellung von Laugengebäck Aluminiumbleche verwendet werden.
Während Bier in Aluminiumfässern transportiert wird, hat sich für den Weintransport der Werkstoff Aluminium nicht durchgesetzt. Ein kurzfristiger Kontakt schadet nicht, doch können nach längerem Kontakt Weinfehler in Geruch und Geschmack oder als Trübung auftreten, vor allem beim offenen Stehen an der Luft.
Bei eingeschränkter Nierenfunktion und bei Dialyse-Patienten führt die Aufnahme von Aluminium zu progressiver Enzephalopathie (Gedächtnis- und Sprachstörungen, Antriebslosigkeit und Aggressivität) durch Untergang von Hirnzellen und zu fortschreitender Demenz, zu Osteoporose (Arthritis) mit Knochenbrüchen und zu Anämie (weil Aluminium dieselben Speichereiweiße wie Eisen besetzt).
Speziell im Hinblick auf die Verwendung in Deodorants und Lebensmittel-Zusatzstoffen werden die gesundheitlichen Auswirkungen von Aluminium kontrovers diskutiert. So wurde Aluminium mehrfach kontrovers als Faktor im Zusammenhang mit der Alzheimer-Krankheit in Verbindung gebracht.
Laut einer Studie des Bundesinstituts für Risikobewertung (BfR) vom Juli 2007 wurde im allgemeinen Fall zum Zeitpunkt der Erstellung der Studie aufgrund der vergleichsweise geringen Menge kein Alzheimer-Risiko durch Aluminium aus Bedarfsgegenständen erkannt; jedoch sollten vorsorglich keine sauren Speisen in Kontakt mit Aluminiumtöpfen oder -folie aufbewahrt werden. Eine Neubewertung erfuhr im Februar 2014 die Verwendung von aluminiumhaltigen Deodorants und Kosmetikartikel durch das Bundesinstitut für Risikobewertung: Aluminiumsalze aus solchen Produkten können durch die Haut aufgenommen werden, und die regelmäßige Benutzung über Jahrzehnte hinweg könnte möglicherweise zu gesundheitlichen Beeinträchtigungen beitragen.
Die britische Alzheimer-Gesellschaft mit Sitz in London vertritt den Standpunkt, dass die bis 2008 erstellten Studien einen kausalen Zusammenhang zwischen Aluminium und der Alzheimer-Krankheit nicht überzeugend nachgewiesen haben. Dennoch gibt es einige Studien, wie z. B. die PAQUID-Kohortenstudie in Frankreich, mit einer Gesundheitsdatenauswertung von 3777 Personen im Alter ab 65 Jahren seit 1988 bis zur Gegenwart, in welchen eine Aluminium-Exposition als Risikofaktor für die Alzheimer-Krankheit angegeben wird. Demnach wurden viele senile Plaques mit erhöhten Aluminium-Werten in Gehirnen von Alzheimer-Patienten gefunden. Es ist jedoch unklar, ob die Aluminium-Akkumulation eine Folge der Alzheimer-Krankheit ist, oder ob Aluminium in ursächlichem Zusammenhang mit der Alzheimer-Krankheit zu sehen ist.
Die Herstellung von Aluminium ist sehr energieaufwendig. Allein für die Schmelzflusselektrolyse zur Gewinnung eines Kilogramms Aluminium werden je nach Errichtungsdatum und Modernität der Anlage zwischen 12,9 und 17,7 kWh elektrische Energie benötigt. Bei der Stromerzeugung für die Produktion von einem Kilogramm Aluminium werden im deutschen Kraftwerkspark 8,4 kg CO freigesetzt, im weltweiten Durchschnitt etwa 10 kg. Es ist aber auch zu bedenken, dass aufgrund des Kostenfaktors Energie die Elektrolyse verstärkt an Orten erfolgt, an denen auf billige, CO-emissionsarme Wasserkraft zurückgegriffen werden kann, wie etwa in Brasilien, Kanada, Venezuela oder Island. Allerdings ist auch bei Verwendung von Elektrizität aus vollständig regenerativen Energien die Produktion von Aluminium nicht CO-frei, da der bei der Schmelzflusselektrolyse entstehende Sauerstoff mit dem Kohlenstoff der Elektroden zu CO reagiert. Die Verbrauchswerte für Roh-Aluminium erhöhen sich durch Transport- und Verarbeitungsanteile für das Wiederaufschmelzen, Gießen, Schleifen, Bohren, Polieren etc. auf ca. 16,5 kg CO pro kg Aluminium-Konsumgut.
Die europaweite Recyclingrate von Aluminium liegt bei 67 Prozent. In Österreich gelangen (laut einer Studie aus dem Jahr 2000) 16.000 Tonnen Aluminium pro Jahr "über Verpackungen" in den Konsum, ebenso gelangen 16.000 Tonnen Aluminium ohne Wiederverwertung in den Hausmüll (dabei sind unter anderem auch die Aluminiumhaushaltsfolien eingerechnet, die nicht als „Verpackung“ gelten). 66 Prozent der Verpackungen im Restmüll sind Aluminium[getränke]dosen. Diese liegen nach der Müllverbrennung in der Asche noch metallisch vor und machen in Europa durchschnittlich 2,3 Prozent der Asche aus. In der EU werden durchschnittlich 70 Prozent des in der Bodenasche enthaltenen Aluminiums zurückgewonnen.
Durch den Abbau des Erzes Bauxit werden große Flächen in Anspruch genommen, die erst nach einer Rekultivierung wieder nutzbar werden. Um eine Tonne Aluminium herzustellen, werden vier Tonnen Bauxit benötigt. Dies erzeugt zehn Tonnen Abraum. Zudem entstehen bei der Herstellung des Aluminiumoxids nach dem Bayer-Verfahren ca. drei Tonnen von eisenreichem alkalischen Rotschlamm, der kaum wiederverwertet wird und dessen Deponierung oder sonstige „Entsorgung“ große Umweltprobleme aufwirft (siehe dort und dort).
Positiv ist hingegen die gute Wiederverwendbarkeit von Aluminium hervorzuheben, wobei die Reststoffe streng getrennt erfasst und gereinigt werden müssen (Aluminiumrecycling, Recycling-Code-41 (ALU)). Aluminium ist dabei besser rezyklierbar als Kunststoffe, wegen Downcycling bei nicht sortenreiner Erfassung jedoch etwas schlechter wiederverwertbar als Stahl. Beim Aluminiumrecycling wird nur 5 Prozent der Energiemenge der Primärproduktion benötigt.
Durch Leichtbau mit Aluminiumwerkstoffen (beispielsweise Aluminiumschaum, Strangpressprofile) wird Masse von beweglichen Teilen und Fahrzeugen gespart, was zur Einsparung von Treibstoff führen kann.
Aluminium ist durch seine Selbstpassivierung korrosionsbeständiger als Eisen und erfordert daher weniger Korrosionsschutzmaßnahmen.
Aluminiumsalze weist man durch Glühen mit verdünnter Kobaltnitratlösung auf der Magnesia-Rinne nach. Dabei entsteht das Pigment Thénards Blau (auch Kobaltblau oder Cobaltblau, Dumonts Blau, Coelestinblau, Leithners Blau, Cobaltaluminat). Es ist ein Cobaltaluminiumspinell mit der Formel CoAlO.
Diese Nachweisreaktion wurde 1795 von Leithner durch Glühen von Aluminiumsulfat und Cobalt(II)-nitrat (Co(NO)) entdeckt.
Die Probelösung wird alkalisch gemacht, um Aluminium als Aluminiumhydroxid Al(OH) zu fällen. Der Niederschlag wird abfiltriert und mit einigen Tropfen Phenolphthalein versetzt, dann gewaschen, bis keine Rotfärbung durch Phenolphthalein mehr vorhanden ist. Anschließend festes Natriumfluorid (NaF) auf den Niederschlag streuen: Es bildet sich eine Rotfärbung durch Phenolphthalein, verursacht von freigesetzten Hydroxidionen bei der Bildung von Kryolith Na[AlF].
Die Probe wird mit Salzsäure (HCl) versetzt und eventuell vorhandenes Aluminium somit gelöst. Anschließend wird die Probelösung mit Kaliumhydroxid (KOH) stark alkalisch gemacht. Gibt man nun einige Tropfen der Probelösung zusammen mit der gleichen Menge Morin-Lösung auf eine Tüpfelplatte und säuert anschließend mit konzentrierter Essigsäure ("Eisessig", CHCOOH) an, so ist unter UV-Strahlung (λ = 366 nm) eine grüne Fluoreszenz beobachtbar. Der Nachweis ist dann sicher, wenn diese Fluoreszenz bei Zugabe von Salzsäure wieder verschwindet.
Grund hierfür ist, dass Al(III) in neutralen sowie essigsauren Lösungen in Verbindung mit Morin eine fluoreszierende kolloidale Suspension bildet.
Unter besonderen Bedingungen tritt Aluminium auch einwertig auf. Diese Verbindungen werden zur Gewinnung von hochreinem Aluminium genutzt (Subhalogeniddestillation).

</doc>
<doc id="95" url="https://de.wikipedia.org/wiki?curid=95" title="Antimon">
Antimon

Antimon [] (von lateinisch "Antimonium", vermutlich von arabisch „al-ithmîd(un)“ (, Antimonsulfid bzw. Stibnit)) ist ein chemisches Element mit dem Elementsymbol Sb (von lat. Stibium „(Grau-)Spießglanz“) und der Ordnungszahl 51. Im Periodensystem steht es in der 5. Periode und der 5. Hauptgruppe, bzw. 15. IUPAC-Gruppe oder Stickstoffgruppe. In der stabilen Modifikation ist es ein silberglänzendes und sprödes Halbmetall.
Es wird auch vermutet, dass der Name auf das spätgriechische "anthemon" (dt. "Blüte") zurückgeht. Damit sollten die stängelartigen Kristalle von Antimonsulfid beschrieben werden, die büschelförmig erschienen und wie eine Blüte aussähen. Im 11. Jahrhundert findet sich der lateinische Begriff "antimonium" dann bei Constantinus Africanus.
Im 17. Jahrhundert ging der Name Antimon als Bezeichnung auf das Metall über. Die koptische Bezeichnung für das Schminkpuder Antimonsulfid ging über das Griechische in das Lateinische "stibium" über. Die vom schwedischen Mediziner und Chemiker Jöns Jakob Berzelius („Vater der modernen Chemie“) benutzte Abkürzung Sb wird noch heute als Elementsymbol genutzt.
Eine späte legendäre Volksetymologie, die von Samuel Johnson in seinem Wörterbuch verewigt wurde, besagt, dass der deutsche Mönch Basilius Valentinus die Beobachtung machte, dass Schweine durch die Aufnahme von Antimon schnell fett wurden. Er probierte dies auch an seinen Ordensbrüdern aus, woraufhin diese allerdings starben, sodass der Begriff „antimoine“ ("antimönchisch") geprägt wurde, aus dem später „Antimon“ entstanden sei.
Als Typlokalität für gediegenes Antimon gilt die Silbermine in der schwedischen Gemeinde Sala im Västmanland. Allerdings war metallisches Antimon schon den Chinesen und Babyloniern bekannt. Einige seiner Verbindungen wurden schon in der Bronzezeit als Zuschlag zu Kupfer verwendet, um Bronze herzustellen (Funde von Velem-St. Vid in Ungarn).
Antimon ist ein selten vorkommendes Element. Da es in der Natur auch gediegen (das heißt in elementarer Form) gefunden werden kann, wird es von der International Mineralogical Association (IMA) unter der System-Nr. 1.CA.05 als Mineral anerkannt.
Weltweit konnte gediegenes Antimon bisher (Stand: 2011) an rund 300 Fundorten nachgewiesen werden. So unter anderem in mehreren Regionen von Australien; in den bolivianischen Departements La Paz und Potosí; Minas Gerais in Brasilien; Schwarzwald, Fichtelgebirge, Oberpfälzer Wald, Odenwald und im Harz in Deutschland; Seinäjoki in Finnland; mehreren Regionen von Frankreich; Lombardei, Piemont, Sardinien und Trentino-Südtirol in Italien; einigen Regionen von Kanada; einigen Regionen von Österreich; Ost- und Westsibirien und Ural in Russland; neben Västmanland noch Dalarna, Gästrikland, Närke, Södermanland, Värmland und Västerbotten in Schweden; in einigen Regionen der Slowakei; Böhmen und Mähren in Tschechien sowie in vielen Regionen der USA. Eine der weltweit bedeutendsten Lagerstätte für gediegen Antimon und Antimonerze ist der "Murchison greenstone belt" in der Murchison Range von Südafrika.
Bisher sind 264 Antimon-Minerale bekannt (Stand: 2010). Industriell genutzt wird überwiegend das Sulfid-Mineral Stibnit SbS (Grauspießglanz) mit einem Gehalt von max. 71,7 % Sb. Das Mineral mit dem höchsten Sb-Gehalt in einer chemischen Verbindung ist die natürliche Antimon-Arsen-Legierung Paradocrasit (max. 92 %). Allerdings kommt sie mit nur drei Fundorten, im Gegensatz zum Stibnit (rund 2500 Fundorte), sehr viel seltener vor. Weitere Quellen für Antimon sind die Minerale Valentinit SbO (Weißspießglanz), Breithauptit NiSb (Antimonnickel, Nickelantimonid), Kermesit SbSO (Rotspießglanz) und SbS (Goldschwefel).
Technisch wird Antimon aus dem Antimonglanz gewonnen. Ein Verfahren beruht auf dem Abrösten und der Reduktion mit Kohlenstoff (Röstreduktionsverfahren):
Eine andere Möglichkeit besteht darin, die Reduktion mit Eisen durchzuführen (Niederschlagsverfahren):
Weltweit wurden zu Beginn des 21. Jahrhunderts zwischen 110.000 und 160.000 Tonnen pro Jahr an Antimon gefördert. Seit 1900 hat sich damit die Fördermenge mehr als verzehnfacht.
87 % der Antimonproduktion findet in China statt (Stand: 2015).
Antimon kann in drei verschiedenen Modifikationen auftreten, wobei metallisches bzw. "graues" Antimon die beständigste Modifikation ist.
Durch Abschrecken von Antimondampf an kalten Flächen entsteht amorphes, "schwarzes" und sehr reaktives Antimon, welches sich durch Erhitzen wieder in metallisches Antimon umwandelt. Durch elektrolytische Herstellung entsteht "explosives" Antimon, das beim Ritzen explosionsartig aufglühend und funkensprühend in metallisches Antimon übergeht. Diese Form enthält jedoch immer etwas Chlor und kann nicht als Modifikation betrachtet werden. "Gelbes" Antimon ist ebenfalls keine eigenständige Modifikation, sondern eine hochpolymere chemische Verbindung mit Wasserstoff.
Unter Normalbedingungen kristallisiert Antimon trigonal in rhomboedrischer Aufstellung in der nach der Hermann-Mauguin-Symbolik beschriebenen Raumgruppe  mit den Gitterparametern "a" = 431 pm und "c" = 1127 pm sowie sechs Formeleinheiten pro Elementarzelle.
Metallisches Antimon ist silberweiß, stark glänzend, blättrig-grobkristallin. Es lässt sich aufgrund seiner Sprödigkeit leicht zerkleinern. Elektrische und thermische Leitfähigkeit sind gering.
Mit naszierendem Wasserstoff reagiert Antimon zum instabilen Antimonhydrid SbH. Von Luft und Wasser wird Antimon bei Raumtemperatur nicht angegriffen. Oberhalb des Schmelzpunkts verbrennt es in Luft mit bläulich-weißer Flamme zu Antimon(III)-oxid. In heißen konzentrierten Mineralsäuren löst es sich auf. Mit den Halogenen reagiert es schon bei Raumtemperatur heftig zu den entsprechenden Halogeniden.
In Verbindungen liegt Antimon überwiegend in den Oxidationsstufen +3 und +5 vor. In Metallantimoniden wie Kaliumantimonid KSb bildet es Sb-Ionen.
Es existieren zwei stabile Antimon-Isotope: Sb und Sb.
Der überwiegende Teil des hergestellten Antimons wird zu Legierungen verarbeitet und zeigt dabei folgende Eigenschaften:
Wichtige Legierungen:
Brechweinstein wurde lange als brechreizerregendes Mittel verwendet (Antimonpille), heute wird es noch manchmal verwendet, um den Mageninhalt von Vögeln zu untersuchen.
Sowohl Schistosomiasis als auch Trypanosomen wurden beginnend Anfang des 19. Jahrhunderts mit Brechweinstein (Kaliumantimonyltartrat) bekämpft. Brechweinstein wurde hergestellt, indem man für einen Tag Wein in einem Antimonbecher lagerte, und diesen dann austrank. Inzwischen kommen effektivere und verträglichere Medikamente zur Anwendung.
Antimonpräparate werden meist als weniger toxische pentavalente Formen zur medikamentösen Therapie der Leishmaniose und Schistosomiasis eingesetzt, allerdings in entwickelten Ländern nicht mehr als Mittel der ersten Wahl. Hierbei hemmt Antimon das Enzym Phosphofructokinase, das den geschwindigkeitsbestimmenden Schritt der Glykolyse darstellt.
Antimon kann bereits bei Ingestion von 200 bis 1200 mg tödlich sein. In der Toxikologie sind drei Antimon-Formen bekannt, von denen das gasförmige Antimonhydrid (Stiban, SbH) die gefährlichste Form ist, die eine massive Hämolyse induziert. Nach der Toxizität folgt Brechweinstein mit dreiwertigem („trivalentem“) Antimon, während fünfwertiges Antimon am wenigsten toxisch ist.
Das trivalente Antimon wird innerhalb der ersten zwei Stunden nach der Einnahme zu 95 % in rote Blutkörperchen aufgenommen und damit vorwiegend in stark durchbluteten Organen angereichert. Die Exkretion erfolgt vorwiegend durch Bindung an Glutathion über die Galle mit entsprechend hohem enterohepatischen Kreislauf, und nur ein geringer Teil wird über die Nieren ausgeschieden. Kaliumantimonyltartrat wird zu 90 % innerhalb des ersten Tages nach Aufnahme ausgeschieden, die übrigen 10 % aufgrund einer langsameren Eliminationskinetik über 16 Tage.
Es wird vermutet, dass Antimon ähnlich wie Arsen die Funktion des Pyruvatdehydrogenase-Komplex hemmt und somit zu einem Mangel des intrazellulären Energieträgers Adenosintriphosphat (ATP) führt. Dabei kommt es zur Bildung von Chelatkomplexen zwischen dem Antimon und Thiol-Gruppen der entsprechenden Enzyme. Im Körper wirkt es in zahlreichen Organen toxisch, so im Verdauungstrakt, in der Leber, in den Nieren, im Herz und im Zentralnervensystem. Die höchste Konzentration erreicht Antimon in der Leber, wo es zu einer Hepatitis bis hin zum Leberversagen kommen kann. Am Herzen kommt es zu EKG-Veränderungen mit Inversion und Verminderung der T-Welle und verlängertem QT-Intervall. Ein akutes Nierenversagen kann zur temporären oder permanenten Hämodialyse führen.
Therapeutisch erfolgt bei einer Antimon-Vergiftung neben unterstützenden Maßnahmen wie Infusionstherapie (sowohl zum Ausgleich des Flüssigkeitsverlustes durch das Erbrechen als auch zum Schutz der Nieren), und engmaschiger Überwachung der Vitalfunktionen und des EKGs die Gabe von Aktivkohle, N-Acetylcystein als Vorläufer des Glutathions zur vermehrten Sekretion und eines Chelatbildners, z. B. Dimercaprol.
Von den Antimonverbindungen sind seitens der EU Antimonfluorid als giftig (T) und die Chloride als ätzend (C) eingestuft, außerdem als umweltgefährlich (N); alle anderen Antimonverbindungen als gesundheitsschädlich (Xn) und umweltgefährlich (N). Antimon selbst ist dort nicht aufgeführt, laut Sicherheitsdatenblatt ist es als "reizend" gekennzeichnet.
Die Internationale Agentur für Krebsforschung (IARC) stuft Antimon(III)-oxid als "möglicherweise krebserzeugende Substanz" ein.
Ergebnisse aus Untersuchungen deuten darauf hin, dass Antimonverbindungen Haut und Schleimhäute reizen. Diese Verbindungen lösen sich vermutlich aus Kunststoff und Textilien.
In der EU gilt für Trinkwasser ein Grenzwert von 5 µg/l. Untersuchungen von in PET-Flaschen abgefüllten Fruchtsäften (für die keine Richtlinien existieren) ergaben Antimonkonzentrationen bis zu 44,7 µg/l in unverdünnten Saftkonzentraten.
Vorproben:
Flammenfärbung: Flamme fahlblau, wenig charakteristische
Phosphorsalzperle: Farblos (gestört durch alle Elemente, die eine farbige Perle erzeugen)
Nachweisreaktion:
Reduktion durch unedle Metalle, zum Beispiel Eisen, Zink oder Zinn.
In nicht zu sauren Lösungen reduzieren unedle Metalle Antimon-Kationen Sb(III), Sb(V) und Sb(III)/(V) zu metallischem Antimon:
Die auf Antimon zu prüfende Substanz wird in salzsaure Lösung gegeben und mit Eisenpulver versetzt. Es entsteht ein schwarzer, flockiger Niederschlag aus metallischem Antimon in der Lösung oder direkt am Eisen. Auch der Nachweis an einem Eisennagel ist möglich. Dabei ist eine schwarze Ablagerung am Nagel ein Nachweis für Antimon, welches sich hier elementar niedergeschlagen hat.
Die Marshsche Probe gestattet einen eindeutigen Nachweis von Antimon. Wenn die pyrolytisch abgeschiedene Substanz (dunkel glänzender Spiegel) sich nicht in ammoniakalischem Wasserstoffperoxid löst, sind Arsen und Germanium als mögliche Alternativen ausgeschlossen.
Die hochempfindliche Bestimmung winziger Antimonspuren erfolgt durch die Hydridtechnik der Atomspektrometrie. Hierbei wird im Prinzip die Marshsche Probe mit der Atomabsorptionsspektrometrie gekoppelt. Die Matrixeffekte der Probelösung lassen sich dadurch sehr wirksam unterdrücken.
Eine weitere Methode besteht darin, eine wässrige Lösung, in der Antimonionen enthalten sind, mit Rhodamin-B-Lösung zu versetzen. Es bildet sich ein farbiger Komplex, der mit Isopropylether extrahierbar ist. Dieser Nachweis ist allerdings recht unspezifisch, da auch Gold-, Cadmium-, Gallium, Thallium-, Uran- und Wolfram-ionen farbige Komplexe bilden.

</doc>
<doc id="96" url="https://de.wikipedia.org/wiki?curid=96" title="Argon">
Argon

Argon (griechisch "" „träge“) ist ein chemisches Element mit dem Symbol Ar (bis 1957 nur A) und der Ordnungszahl 18. Im Periodensystem steht es in der 8. Hauptgruppe bzw. der 18. IUPAC-Gruppe und zählt daher zu den Edelgasen. Wie die anderen Edelgase ist es ein farbloses, äußerst reaktionsträges, einatomiges Gas. In vielen Eigenschaften wie Schmelz- und Siedepunkt oder Dichte steht es zwischen dem leichteren Neon und dem schwereren Krypton.
Argon ist das häufigste auf der Erde vorkommende Edelgas, der Anteil an der Atmosphäre beträgt etwa 0,934 %. Damit ist Argon der dritthäufigste Bestandteil der Erdatmosphäre, nach Stickstoff und Sauerstoff. Dies ist großteils auf den Zerfall des Kaliumisotops K zurückzuführen, bei dem Ar entsteht.
Argon war das erste Edelgas, das als Stoff entdeckt und gewonnen wurde, daher der Name, der im Grunde zu jedem Edelgas passt. Helium (von „Helios“, griech. für „Sonne“) wurde vorher lediglich spektroskopisch im Sonnenlicht sowie in irdischen Proben nachgewiesen und Neon erst später entdeckt. Argon wurde 1894 von Lord Rayleigh und William Ramsay durch fraktionierte Destillation von flüssiger Luft gefunden. Als preiswertestes Edelgas wird Argon in großen Mengen als Schutzgas etwa beim Schweißen und in der Produktion von manchen Metallen, aber auch als Füllgas von Glühlampen verwendet.
Einen ersten Hinweis auf das später entdeckte Argon fand Henry Cavendish, der 1783 die Reaktivität der Luft erforschte. Er erzeugte elektrische Entladungen in einer bestimmten Menge Luft, die mit Sauerstoff im Verhältnis von 5:3 angereichert war. Stickstoff und Sauerstoff reagierten miteinander und die entstandenen Stickoxide konnten ausgewaschen werden. Dabei blieb stets ein kleiner Rest nicht-reagierten Gases zurück. Cavendish erkannte jedoch nicht, dass es sich dabei um ein anderes Element handelte und setzte seine Experimente nicht fort.
Nachdem John William Strutt, 3. Baron Rayleigh 1892 die Dichte von aus Luft isoliertem Stickstoff bestimmt hatte, fiel ihm auf, dass aus Ammoniak gewonnener Stickstoff eine niedrigere Dichte aufwies. Es gab verschiedene Spekulationen zu diesem Befund; so meinte James Dewar, es müsse sich um ein N, also ein Stickstoff-Analogon zu Ozon handeln. Rayleigh wiederholte Cavendishs Experimente, indem er in einer luftgefüllten Glaskugel elektrische Funken erzeugte und so Stickstoff und Sauerstoff zur Reaktion brachte. Nach Bestätigung von Cavendishs Ergebnis eines unreaktiven Rückstandes untersuchte William Ramsay diesen ab 1894 durch Überleitung über heißes Magnesium genauer. Da Magnesium mit Stickstoff zum Nitrid reagiert, konnte er dem Gemisch weiteren Stickstoff entziehen. Dabei stellte er eine Erhöhung der Dichte fest und fand schließlich ein bislang unbekanntes, reaktionsträges Gas. Am 31. Januar 1895 gaben Ramsay und Rayleigh schließlich die Entdeckung des neuen Elements bekannt, das sie nach dem altgriechischen "argos", „träge“, "Argon" nannten. Als William Ramsay ab 1898 das aus der Luft isolierte Argon weiter untersuchte, entdeckte er darin drei weitere Elemente, die Edelgase Neon, Krypton und Xenon.
Erste technische Anwendungen fand das Gas in der Elektroindustrie: Es wurden unter anderem Gleichrichter auf der Basis der Glimmentladung in Argon hergestellt, die sogenannten "Tungar-Röhren."
Argon zählt im Universum zu den häufigeren Elementen, in seiner Häufigkeit ist es vergleichbar mit derjenigen von Schwefel und Aluminium. Es ist im Universum nach Helium und Neon das dritthäufigste Edelgas. Dabei besteht das primordiale Argon, das etwa in der Sonne oder Gasplaneten wie Jupiter gefunden wird, hauptsächlich aus den Isotopen Ar und Ar, während das dritte stabile Isotop, Ar, dort nur in geringer Menge vorkommt. Das Verhältnis von Ar zu Ar beträgt etwa 5,7.
Auf der Erde ist Argon dagegen das häufigste Edelgas. Es macht 0,934 % des Volumens der Atmosphäre (ohne Wasserdampf) aus und ist damit nach Stickstoff und Sauerstoff der dritthäufigste Atmosphärenbestandteil. Die Zusammensetzung des terrestrischen Argons unterscheidet sich erheblich von derjenigen des primordialen Argons im Weltall. Es besteht zu über 99 % aus dem Isotop Ar, das durch Zerfall des Kaliumisotops K entstanden ist. Die primordialen Isotope sind dagegen nur in geringen Mengen vorhanden.
Da das Argon durch den Kaliumzerfall in der Erdkruste entsteht, findet man es auch in Gesteinen. Beim Schmelzen von Gesteinen im Erdmantel gast das Argon, aber auch das bei anderen Zerfällen entstehende Helium aus. Es reichert sich daher vorwiegend in den Basalten der ozeanischen Erdkruste an. Aus den Gesteinen wird das Argon an das Grundwasser abgegeben. Daher ist in Quellwasser, vor allem wenn es aus größerer Tiefe kommt, Argon gelöst.
Die Gewinnung des reinen Argons erfolgt ausschließlich aus der Luft, in der Regel im Rahmen der Luftverflüssigung im Linde-Verfahren. Das Argon wird dabei nicht in der Haupt-Rektifikationskolonne des Verfahrens von den Hauptluftbestandteilen getrennt, sondern in einer eigenen Argon-Kolonne. In dieser wird durch Rektifikation zunächst Rohargon hergestellt, das noch etwa 3–5 % Sauerstoff und 1 % Stickstoff enthält.
Anschließend wird das Rohargon in weiteren Stufen gereinigt. Das Gasgemisch wird zunächst auf Raumtemperatur erwärmt und auf 4–6 bar verdichtet. Um den restlichen Sauerstoff zu entfernen, wird danach Wasserstoff eingespritzt, der an Edelmetall-Katalysatoren mit dem Sauerstoff zu Wasser reagiert. Nachdem dieses entfernt wurde, wird in einer weiteren Kolonne das Argon, das sich am unteren Ende der Kolonne anreichert, vom restlichen Stickstoff getrennt, so dass Argon mit einer Reinheit von 99,9999 % (Argon 6.0) produziert werden kann.
Weitere Quellen für die Gewinnung von Argon sind die Produktion von Ammoniak im Haber-Bosch-Verfahren sowie die Synthesegasherstellung, etwa zur Methanolproduktion. Bei diesen Verfahren, die Luft als Ausgangsstoff nutzen, reichern sich Argon und andere Edelgase im Produktionsprozess an und können aus dem Gasgemisch isoliert werden. Wie beim Linde-Verfahren werden auch hier die verschiedenen Gase durch Adsorption oder Rektifikation voneinander getrennt und so reines Argon gewonnen.
Argon ist bei Normalbedingungen ein einatomiges, farbloses und geruchloses Gas, das bei 87,15 K (−186 °C) kondensiert und bei 83,8 K (−189,3 °C) erstarrt. Wie die anderen Edelgase außer dem Helium kristallisiert Argon in einer kubisch dichtesten Kugelpackung mit dem Gitterparameter "a" = 526 pm bei 4 K.
Wie alle Edelgase besitzt Argon nur abgeschlossene Schalen (Edelgaskonfiguration). Dadurch lässt sich erklären, dass das Gas stets einatomig vorliegt und die Reaktivität gering ist.
Mit einer Dichte von 1,784 kg/m bei 0 °C und 1013 hPa ist Argon schwerer als Luft, es sinkt also ab. Im Phasendiagramm liegt der Tripelpunkt bei 83,8 K und 689 hPa, der kritische Punkt bei 150,86 K, 4896 kPa sowie einer kritischen Dichte von 0,536 g/cm.
In Wasser ist Argon etwas löslich. In einem Liter Wasser können sich bei 0 °C und Normaldruck maximal 5,6 g Argon lösen.
Als Edelgas reagiert Argon fast nicht mit anderen Elementen oder Verbindungen. Bislang ist nur das experimentell dargestellte Argonfluorohydrid HArF bekannt, das durch Photolyse von Fluorwasserstoff in einer Argonmatrix bei 7,5 K gewonnen wird und anhand neuer Linien im Infrarotspektrum identifiziert wurde. Oberhalb von 27 K zersetzt es sich. Nach Berechnungen sollten weitere Verbindungen des Argons metastabil sein und sich verhältnismäßig schwer zersetzen; diese konnten jedoch experimentell bislang nicht dargestellt werden. Beispiele hierfür sind das Chloranalogon des Argonfluorohydrides HArCl, aber auch um Verbindungen, bei denen das Proton durch andere Gruppen ersetzt ist, etwa FArCCH als organische Argonverbindung und FArSiF mit einer Argon-Silicium-Bindung.
Argon bildet einige Clathrate, in denen es physikalisch in Hohlräume eines umgebenden Kristalls eingeschlossen ist. Bei −183 °C ist ein Argon-Hydrat stabil, jedoch ist die Geschwindigkeit der Bildung sehr langsam, da eine Umkristallisierung stattfinden muss. Ist das Eis mit Chloroform gemischt, bildet sich das Clathrat schon bei −78 °C. Stabil ist auch ein Clathrat von Argon in Hydrochinon.
Insgesamt sind 23 Isotope sowie ein weiteres Kernisomer von Argon bekannt. Von diesen sind drei, nämlich die Isotope Ar, Ar und Ar, stabil und kommen in der Natur vor. Dabei überwiegt bei weitem Ar mit einem Anteil von 99,6 % am natürlichen irdischen Isotopengemisch. Ar und Ar sind mit einem Anteil von 0,34 % beziehungsweise 0,06 % selten. Von den instabilen Isotopen besitzen Ar mit 269 Jahren und Ar mit 32,9 Jahren die längsten Halbwertszeiten. Alle anderen Isotope besitzen kurze Halbwertszeiten im Bereich von 20 ns bei Ar bis 35,04 Tagen bei Ar.
Ar wird für die Altersbestimmung von Gesteinen genutzt (Kalium-Argon-Datierung). Dabei wird ausgenutzt, dass instabiles K, das in diesen enthalten ist, langsam zu Ar zerfällt. Je mehr Kalium zu Argon zerfallen ist, desto älter ist das Gestein. Das kurzlebige Isotop Ar kann zur Überprüfung von Gasleitungen verwendet werden. Durch das Durchleiten von Ar kann die Leistungsfähigkeit einer Belüftung oder Dichtigkeit einer Leitung festgestellt werden.
"→ Liste der Argon-Isotope"
Wie die anderen Edelgase hat Argon auf Grund der Reaktionsträgheit keine biologische Bedeutung und ist auch nicht toxisch. In höheren Konzentrationen wirkt es durch Verdrängung des Sauerstoffs erstickend. Bei Drücken von mehr als 24 bar wirkt es narkotisierend.
Als günstigstes und in großen Mengen verfügbares Edelgas wird Argon in vielen Bereichen verwendet. Die Produktion betrug 1998 weltweit etwa 2 Milliarden m³ bzw. 2 km³. Der größte Teil des Argons wird als Schutzgas verwendet. Es wird immer dann genutzt, wenn der billigere Stickstoff nicht anwendbar ist. Dazu zählen vor allem Schweißverfahren für Metalle, die mit Stickstoff bei hohen Temperaturen reagieren, etwa Titan, Tantal und Wolfram. Auch beim Metallinertgasschweißen und Wolfram-Inertgasschweißen, die etwa beim Schweißen von Aluminiumlegierungen oder hoch legierten Stählen angewendet werden, dient Argon als Inertgas. Weiterhin wird es in der Metallurgie als Schutzgas, etwa für die Produktion von Titan, hochreinem Silicium oder der Schmelzraffination sowie zum Entgasen von Metallschmelzen genutzt.
Argon ist ein Lebensmittelzusatzstoff (E 938) und dient als Treib- und Schutzgas bei der Verpackung von Lebensmitteln und der Weinherstellung.
Argon wird als gasförmiges Löschmittel vorwiegend für den Objektschutz, vor allem bei elektrischen und EDV-Anlagen eingesetzt und wirkt dabei durch Sauerstoffverdrängung. Für diesen Zweck wird reines Argon oder ein Gasgemisch zusammen mit Stickstoff verwendet.
In der Analytik wird Argon als Träger- und Schutzgas für die Gaschromatographie und das induktiv gekoppelte Plasma (ICP-MS, ICP-OES) verwendet.
Glühlampen werden häufig mit Argon-Stickstoff-Gemischen gefüllt, weil eine Gasfüllung die Sublimation des Glühfadens vermindert. Argon hat dabei eine geringere Wärmeleitfähigkeit als leichtere Gase, ist aber preiswerter als andere schwerere und damit noch geringer wärmeleitende Gase wie Krypton oder Xenon. Ein Vorteil der geringeren Wärmeleitfähigkeit ist eine höhere mögliche Glühtemperatur und damit höhere Lichtausbeute. Ebenfalls wegen der geringen Wärmeleitfähigkeit wird es als Füllgas für Isolierglasscheiben verwendet. Auch in Gasentladungslampen dient Argon als Leuchtgas mit einer typischen violetten Farbe. Wird etwas Quecksilber dazugegeben, ändert sich die Farbe ins Blaue. Weiterhin ist Argon das Lasermedium in Argon-Ionen-Lasern.
Im Bereich der Stahlerzeugung kommt Argon eine besonders wichtige Rolle im Bereich der Sekundärmetallurgie zu. Mit der Argon-Spülung kann die Stahllegierung entgast und gleichzeitig homogenisiert werden, speziell wird dabei der unerwünschte, gelöste Stickstoff aus der Schmelze entfernt.
Beim Tauchen wird Argon – insbesondere bei der Nutzung des Helium­haltigen Trimix als Atemgas – dazu verwendet, um Trockentauchanzüge zu füllen bzw. damit zu tarieren. Hierbei wird ebenfalls die geringe Wärmeleitfähigkeit des Gases genutzt, um das Auskühlen des Anzugträgers zu verzögern.
Seit Mai 2014 ist Argon auf der Dopingliste der Welt-Anti-Doping-Agentur (WADA). Durch den bei der Inhalation von Argon entstehenden Sauerstoffmangel wird offensichtlich die Bildung von körpereigenem Erythropoetin (EPO) aktiviert. Aus demselben Grund ist auch Xenon auf der Dopingliste.

</doc>
<doc id="97" url="https://de.wikipedia.org/wiki?curid=97" title="Arsen">
Arsen

Arsen [] ist ein chemisches Element mit dem Elementsymbol As und der Ordnungszahl 33. Im Periodensystem der Elemente steht es in der 4. Periode und der 5. Hauptgruppe, bzw. 15. IUPAC-Gruppe oder Stickstoffgruppe. Arsen kommt selten gediegen vor, meistens in Form von Sulfiden. Es gehört zu den Halbmetallen, da es je nach Modifikation metallische oder nichtmetallische Eigenschaften zeigt.
Umgangssprachlich wird auch das als Mordgift bekannte Arsenik meist einfach „Arsen“ genannt. Arsenverbindungen kennt man schon seit dem Altertum. Als mutagenes Klastogen können Arsenverbindungen als Gift wirken, welches Chromosomenaberrationen hervorrufen kann und somit kanzerogene Wirkung besitzen kann.
Arsen wird zur Dotierung von Halbleitern und als Bestandteil von III-V-Halbleitern wie Galliumarsenid genutzt. Die organische Arsenverbindung Arsphenamin ("Salvarsan") war Anfang des 20. Jahrhunderts ein Durchbruch in der Behandlung der Syphilis. Heute wird Arsentrioxid als letzte Behandlungsoption in der Therapie der Promyelozytenleukämie angewendet.
Der Name Arsen geht unmittelbar auf zurück, der Bezeichnung des Arsenminerals Auripigment. Sie findet sich schon bei Dioskurides im 1. Jahrhundert. Die griechische Bezeichnung scheint ihrerseits ihren Ursprung im Altpersischen "(al-)zarnik" (goldfarben, Auripigment, „Arsen“) zu haben und gelangte wohl durch semitische Vermittlung ins Griechische. Volksetymologisch wurde der Name fälschlicherweise vom (alt- und neu-)griechischen Wort αρσενικός "arsenikós" abgeleitet, das sich etwa mit männlich/stark übersetzen lässt. Erst seit dem 19. Jahrhundert ist die Bezeichnung Arsen gebräuchlich. Das Elementsymbol wurde 1814 von Jöns Jakob Berzelius vorgeschlagen.
Der erste Kontakt von Menschen mit Arsen lässt sich aus dem 3. Jahrtausend v. Chr. nachweisen: In den Haaren der im Gletschereis erhaltenen Mumie des volkstümlich Ötzi genannten Alpenbewohners ließen sich größere Mengen Arsen nachweisen, was archäologisch als Hinweis darauf gedeutet wird, dass der betroffene Mann in der Kupferverarbeitung tätig war – Kupfererze sind oft mit Arsen verunreinigt. Im klassischen Altertum war Arsen in Form der Arsen-Sulfide Auripigment (AsS) und Realgar (AsS) bekannt, die etwa von dem Griechen Theophrastos, dem Nachfolger Aristoteles, beschrieben wurden. Auch der griechische Philosoph Demokrit hatte im 2. Jahrhundert v. Chr. nachweislich Kenntnisse über Arsenverbindungen. Der Leidener Papyrus X aus dem 3. Jahrhundert nach Chr. lässt darauf schließen, dass sie benutzt wurden, um Silber goldartig und Kupfer weiß zu färben. Der römische Kaiser Caligula hatte angeblich bereits im 1. Jahrhundert nach Chr. ein Projekt zur Herstellung von Gold aus dem (goldgelben) Auripigment in Auftrag gegeben. Die Alchimisten, die Arsen-Verbindungen nachweislich der Erwähnung im antiken Standardwerk "Physica et Mystica" kannten, vermuteten eine Verwandtschaft mit Schwefel und Quecksilber. Arsen(III)-sulfid kam als Malerfarbe und Enthaarungsmittel zum Einsatz sowie zur äußerlichen als auch inneren Behandlung von Lungenkrankheiten.
Im Mittelalter wurde Arsenik (Arsen(III)-oxid) im Hüttenrauch (staubbeladenes Abgas metallurgischer Öfen) gefunden. Albertus Magnus beschrieb um 1250 erstmals die Herstellung von Arsen durch Reduktion von Arsenik mit Kohle. Er gilt daher als Entdecker des Elements, auch wenn es Hinweise darauf gibt, dass das elementare Metall schon früher hergestellt wurde. Paracelsus führte es im 16. Jahrhundert in die Heilkunde ein. Etwa zur gleichen Zeit wurden Arsenpräparate in der chinesischen Enzyklopädie "Pen-ts'ao Kang-mu" des Apothekers Li Shi-zhen beschrieben. Dieser Autor hebt insbesondere die Anwendung als Pestizid in Reisfeldern hervor.
Im 17. Jahrhundert wurde das gelbe Auripigment bei holländischen Malern als "Königsgelb" populär. Da sich das Pigment über längere Zeiträume hinweg in Arsen(III)-oxid umwandelt und von der Leinwand bröckelt, entstehen Schwierigkeiten bei der Restaurierung. Ab 1740 wurden Arsenpräparate in Europa mit Erfolg als Beizmittel im Pflanzenschutz eingesetzt. Diese Nutzung verbot man jedoch 1808 wegen ihrer hohen Giftigkeit wieder. Der Einsatz von Arsenzusätzen für den Bleiguss beruht auf der größeren Härte solcher Bleilegierungen, typische Anwendung sind Schrotkugeln. Obwohl die Giftigkeit und die Verwendung als Mordgift bekannt war, ist Arsen im beginnenden 19. Jahrhundert eines der bedeutendsten Asthmamittel. Grundlage sind anscheinend Berichte, in denen den Chinesen nachgesagt wurde, sie würden Arsen in Kombination mit Tabak rauchen, um Lungen zu bekommen, die stark wie Blasebälge seien. Es wurde in Form von Kupferarsenaten in Farbmitteln wie dem Pariser Grün eingesetzt, um Tapeten zu bedrucken. Bei hoher Feuchtigkeit wurden diese Pigmente durch Schimmelpilzbefall in giftige flüchtige Arsenverbindungen umgewandelt, die nicht selten zu chronischen Arsenvergiftungen führten.
Doch auch in Kriegen fand Arsen Verwendung: Im Ersten Weltkrieg wurden Arsenverbindungen in chemischen Kampfstoffen (Blaukreuz) oder Lewisit eingesetzt. Bei den Opfern bewirkten sie durch Angriff auf Haut und Lungen grausame Schmerzen und schwerste körperliche Schädigungen.
Arsen kommt in geringen Konzentrationen von bis zu 10 ppm praktisch überall im Boden vor. Es ist in der Erdkruste ungefähr so häufig wie Uran oder Germanium. In der kontinentalen Erdkruste kommt Arsen mit durchschnittlich 1,7 ppm vor, wobei es durch seinen lithophilen Charakter (= Silikat liebend) in der oberen Kruste angereichert ist (2 ppm gegenüber 1,3 ppm in der unteren Kruste); damit liegt Arsen in der Tabelle der häufigsten Elemente an 53. Stelle.
Arsen ("Scherbenkobalt") kommt in der Natur gediegen, das heißt in elementarer Form, vor und ist daher von der International Mineralogical Association (IMA) als eigenständiges Mineral anerkannt. Gemäß der Systematik der Minerale nach Strunz (9. Auflage) wird Arsen unter der System-Nr. 1.CA.05 (Elemente – Halbmetalle (Metalloide) und Nichtmetalle – Arsengruppen-Elemente) (8. Auflage: "I/B.01-10") eingeordnet. Die im englischsprachigen Raum ebenfalls geläufige Systematik der Minerale nach Dana führt das Element-Mineral unter der System-Nr. 01.03.01.01.
Weltweit sind zurzeit (Stand: 2011) rund 330 Fundorte für gediegenes Arsen bekannt. In Deutschland wurde es an mehreren Fundstätten im Schwarzwald (Baden-Württemberg), im bayerischen Spessart und Oberpfälzer Wald, im hessischen Odenwald, in den Silberlagerstätten des Westerzgebirges (Sachsen), am Hunsrück (Rheinland-Pfalz) sowie im Thüringer Wald gefunden. In Österreich trat Arsen an mehreren Fundstätten in Kärnten, Salzburg und der Steiermark zutage. In der Schweiz fand sich gediegen Arsen in den Kantonen Aargau und Wallis.
Weitere Fundorte sind in Australien, Belgien, Bolivien, Bulgarien, Chile, China, Finnland, Frankreich, Griechenland, Irland, Italien, Japan, Kanada, Kasachstan, Kirgisistan, Madagaskar, Malaysia, Marokko, Mexiko, Mongolei, Neuseeland, Norwegen, Österreich, Peru, Polen, Rumänien, Russland, Schweden, Slowakei, Spanien, Tschechien, Ukraine, Ungarn, im Vereinigten Königreich (Großbritannien) und in den Vereinigten Staaten (USA) bekannt.
Weit häufiger kommt das Element allerdings in verschiedenen intermetallischen Verbindungen mit Antimon (Allemontit) und Kupfer (Whitneyit) sowie in verschiedenen Mineralen vor, die überwiegend der Klasse der Sulfide und Sulfosalze angehören. Insgesamt sind bisher (Stand: 2011) 565 Arsenminerale bekannt. Die höchsten Konzentrationen an Arsen enthalten dabei unter anderem die Minerale Duranusit (ca. 90 %), Skutterudit und Arsenolith (jeweils ca. 76 %), die allerdings selten zu finden sind. Weit verbreitet sind dagegen Arsenopyrit ("Arsenkies"), Löllingit, Realgar ("Rauschrot") und Auripigment ("Orpiment", "Rauschgelb"). Weitere bekannte Minerale sind Cobaltit ("Kobaltglanz"), Domeykit ("Arsenkupfer"), Enargit, Gersdorffit ("Nickelarsenkies"), Proustit ("Lichtes Rotgültigerz", "Rubinblende"), Rammelsbergit sowie Safflorit und Sperrylith.
Arsenate finden sich häufig in phosphathaltigen Gesteinen, da sie eine vergleichbare Löslichkeit aufweisen und das häufigste Sulfidmineral Pyrit kann bis zu einigen Massenprozent Arsen einbauen.
Arsen wird heutzutage als Nebenprodukt der Verhüttung von Gold-, Silber-, Zinn-, Kupfer-, Cobalt- und weiteren Buntmetallerzen sowie bei der Verarbeitung von Phosphatrohstoffen gewonnen. Die größten Produzenten im Jahr 2009 waren China, Chile, Marokko und Peru. Arsen ist nur schwer wasserlöslich und findet sich daher nur in geringen Spuren, etwa 1,6 ppb (Milliardstel Massenanteilen) in Meeren und Ozeanen.
In der Luft findet man Arsen in Form von partikulärem Arsen(III)-oxid. Als natürliche Ursache dafür hat man Vulkanausbrüche identifiziert, die insgesamt jährlich geschätzte 3000 Tonnen in die Erdatmosphäre eintragen. Bakterien setzen weitere 20.000 Tonnen in Form organischer Arsenverbindungen wie Trimethylarsin frei. Ein großer Teil am freigesetzten Arsen entstammt der Verbrennung fossiler Brennstoffe wie Kohle oder Erdöl. Die geschätzten Emissionen, verursacht durch den Straßenverkehr und stationäre Quellen, betrugen 1990 in der Bundesrepublik Deutschland 120 Tonnen (20 Tonnen in den alten, 100 Tonnen in den neuen Bundesländern). Die Außenluftkonzentration von Arsen liegt zwischen 0,5 und 15 Nanogramm pro Kubikmeter.
Arsen fällt in größeren Mengen als Nebenprodukt bei der Gewinnung von Kupfer, Blei, Cobalt und Gold an. Dies ist die Hauptquelle für die kommerzielle Nutzung des Elements.
Es kann durch thermische Reduktion von Arsen(III)-oxid mit Koks oder Eisen und durch Erhitzen von Arsenkies (FeAsS) oder Arsenikalkies (FeAs) unter Luftabschluss in liegenden Tonröhren gewonnen werden. Dabei sublimiert elementares Arsen, das an kalten Oberflächen wieder in den festen Aggregatzustand zurückkehrt.
Für die Halbleitertechnik wird Arsen, dessen Reinheit über 99,99999 Prozent betragen muss, durch Reduktion von mehrfach destilliertem Arsen(III)-chlorid im Wasserstoffstrom hergestellt:
Früher wurde es auch durch Sublimation aus Lösungen in flüssigem Blei erzeugt. Dabei wird der Schwefel der Arsen-Erze durch das Blei in Form von Blei(II)-sulfid gebunden. Die hierbei erzielten Reinheiten von über 99,999 Prozent waren für Halbleiteranwendungen nicht ausreichend. Eine andere Möglichkeit besteht im Auskristallisieren bei hohen Temperaturen aus geschmolzenem Arsen oder in der Umwandlung in Monoarsan, einer anschließenden Reinigung sowie der Zersetzung bei 600 °C in Arsen und Wasserstoff.
Arsen bildet mit Stickstoff, Phosphor, Antimon und Bismut die 5. Hauptgruppe des Periodensystems und nimmt wegen seiner physikalischen und chemischen Eigenschaften den Mittelplatz in dieser Elementgruppe ein. Arsen hat eine relative Atommasse von 74,92159. Der Radius des Arsen-Atoms beträgt 124,5 Pikometer. In kovalent gebundenem Zustand ist er etwas kleiner (121 Pikometer). Aufgrund der Abgabe der äußeren Elektronen (Valenzelektronen) bei der Ionisierung reduziert sich der Radius beträchtlich auf 34 Pikometer (As; das äußerste p- und das äußerste s-Atomorbital bleiben unbesetzt) beziehungsweise 58 Pikometer (As; nur das p-Orbital ist unbesetzt). In chemischen Komplexverbindungen ist das As-Kation von vier Bindungspartnern (Liganden), As von sechs umgeben. Arsen tritt allerdings nur sehr selten in eindeutig ionischer Form auf.
Der Wert für die Elektronegativität liegt nach Pauling auf der von 0 (Metalle) bis 4 (Nichtmetall) reichenden Skala bei 2,18 und ist damit mit dem Wert des Gruppennachbarn Phosphor vergleichbar. Der Halbmetall-Charakter des Arsens zeigt sich zudem darin, dass die benötigte Dissoziationsenergie von 302,7 kJ/mol, also die Energie, die aufgebracht werden muss, um ein einzelnes Arsen-Atom aus einem Arsen-Festkörper herauszulösen, zwischen der des Nichtmetalls Stickstoff (473,02 kJ/mol; kovalente Bindung) und des Metalls Bismut (207,2 kJ/mol; metallische Bindung) liegt. Unter Normaldruck sublimiert Arsen bei einer Temperatur von 613 °C, geht also aus dem festen Aggregatzustand direkt in die Gasphase über. Arsendampf ist zitronengelb und setzt sich bis ungefähr 800 °C aus As-Molekülen zusammen. Oberhalb von 1700 °C liegen As-Moleküle vor.
Arsen zeigt je nach Verbindungspartner Oxidationsstufen zwischen −3 und +5. Mit elektropositiven Elementen wie Wasserstoff oder Metallen bildet es Verbindungen, in denen es eine Oxidationsstufe von −3 einnimmt. Beispiele dafür sind Monoarsan (AsH) und Arsenkupfer (CuAs). In Verbindungen mit elektronegativen Elementen wie den Nichtmetallen Sauerstoff, Schwefel und Chlor besitzt es die Oxidationsstufe +3 oder +5; erstere ist dabei gegenüber den in derselben Hauptgruppe stehenden Elementen Stickstoff und Phosphor tendenziell bevorzugt.
Arsen kommt wie andere Elemente der Stickstoffgruppe in verschiedenen allotropen Modifikationen vor. Anders als beim Stickstoff, der in Form zweiatomiger Moleküle mit kovalenter Dreifachbindung vorkommt, sind die entsprechenden As-Moleküle instabil und Arsen bildet stattdessen kovalente Netzwerke aus.
Graues oder metallisches Arsen ist die stabilste Form. Es hat eine Dichte von 5,72 g/cm. Seine Kristalle sind stahlgrau, metallisch glänzend und leiten den elektrischen Strom.
Betrachtet man den strukturellen Aufbau des grauen Arsens, dann erkennt man Schichten aus gewellten Arsen-Sechsringen, welche die Sesselkonformation einnehmen. Darin bilden die Arsen-Atome eine Doppelschicht, wenn man sich den Aufbau der Schicht im Querschnitt ansieht. Die Übereinanderlagerung dieser Doppelschichten ist sehr kompakt. Bestimmte Atome der nächsten darüberliegenden oder darunterliegenden Schicht sind von einem Bezugsatom fast ähnlich weit entfernt wie innerhalb der betrachteten Doppelschicht. Dieser Aufbau bewirkt, dass die graue Arsen-Modifikation wie die homologen Elemente Antimon und Bismut sehr spröde ist. Deswegen werden diese drei Elemente häufig auch als Sprödmetalle bezeichnet.
Wird Arsen-Dampf, in dem Arsen gewöhnlich als As-Tetraeder vorliegt, schnell abgekühlt, so bildet sich das metastabile gelbe Arsen mit einer Dichte von 1,97 g/cm. Es besteht ebenfalls aus tetraedrischen As-Molekülen. Gelbes Arsen ist ein Nichtmetall und leitet infolgedessen den elektrischen Strom nicht. Es kristallisiert aus Schwefelkohlenstoff und bildet kubische, stark lichtbrechende Kristalle, die nach Knoblauch riechen. Bei Raumtemperatur und besonders schnell unter Lichteinwirkung wandelt sich gelbes Arsen in graues Arsen um.
Schwarzes Arsen selbst kann seinerseits in zwei verschiedenen Formen vorkommen. "Amorphes schwarzes Arsen" entsteht durch Abkühlung von Arsen-Dampf an 100 bis 200 °C warmen Oberflächen. Es besitzt keine geordnete Struktur, sondern liegt in einer amorphen, glasartigen Form vor, analog zum roten Phosphor. Die Dichte beträgt 4,7 bis 5,1 g/cm. Oberhalb 270 °C wandelt sich das schwarze Arsen in die graue Modifikation um. Wird glasartiges, amorphes schwarzes Arsen bei Anwesenheit von metallischem Quecksilber auf 100 bis 175 °C erhitzt, so entsteht das metastabile "orthorhombische schwarze Arsen", das mit dem schwarzen Phosphor vergleichbar ist.
Natürlich gebildetes orthorhombisches schwarzes Arsen ist in der Natur als seltenes Mineral Arsenolamprit bekannt.
Bei der Reduktion von Arsenverbindungen in wässriger Lösung entstehen ähnlich wie beim Phosphor Mischpolymerisate. Bei diesen bindet ein Teil der freien Valenzen des Arsens Hydroxygruppen (–OH). Man nennt diese Form des Arsens "braunes Arsen".
Arsen reagiert heftig mit Oxidationsmitteln und Halogenen. So verbrennt Arsen an der Luft mit bläulicher Flamme zu einem weißen Rauch von giftigem Arsen(III)-oxid.
Ohne äußere Wärmezufuhr findet die Reaktion mit Chlor unter Feuererscheinung zu Arsen(III)-chlorid statt.
Eine weitere Oxidation ist möglich.
Analoge Reaktionsgleichungen gelten für die entsprechenden Reaktionen mit Fluor.
Stark oxidierende Säuren, wie konzentrierte Salpetersäure oder Königswasser, wandeln Arsen in Arsensäure um.
Ist die Oxidationsstärke weniger groß – etwa bei Verwendung von verdünnter Salpetersäure oder Schwefelsäure – entsteht Arsenige Säure.
Unter sauren Bedingungen und bei Anwesenheit von Metallen beziehungsweise Leichtmetallen, zum Beispiel Zink, reagiert Arsen mit dem gebildeten Wasserstoff zu Monoarsan.
Mit basischem Natriumhydroxid bildet sich das entsprechende Arsenitsalz.
Vom Arsen sind künstlich hergestellte, radioaktive Isotope mit Massenzahlen zwischen 65 und 87 bekannt. Die Halbwertszeiten liegen zwischen 96 Millisekunden (As) und 80,3 Tagen (As). Natürlich vorkommendes Arsen besteht zu 100 Prozent aus dem Isotop As, es ist daher ein anisotopes Element. Der entsprechende Arsen-Kern besteht also aus genau 33 Protonen und 42 Neutronen. Physikalisch zählt man ihn daher zu den ug-Kernen (u steht hier für ungerade, g für gerade). Sein Kernspin beträgt 3/2.
Arsen wird Bleilegierungen zugesetzt, um ihre Festigkeit zu verbessern und das Blei gießbar zu machen. Vor allem die fein strukturierten Platten von Akkumulatoren könnten ohne Arsen nicht gegossen werden. Historisch war Arsen eine wichtige Zutat von Kupferlegierungen, die dadurch besser verarbeitbar wurden. Metallisches Arsen wurde früher gelegentlich zur Erzeugung mattgrauer Oberflächen auf Metallteilen verwendet, um eine Alterung vorzutäuschen.
In der Elektronik spielt es als mindestens 99,9999 Prozent reines Element für Gallium-Arsenid-Halbleiter, so genannte III-V-Halbleiter (aufgrund der Kombination von Elementen aus der 3. und 5. Hauptgruppe des Periodensystems), sowie für Epitaxieschichten auf Wafern in Form von Indiumarsenidphosphid und Galliumarsenidphosphid eine wesentliche Rolle in der Herstellung von Hochfrequenzbauelementen wie Integrierten Schaltkreisen (ICs), Leuchtdioden (LEDs) beziehungsweise Laserdioden (LDs). Es gibt Anfang 2004 weltweit nur drei Hersteller von hochreinem Arsen, zwei in Deutschland und einen in Japan.
Arsen wird in Form seiner Verbindungen in einigen Ländern als Schädlingsbekämpfungsmittel im Weinbau, als Fungizid (Antipilzmittel) in der Holzwirtschaft, als Holzschutzmittel, als Rattengift und als Entfärbungsmittel in der Glasherstellung verwendet. Der Einsatz ist umstritten, da die eingesetzten Arsenverbindungen (hauptsächlich Arsen(III)-oxid) hochtoxisch sind.
Die Verwendung arsenhaltiger Mineralien als Heilmittel ist bereits in der Antike durch Hippokrates und Plinius bezeugt. Sie wurden als Fiebermittel, als Stärkungsmittel und zur Therapie von Migräne, Rheumatismus, Malaria, Tuberkulose und Diabetes eingesetzt. Im 18. Jahrhundert wurde eine Mischung aus Kaliumarsenit und Lavendelwasser als Fowler'sche Lösung bekannt, die lange als medizinisches Wundermittel galt und als Fiebersenker, Heilwasser und sogar als Aphrodisiakum Anwendung fand. Kaliumarsenit war als Bestandteil der Fowler'schen Lösung bis in die 1960er Jahre in Deutschland als Mittel zur Behandlung der Psoriasis im Einsatz.
Constantinus Africanus (1017–1087) empfahl eine Arsenapplikation zur Bekämpfung von Zahnschmerzen. Bereits um 2700 vor Christus soll die Anwendung von Arsen zur Behandlung eines schmerzenden Zahnes in der chinesischen Heilkunst beschrieben worden sein. In dem Mitte des 10. Jahrhunderts erschienenen Werk „"Liber Regius"“ empfahl der arabische Arzt Haly Abbas (ʿAli ibn al-ʿAbbās, † 944) ebenfalls den Einsatz von Arsenik zur Devitalisation der Pulpa. Arsen(III)-oxid wurde bis in die Neuzeit zur Devitalisation der Zahnpulpa verwendet und verschwand in den 1970er Jahren wegen der krebserregenden Wirkung, Entzündungen des Zahnhalteapparates, des Verlustes eines oder mehrerer Zähne einschließlich Nekrosen des umliegenden Alveolarknochens, Allergien und Vergiftungserscheinungen aus dem Therapiespektrum.
Einen Aufschwung erlebten arsenhaltige Arzneimittel zu Beginn des 20. Jahrhunderts. Harold Wolferstan Thomas und Anton Breinl konnten 1905 beobachten, dass das arsenhaltige Präparat Atoxyl Trypanosomen, zu denen die Erreger der Schlafkrankheit gehören, abtötet. 1920 wurde eine Weiterentwicklung, das Tryparsamid, in der Zeit von 1922 bis 1970 im tropischen Afrika zur Therapie der Schlafkrankheit eingesetzt. Es war bedeutsam für die Eingrenzung dieser Epidemie in der ersten Hälfte des vorigen Jahrhunderts, konnte jedoch zur Erblindung führen. Das in den 1950er Jahren entwickelte Melarsoprol war über mehrere Jahrzehnte das Mittel der Wahl zur Behandlung der Schlafkrankheit und wird heute noch eingesetzt, da keine effektiven Nachfolgepräparate zur Verfügung stehen.
Ebenfalls angeregt durch die Trypanosomen-toxische Wirkung von Atoxyl entwickelte Paul Ehrlich das arsenhaltige Arsphenamin (Salvarsan). Das 1910 in die Therapie der Syphilis eingeführte Mittel stellte das erste auf theoretischen Vorüberlegungen beruhende, systematisch entwickelte, spezifisch wirkende Chemotherapeutikum dar und war Vorbild für die Entwicklung der bis heute verwendeten Sulfonamide. Es wurde lange Zeit auch bei der Behandlung von Dysenterie eingesetzt.
Im Jahr 2000 wurde ein arsenikhaltiges Präparat unter dem Namen "Trisenox" in den USA zur Behandlung der promyelozytären Leukämie (APL) zugelassen. Seit 2002 besteht für "Trisenox" in Europa eine Zulassung zur Behandlung der APL, (Vertrieb in EU und USA: Cephalon). Seine Wirksamkeit bei der Krebstherapie wird auch auf die antiangioneogenetische Wirkung zurückgeführt.
Die verschiedenen Arsensulfide sind Bestandteil von Arzneimitteln der Chinesischen Medizin.
Aufgrund der toxischen Eigenschaften von Arsenverbindungen wurde früher überwiegend Arsenik zur Haltbarmachung von Wirbeltieren ("Taxidermie") als Insektizid verwendet. Viele andere Stoffe, wie auch Lindan, wurden zum selben Zweck verwendet, wie es die Fachliteratur der Präparatoren aus der Zeit von 1868 bis 1996 beschreibt. Solche Stoffe sind jedoch auch für Menschen giftig und stellen heute an Präparatoren besondere Anforderungen, da diese auch in Kontakt mit derart kontaminierten Präparaten kommen.
Die biologische Bedeutung des Arsens für den Menschen ist nicht vollständig geklärt. Es gilt als Spurenelement im Menschen, Mangelerscheinungen wurden bisher aber nur an Tieren nachgewiesen. Der notwendige Bedarf liegt, falls er bestehen sollte, zwischen 5 und 50 µg pro Tag. Eine tägliche Arsenaufnahme von – je nach Wahl der Nahrungsmittel – bis zu einem Milligramm gilt als harmlos. In einer neuen Studie konnte eine erhöhte Arsenbelastung durch hohe Arsengehalte im Grundwasser von Reisanbaugebieten mit der Entstehung von Krebserkrankungen in Verbindung gebracht werden. Die Förderung der Krebsentwicklung ist jedoch dosisabhängig und nur bei Verzehr von belastetem Reis als täglichem Grundnahrungsmittel gegeben. Es gibt bei regelmäßigem Verzehr von Arsenverbindungen, speziell Arsentrioxid eine Gewöhnung, die beim Absetzen der Dosis sogar mit Entzugserscheinungen begleitet werden. Menschen, die eine solche Gewöhnung erworben haben, werden Arsenikesser genannt.
Meerestiere wie Muscheln oder Garnelen enthalten besonders viel Arsen, letztere bis zu 175 ppm. Vermutlich agiert es durch die Bindung an freie Thiolgruppen in Enzymen als Inhibitor, verhindert also deren Wirkung.
Für viele Tiere ist Arsen ein essentielles Spurenelement. So zeigen Hühner oder Ratten bei arsenfreier Ernährung deutliche Wachstumsstörungen; dies hängt wahrscheinlich mit dem Einfluss des Elements auf die Verstoffwechslung der Aminosäure Arginin zusammen. Zahlreiche Algen und Krebstiere enthalten organische Arsen-Verbindungen wie das schon erwähnte Arsenobetain. Arsen führt zur verstärkten Bildung der sauerstofftransportierenden roten Blutkörperchen. Aus diesem Grund wurde es früher dem Futter von Geflügel und Schweinen zugesetzt, um eine schnellere Mästung zu ermöglichen. Trainer von Rennpferden benutzten es zum illegalen Doping ihrer Tiere – heute kann der Zusatz von Arsen zur Nahrung allerdings leicht im Urin nachgewiesen werden.
Lösliche Arsenverbindungen werden leicht über den Magen-Darm-Trakt aufgenommen und rasch innerhalb von 24 Stunden im Körper verteilt. Man findet den größten Teil des aufgenommenen Arsens in den Muskeln, Knochen, Nieren und Lungen. Im Menschen wurde es zusammen mit Thallium in fast jedem Organ nachgewiesen. Blut enthält bis zu 8 ppb Arsen, in den anderen Organen des Körpers wie etwa den Knochen hat es einen Anteil von zwischen 0,1 und 1,5 ppm, in Haaren liegt der Anteil bei etwa 1 ppm. Der Gesamtgehalt von Arsen im Körper eines Erwachsenen liegt im Durchschnitt bei etwa 7 Milligramm.
Organische Arsenverbindungen wie die aus Fischen und Meeresfrüchten stammende Dimethylarsinsäure, Trimethylarsenoxid, Trimethylarsin sowie Arsenobetain verlassen den menschlichen Körper fast unverändert innerhalb von zwei bis drei Tagen über die Nieren. Anorganische Arsenverbindungen werden in der Leber zu Monomethylarsonsäure (MMAA) und Dimethylarsinsäure (DMAA) umgewandelt und anschließend ebenso über die Nieren ausgeschieden.
Bei Pflanzen erhöht das Element den Kohlenhydrat-Umsatz. Der Gebänderte Saumfarn "(Pteris vittata)" nimmt das Halbmetall bevorzugt aus dem Boden auf und kann bis zu fünf Prozent seines Trockengewichts an Arsen aufnehmen. Aus diesem Grund wird die schnellwachsende Pflanze zur biologischen Säuberung arsenkontaminierter Böden eingesetzt.
Die stimulierende Wirkung des Arsens ist vermutlich auch Ursache des früher in einigen Alpengegenden verbreiteten Arsenikessens. Im 17. Jahrhundert verzehrten manche der dortigen Bewohner lebenslang zweimal wöchentlich bis zu 250 Milligramm Arsen – bei Männern, weil es bei der Arbeit in den Höhenlagen half, bei Frauen, da es angeblich zu einer kräftigen Gesichtsfarbe beitrug. In der Wissenschaft lange als Märchen abgetan, nahm ein Bauer aus den Steirischen Alpen 1875 vor der in Graz versammelten deutschen Fachwelt eine Dosis von 400 Milligramm Arsentrioxid zu sich, die sich später auch in seinem Urin nachweisen ließ. Die Dosis lag weit über dem Doppelten der für normale Menschen tödlichen Arsenmenge, zeigte aber keinerlei negative Auswirkungen auf den Bauern. Ähnliches wird von Bewohnern einer Siedlung in der hochgelegenen chilenischen Atacamawüste berichtet, deren Trinkwasser hochgradig mit Arsen belastet ist, die jedoch keinerlei Vergiftungssymptome zeigen. Heute geht man davon aus, dass eine langsame Gewöhnung an das Gift mit sukzessive steigenden Dosen physiologisch möglich ist.
Über den Bakterienstamm GFAJ-1 wurde 2010 berichtet, dass er unter bestimmten Bedingungen in arsenathaltigen Nährmedien in der Lage sei, Arsenat anstatt Phosphat in Biomoleküle wie die DNA einzubauen, ohne dabei abzusterben, was bisher eher als unmöglich galt. Der Befund scheint jedoch auf unsauberen Arbeitsmethoden zu basieren, die Befunde konnten nicht repliziert werden.
Arsen-Stäube sind leicht entzündlich.
Dreiwertige lösliche Verbindungen des Arsens sind hoch toxisch, weil sie biochemische Prozesse wie die DNA-Reparatur, den zellulären Energiestoffwechsel, rezeptorvermittelte Transportvorgänge und die Signaltransduktion stören. Dabei kommt es mutmaßlich nicht zu einer direkten Einwirkung auf die DNA, sondern zu einer Verdrängung des Zink-Ions aus seiner Bindung zu Metallothioneinen und damit zur Inaktivierung von Tumor-Repressor-Proteinen (siehe auch Zinkfingerprotein). Arsen(III)- und Zink(II)-Ionen haben vergleichbare Ionenradien und damit ähnliche Affinität zu diesen Zink-Finger-Proteinen, allerdings führt Arsen dann nicht zur Aktivierung der Tumor-Repressor-Proteine.
Eine akute Arsenvergiftung führt zu Krämpfen, Übelkeit, Erbrechen, inneren Blutungen, Durchfall und Koliken, bis hin zu Nieren- und Kreislaufversagen. Bei schweren Vergiftungen fühlt sich die Haut feucht und kalt an und der Betroffene kann in ein Koma fallen. Die Einnahme von 60 bis 170 Milligramm Arsenik gilt für Menschen als tödliche Dosis (LD = 1,4 mg/kg Körpergewicht); meist tritt der Tod innerhalb von mehreren Stunden bis wenigen Tagen durch Nieren- und Herz-Kreislauf-Versagen ein. Eine chronische Arsenbelastung kann Krankheiten der Haut und Schäden an den Blutgefäßen hervorrufen, was zum Absterben der betroffenen Regionen (Black Foot Disease) sowie zu bösartigen Tumoren der Haut, Lunge, Leber und Harnblase führt. Diese Symptome wurden auch als Reichensteiner Krankheit bezeichnet, nach einem Ort in Schlesien, dessen Trinkwasser durch den Arsenik-Abbau bis zu 0,6 mg Arsen pro Liter enthielt.
Die chronische Arsen-Vergiftung führt über die Bindung an Sulfhydryl-Gruppen von Enzymen der Blutbildung (zum Beispiel Delta-Amino-Laevulin-Säure-Synthetase) zu einem initialen Abfall des Hämoglobins im Blut, was zu einer reaktiven Polyglobulie führt. Des Weiteren kommt es bei chronischer Einnahme von Arsen zur Substitution der Phosphor-Atome im Adenosin-Triphosphat (ATP) und damit zu einer Entkopplung der Atmungskette, was zu einer weiteren reaktiven Polyglobulie führt. Klinisch finden sich hier nach Jahren der As-Exposition Trommelschlägelfinger, Uhrglasnägel, Mees-Nagelbänder und Akrozyanose (Raynaud-Syndrom), mit Folge der "Black Foot Disease".
Metallisches Arsen dagegen zeigt wegen seiner Unlöslichkeit nur eine geringe Giftigkeit, da es vom Körper kaum aufgenommen wird (LD = 763 mg/kg Ratte, oral). Es sollte aber, da es sich an der Luft leicht mit seinen sehr giftigen Oxiden wie dem Arsenik überzieht, stets mit größter Vorsicht behandelt werden. Anders verhält es sich mit Arsenik, das in früheren Zeiten als Stimulans von Arsenikessern benutzt wurde, um einer Arsenvergiftung vorzubeugen. Der Mechanismus dieser Immunisierung gegen Arsen ist nicht bekannt.
Anionisches Arsen tritt als Arsenit ([AsO]) und Arsenat ([AsO]) in vielen Ländern im Grundwasser in hohen Konzentrationen auf. Durch Auswaschungen aus arsenhaltigen Erzen in Form von drei- und fünfwertigen Ionen trinken weltweit über 100 Millionen Menschen belastetes Wasser. Besonders in Indien, Bangladesh und Thailand, wo im 20. Jahrhundert mit internationaler Unterstützung zahlreiche Brunnen gegraben wurden, um von mit Krankheitserregern kontaminiertem Oberflächenwasser auf Grundwasser ausweichen zu können, führte diese unerkannte Belastung des Trinkwassers zu chronischer Arsenvergiftung bei weiten Teilen der betroffenen Bevölkerung. Das Problem kann, wo es bekannt wird, chemisch durch Oxidation der Arsenverbindungen und nachfolgende Ausfällung mit Eisenionen behoben werden. Von der Rice University wurde eine kostengünstige Filtermöglichkeit mit Nano-Magnetit entwickelt
Die Weltgesundheitsorganisation (WHO) empfiehlt seit 1992 einen Grenzwert für Arsen im Trinkwasser von 10 Mikrogramm pro Liter. Der Wert wird in vielen Staaten Europas und in den USA immer noch überschritten. In Deutschland wird er dagegen seit 1996 eingehalten. Eine Richtlinie der Europäischen Union (EU) von 1999 schreibt einen Höchstwert von 10 Mikrogramm pro Liter Trinkwasser EU-weit vor. Die USA verpflichteten sich im Jahre 2001, diesen Grenzwert ab 2006 einzuhalten.
Das im Grundwasser vorkommende Arsen reichert sich in Reis zehnmal so stark an wie in anderen Getreidearten; auf dem Weltmarkt angebotene Sorten enthalten zwischen 20 und 900 Mikrogramm Arsen pro Kilogramm. Im Jahr 2005 senkte die chinesische Regierung den zulässigen Gehalt anorganischer Arsenverbindungen von 700 auf 150 Mikrogramm pro Kilogramm Lebensmittel, im Juli 2014 beschloss die Codex Alimentarius-Kommission erstmals einen Höchstwert von 200 Mikrogramm für polierten Reis. Die für Lebensmittelsicherheit zuständige EU-Kommission diskutiert für Erzeugnisse aus Puffreis einen um 15 Prozent höheren Grenzwert und für spezielle Produkte für Kleinkinder einen nur halb so hohen (d. h. 100 Mikrogramm pro kg).
Für andere belastete Lebensmittel wie Bier oder Fruchtsäfte gibt es noch keine Grenzwerte, obwohl sie mehr Arsen enthalten können als für Trinkwasser zulässig ist. Verbraucherorganisationen fordern für Apfelsaft einen Grenzwert von 3, höchstens aber 4,4 ppb (entspricht Mikrogramm pro kg).
Fische und Meeresfrüchte weisen zwar hohe Gehalte an Arsen auf, jedoch nahezu ausschließlich in der als unbedenklich geltenden organisch gebundenen Form. Grenzwerte wie z. B. für Quecksilber oder Cadmium gibt es nicht.
Das neue Chemikaliengesetz der EU umgesetzt in der Gefahrstoffverordnung Deutschlands von 2005 verbietet im Anhang 4 die „gewerbliche“ (nicht private) Verarbeitung von arsenhaltigen Mitteln und Zubereitungen, die mehr als 0,3 Gewichtsprozent an Arsen aufweisen. Derartige Grenzwertregelungen sind gegeben da Arsen – genau wie Cadmium – in den Verzinkereien der Galvanikindustrie weltweit der Zinkschmelze zugesetzt wird, um die Haftungseigenschaften des Zinks an der Eisenoberfläche des zu verzinkenden Metallstückes zu verbessern. Auf Grund der Temperatur im Zink-Schmelzbad von 460 °C bis 480 °C kommt es zum Verdampfen von Arsen, Cadmium und anderen leicht flüchtigen Metallen und deren Anreicherung in der Luft des Arbeitsplatzes. So können jedoch zulässige Grenzwerte kurzfristig um das tausendfache überschritten werden, mit der Folge der aerogen-alveolaren Aufnahme in den Körper. Messungen ergaben, dass Arsen (und Cadmium) im hochreinen Zink (99,995 Reinheitsgrad, DIN-1179-Reinheitsgrad) mit weniger als 0,0004 Gewichts-% ausgewiesen waren und nach Zugabe von 450 Gramm dieses hochreinen Zinks in die Zinkschmelze zu einem Anstieg der Cd-/As-Konzentration von 3 bis 7 µg/m³ Luft auf über 3000 µg/m³ Luft führten. Für Arsen wurde diese Tatsache überraschend in einer Verzinkerei durch Messung der Arsen-Konzentration in Zinkschmelze, Blut und Urin festgestellt (unveröffentlicht). Bei Galvanik-Arbeitern wird die Urin-Arsen-Konzentration mit 25 bis 68 µg/l Urin gemessen, im Vergleich zu unbelasteter Bevölkerung mit 0,1 µg Arsen/l Urin.
Für die Entfernung von ionischem Arsen aus dem Trinkwasser gibt es Verfahren, die auf Adsorption an Aktivkohle, aktiviertem Aluminiumoxid oder Eisenhydroxid-Granulat beruhen. Daneben werden Ionenaustauscher verwendet. Es ist möglich, Arsen mittels gentechnisch veränderten Pflanzen aus dem Boden zu entfernen, die es in Blättern speichern. Zur Phytosanierung von Trinkwasser bietet sich die Dickstielige Wasserhyazinthe an, die Arsen insbesondere in ihr Wurzelgewebe einlagert und so eine Abreicherung des kontaminierten Wassers bewirkt. Organische Arsenverbindungen in belasteten Böden können enzymatisch mit Hilfe von Pilzen abgebaut werden.
In Bangladesh wird nach einem Verfahren der schweizerischen Forschungseinrichtung EAWAG versucht, Arsen mit Hilfe von transparenten PET-Flaschen und Zitronensaft abzureichern. Bei dieser SORAS "(Solar Oxidation and Removal of Arsenic)" genannten Methode oxidiert Sonnenlicht das Arsen; die Inhaltsstoffe des Zitronensafts helfen bei der Ausfällung. Mit dieser kostengünstigen Methode lässt sich der Arsengehalt um 75 bis 90 Prozent senken.
In Gewässern des Yellowstone-Nationalparks, die sich aus Geysiren und anderen Thermalquellen vulkanischen Ursprungs speisen, wurden eukaryontische Algen der Gattung "Cyanidioschyzon" gefunden, die die hohen Arsenkonzentrationen der Gewässer tolerieren und sie zu biologisch weniger verfügbaren organischen Verbindungen oxidieren können. An einer Nutzung zur Abreicherung in Trinkwasser wird gearbeitet.
Als Antidote bei akuten Arsenvergiftungen stehen die schwefelhaltigen Komplexbildner Dimercaptopropansulfonsäure (=DMPS), Unithiol und Succimer zur Verfügung. Sie sind noch bei starken Arsendosen effektiv, wenn die Vergiftung rechtzeitig diagnostiziert wird. Ihr Stellenwert bei der Behandlung chronischer Arsenvergiftungen ist hingegen umstritten. Aktivkohle ein bis mehrere Stunden nach der Einnahme kann das Metall ebenfalls binden und zur Ausscheidung bringen.
Indische Forscher haben im Tierversuch herausgefunden, dass die Einnahme von Knoblauch zur Senkung der Arsengehalte im Blut und der Erhöhung der Arsengehalte im Urin führen kann. Erklärt wird dies über eine Ausfällung des Arsen bei Reaktion mit schwefelhaltigen Substanzen wie etwa Allicin, das Bestandteil des Knoblauchs ist. Zur Prophylaxe werden zwei bis drei Knoblauchzehen täglich empfohlen.
Arsenverbindungen zeigen beim Verbrennen eine wenig charakteristische fahlblaue Flammenfärbung. Bei der Glühröhrchenprobe erhitzt man Arsenverbindungen, welche teilweise sublimieren und sich an kalten Oberflächen in Form von schwarzem Arsen, weißem Arsen(III)-oxid oder gelbem Arsentrisulfid wieder niederschlagen.
Bei der Flammen-AAS werden die Arsenverbindungen in einer reduzierenden Luft-Acetylen-Flamme ionisiert. Anschließend wird eine Atomabsorptionsmessung bei 189,0 nm beziehungsweise 193,8 nm durchgeführt. Nachweisgrenzen bis zu 1 µg/ml wurden beschrieben. Häufig wird das Arsen auch mit Hilfe von NaBH in das gasförmige Arsin (AsH) überführt (Hydridtechnik). In der Quarzrohrtechnik wird AsH zuerst bei rund 1000 °C in einem elektrisch beheizten Quarzröhrchen thermisch in seine atomaren Bestandteile zersetzt, um anschließend die Absorption bei o.g. Wellenlängen zu bestimmen. Die Nachweisgrenze bei dieser Technik liegt bei 0,01 µg/l. Eine weitere Methode ist die sog. Graphitrohrtechnik, bei der das Arsen einer festen Probe bei 1700 °C und höher verflüchtigt und anschließend die Extinktion bei 193,8 nm gemessen wird.
Die Kopplung von Hydridtechnik mit dem induktiv gekoppelten Plasma/ laserinduzierter Fluoreszenzmessung ist eine sehr nachweisstarke Methode zur Bestimmung von Arsen. Mittels Hydriderzeugung freigesetztes AsH wird dabei im Plasma atomisiert und mit einem Laser zur Emission angeregt. Mit dieser Methode wurden Nachweisgrenzen von 0,04 ng/mL erreicht.
Bei der Massenspektrometrie wird die Arsenspezies zunächst durch ein induktiv gekoppeltes Argonplasma (ICP-MS) thermisch ionisiert. Anschließend wird das Plasma in das Massenspektrometer geleitet. Eine Nachweisgrenze von 0,2 µg/l wurde für Arsenit beschrieben.
Weitverbreitet ist die photometrische Erfassung von As als Arsenomolybdänblau. As(V) reagiert zunächst mit (NH)MoO. Danach folgt eine Reduktion mit SnCl oder Hydrazin zu einem blauen Komplex. Die Photometrie erfolgt bei 730 nm und ist somit nahezu störungsfrei. Die Nachweisgrenzen können durch Verwendung von basischen Farbstoffen als Komplexbildner verbessert werden.
Eine sehr empfindliche Arsenbestimmung im ppt-Bereich ist mittels Neutronenaktivierungsanalyse möglich. Sie kommt insbesondere dann zur Anwendung, wenn die Probe eine komplexe Zusammensetzung aufweist oder schwierig aufzuschließen ist. Allerdings gibt diese Methode keinen Hinweis auf die chemische Verbindung, in der das Arsen vorliegt. Bei der Wechselwirkung von Neutronen mit der Probe, die das natürliche Isotop Arsen-75 enthält, wird das schwerere Isotop Arsen-76 gebildet, das jedoch instabil ist und sich unter einem β-Zerfall in Selen-76 umwandelt. Gemessen werden dabei die β-Strahlen, über die ein Rückschluss auf die Menge des Arsens möglich ist.
Bei Biosensoren wird die Biolumineszenz bei Kontakt von in Wasser gelöstem Arsen mit genetisch modifizierten Bakterien (z. B. "Escherichia coli" K12) und eines Lichtmessgeräts (Luminometer) detektiert. Die vorhandene Arsenkonzentration korreliert dabei direkt mit der emittierten Lichtmenge.
Chemische Verbindungen von Arsen und Wasserstoff (→ Arsane) sind im Vergleich zu den entsprechenden Verbindungen der Hauptgruppennachbarn Stickstoff und Phosphor nicht sehr zahlreich und sehr instabil. Es sind zurzeit drei Arsane bekannt.
Arsen bildet mit Halogenen binäre Verbindungen vom Typ AsX, AsX und AsX (X bezeichnet das entsprechende Halogen).
Die wichtigste Sauerstoffverbindung ist Arsen(III)-oxid (Arsenik oder Weißarsenik, AsO), das in der Gasphase in Form von Doppelmolekülen mit der Formel AsO vorliegt. Es ist amphoter und weist damit auf den Halbmetallcharakter des Arsens hin.
Wichtige Sauerstoffsäuren sind
Ein historisch wichtiges Färbe- und Pflanzenschutzmittel ist ein Kupfer-Arsen-Oxid mit dem Trivialnamen Schweinfurter Grün (Cu(AsO)·Cu(CHCOO)).
Es bestehen zwei wichtige Arsensulfide, die beide als Minerale in der Natur vorkommen.
Wichtige Verbindungen von Arsen mit Metallen sind
In Analogie zu den Aminen und Phosphinen findet man entsprechende Verbindungen mit Arsen anstelle von Stickstoff oder Phosphor. Sie werden als Arsine bezeichnet.
Zu den Arsoranen, Verbindungen vom Typ RAs, wobei R für fünf – möglicherweise unterschiedliche – organische Gruppen steht, zählt man etwa Pentaphenylarsen oder Pentamethylarsen. Fehlt eine der fünf Gruppen, bleibt ein einfach positiv geladenes Ion zurück (R steht wiederum für – möglicherweise verschiedene – organische Gruppen), das man als Arsoniumion (AsR) bezeichnet.
Analog zu den Carbonsäuren lassen sich zwei Klassen arseno-organischer Säuren bilden:
Zudem sind Heteroaromaten mit Arsen als Heteroatom bekannt, wie Arsabenzol, das aus einem Benzolring besteht, in dem ein Kohlenstoffatom durch Arsen ersetzt ist und das somit analog zu Pyridin aufgebaut ist.
Auch homocyclische Arsenverbindungen existieren. Beispiele sind
deren Moleküle einen Fünf- beziehungsweise Sechsring aus Arsenatomen als Rückgrat aufweisen, an den nach außen hin je eine Methylgruppe pro Arsenatom gebunden ist. Eine polycyclische Variante bildet das nebenstehende Molekül, dessen Rückgrat sich aus einem Sechs- und zwei angehefteten Fünfringen zusammensetzt (R steht für jeweils eine "tert"-Butylgruppe).
Schließlich lassen sich Arsenpolymere darstellen, lange Kettenmoleküle, die als Polyarsine bezeichnet werden. Sie bestehen aus einer zentralen „Strickleiter“ der Arsenatome, an die außen auf jeder Seite je „Sprosse“ eine Methylgruppe angeheftet ist, so dass sich die chemische Formel (AsCH) ergibt, wobei die natürliche Zahl "n" weit über 100 liegen kann. Polyarsine zeigen deutliche Halbleitereigenschaften.
In der Bioorganik spielen Arsenolipide, Arsenosaccharide und arsenhaltige Glycolipide eine bedeutende Rolle. Wichtige Vertreter dieser Stoffklassen sind zum Beispiel Arsenobetain, Arsenocholin und unterschiedlich substituierte Arsenoribosen. Sie treten vor allem kumuliert in maritimen Lebewesen auf und können auf diesem Weg in die menschliche Nahrungskette gelangen. Arsenhaltige Biomoleküle konnten in Algen, Meeresschwämmen und in Fischgewebe nach erfolgter Extraktion mittels HPLC-ICP-MS nachgewiesen werden. Die Analytik von Organo-Arsenverbindungen (einschließlich ihrer Speziation) ist sehr aufwändig.
Das Element Arsen erreichte zweifelhafte Berühmtheit als Mordgift, belegt durch geschichtliche Aufzeichnungen sowie die Instrumentalisierung in Literatur und Film. Es handelte sich bei dem Mordgift allerdings nie um elementares Arsen, sondern um dessen Verbindungen.
In Italien und Frankreich starben Herzöge, Könige und Päpste an vorsätzlich herbeigeführten Arsenvergiftungen. Im Frankreich des 17. Jahrhunderts steht die Marquise de Brinvilliers, die ihren Vater und zwei Brüder mit einer Arsenikmischung vergiftete, im Mittelpunkt eines Giftskandals. In Deutschland brachte die Serienmörderin Gesche Gottfried aus Bremen 15 Menschen zu Tode. Aufsehen erregte auch der Fall der Serienmörderin Anna Margaretha Zwanziger zu Beginn des 19. Jahrhunderts. Die Urheber der Morde blieben jedoch meist unerkannt, da Arsen bis 1836 in kleinen Mengen nicht nachgewiesen werden konnte. Erst die durch James Marsh entwickelte und nach ihm benannte Marshsche Probe machte es möglich, Spuren des Elementes zu identifizieren und somit eine unnatürliche Todesursache nachzuweisen. Im 19. und 20. Jahrhundert fanden weiter vorsätzliche Vergiftungen mit arsenhaltigen Mitteln statt – zum einen, weil sie leicht als Herbizide verfügbar waren, zum anderen ließ sich bei chronischer Gabe kleiner Dosen ein krankheitsbedingter Tod vortäuschen. Im September 1840 fiel im Prozess gegen Marie Lafarge das erste Urteil, das alleine auf den Ergebnissen der Marshschen Probe beruhte. Im Fall der Marie Besnard, die angeblich zwischen 1927 und 1949 für mehrere Todesfälle in ihrem Umfeld in Loudun verantwortlich sein sollte, konnte ein eindeutiger Beweis nicht erbracht werden, weil Untersuchungsergebnisse widersprüchlich waren, und sie musste 1954 letztendlich freigesprochen werden.
Jahrelang glaubte die Fachwelt, dass der Tod des ehemaligen französischen Kaisers Napoléon Bonaparte mit 51 Jahren auf der Insel St. Helena einem Giftanschlag mit Arsen zugeschrieben werden muss. Zumindest hatte man in seinen Haaren hochkonzentrierte Spuren des Giftes entdeckt. Heute existieren verschiedene andere Thesen zur Erklärung des Faktenbefunds. Eine Möglichkeit besteht darin, dass das Arsen "nach" seinem Tod den Haaren beigegeben wurde, um diese zu konservieren, eine damals durchaus übliche Methode. Möglich ist ein Übermaß der Benutzung der arsenhaltigen Fowlersche Lösung, die zu seiner Zeit bei vielen seiner Zeitgenossen als medizinisches Wundermittel galt. Die dritte und heute als wahrscheinlichste angesehene Möglichkeit ist, dass sich Napoleon durch organische Arsenverbindungen vergiftete, die Schimmelpilze beständig aus seinen mit grünen Arsenpigmenten gefertigten Tapeten freisetzten. Deren hoher Arsengehalt ist durch eine 1980 in einem Notizbuch aufgefundene Materialprobe schlüssig belegt.
Der berühmte Philosoph René Descartes starb 1650 wenige Monate nach seiner Ankunft am Hofe der schwedischen Königin Christine. Der Verdacht, er sei von einem der Jesuiten, die sich am Hofe der protestantischen Königin aufhielten, aus religionspolitischen Gründen mit Arsen vergiftet worden, verstärkte sich, als Christine später tatsächlich zum Katholizismus konvertierte, konnte aber nicht erhärtet werden, so dass die offizielle Todesursache, Lungenentzündung, sich in den Biographien etablierte. Erst kürzlich wurde anhand von neu aufgefundenen und neu interpretierten Dokumenten der alte Verdacht erhärtet und behauptet, dass der „Giftmord an Descartes in sehr hohem Maße wahrscheinlich, um nicht zu sagen, fast sicher“ erscheint.
Im Jahre 1900 kam es im britischen Manchester zu einer Massenvergiftung, von der mehrere Tausend Menschen betroffen waren. Wie sich herausstellte, hatten alle Bier derselben Brauerei getrunken. In Vorstufen der Bierproduktion wurde anscheinend Schwefelsäure eingesetzt, die ihrerseits aus Schwefel hergestellt wurde, der aus mit Arsenopyrit kontaminierten Sulfidmineralen stammte. Etwa 70 Menschen erlagen ihren Vergiftungen.
In den Jahren 2010 und 2011 starben in Österreich zwei Männer an einer Arsenvergiftung. Am 11. April 2013 wurde am Landesgericht Krems eine 52-jährige Polin des Mordes an den beiden für schuldig befunden und von dem Geschworenengericht nicht rechtskräftig zu lebenslanger Haft verurteilt.
Noch in den 1950er Jahren auf dem Höhepunkt des Kalten Krieges erkrankte die US-amerikanische Botschafterin, Clare Booth Luce, in Rom durch eine Vergiftung mit dem aus Tapeten freigesetzten Arsen. Die Tatsache, dass die Krankheit auf die schimmelpilzbefallenen Tapeten und nicht auf gegnerische Geheimagenten zurückgeführt werden konnte, trug in diesem Fall nicht nur zur Genesung der Botschafterin, sondern auch zum Erhalt des Friedens bei.
In Friedrich Schillers bürgerlichem Trauerspiel „Kabale und Liebe“ vergiftet der junge Major Ferdinand von Walter erst seine Geliebte Luise Millerin und dann sich selbst. Allerdings tritt in „Kabale und Liebe“ der Tod unrealistischerweise binnen Minuten ein.
Die Protagonistin des berühmten Romans "Madame Bovary" von Gustave Flaubert, die unglücklich verheiratete Landarztgattin Emma Bovary, stirbt am Ende des Romans durch Suizid mit Arsen in Form eines weißen Pulvers. Der Spross einer Arztfamilie Flaubert beschreibt die Vergiftungssymptome und den äußerst qualvollen Tod der Bovary sehr detailliert.
Die schriftstellerische Tätigkeit der britischen Kriminalautorin Agatha Christie geht auf das Arsen zurück. Während des Ersten Weltkrieges half sie in einer Apotheke eines örtlichen Hospitals aus. Eines Tages verschwand eine beträchtliche Menge Arsen aus dem verschlossenen Giftschrank und tauchte nicht wieder auf. Dies inspirierte sie zu ihrem ersten Kriminalroman "The Mysterious Affair at Styles" („Das fehlende Glied in der Kette“), der drei Jahre später veröffentlicht wurde und ihren Weltruhm begründete.
Im Roman „Starkes Gift“ („Strong Poison“) von Dorothy L. Sayers ist das Opfer mit Arsen vergiftet worden. Die Verdächtige, Krimi-Schriftstellerin Harriet Vane, hat sich zur fraglichen Zeit intensiv mit Arsenmorden beschäftigt und sich dazu sogar vom Apotheker beraten lassen.
Der berühmte Detektiv „Kalle Blomquist“ aus dem gleichnamigen Kinderbuch von Astrid Lindgren wendete die Marshsche Probe an, um ein mit Arsen vergiftetes Stück Schokolade zu überprüfen.
In dem Theaterstück von Joseph Kesselring "Arsen und Spitzenhäubchen" (englisch: "Arsenic and Old Lace)" vergiften zwei alte Damen in gutmeinender Absicht ältere einsame Herren mit einer Arsen-, Strychnin- und Zyankali-Mischung. Bekannt wurde das Stück durch die gleichnamige Verfilmung von Frank Capra mit Cary Grant, Peter Lorre und Priscilla Lane in den Hauptrollen.

</doc>
<doc id="98" url="https://de.wikipedia.org/wiki?curid=98" title="Astat">
Astat

Astat [] (von : „unbeständig, unstet“) ist ein radioaktives chemisches Element mit dem Elementsymbol At und der Ordnungszahl 85. Im Periodensystem steht es in der 7. Hauptgruppe, bzw. der 17. IUPAC-Gruppe und zählt damit zu den Halogenen. Astat entsteht beim natürlichen Zerfall von Uran. Astat ist eines der seltensten natürlich vorkommenden Elemente der Erde, das bei Bedarf künstlich erzeugt werden muss.
Als Dmitri Mendelejew 1869 sein Periodensystem festlegte, sagte er die Existenz einiger zu dieser Zeit noch nicht entdeckter Elemente voraus, darunter eines, das den Platz unter Iod einnehmen würde. In der Folge versuchten einige Wissenschaftler dieses Element, das als „Eka-Iod“ bezeichnet wurde, zu finden.
Im Jahre 1931 behauptete Fred Allison, er und seine Mitarbeiter am Alabama Polytechnic Institute (heute Auburn University) hätten das fehlende Element entdeckt und gaben ihm die Bezeichnung "Alabamine" (Ab). Ihre Entdeckung konnte jedoch nicht bestätigt werden und wurde später als falsch erkannt.
Ebenfalls auf der Suche nach einem Mitglied der Familie des radioaktiven Thoriums fand der Chemiker De Rajendralal Mitra im Jahre 1937 in Dhaka, Bangladesch (damals Britisch-Indien), zwei neue Elemente. Das erste nannte er "Dakin" (Eka-Iod), wohl nach der englischen Bezeichnung für Dhaka (Dacca), das andere "Gourium". Beide Entdeckungen konnten jedoch nicht bestätigt werden.
Der Name "Helvetium" wurde wiederum von dem Schweizer Chemiker Walter Minder vorgeschlagen, als er die Entdeckung des Elements 85 im Jahr 1940 ankündigte. Er änderte im Jahr 1942 jedoch seinen Vorschlag in "Anglohelvetium".
Bestätigt werden konnte die Entdeckung des Astat (altgriechisch ἀστατέω = „unbeständig sein“, aufgrund des radioaktiven Zerfalls) erstmals im Jahre 1940 durch die Wissenschaftler Dale Corson, Kenneth MacKenzie und Emilio Gino Segrè, die es in der University of California künstlich durch Beschuss von Bismut mit Alphateilchen herstellten.
Drei Jahre später konnte das kurzlebige Element von Berta Karlik und Traude Bernert auch als Produkt des natürlichen Zerfallsprozesses von Uran gefunden werden.
Astat wird durch Beschuss von Bismut mit Alphateilchen im Energiebereich von 26 bis 29 MeV hergestellt. Man erhält dabei die relativ langlebigen Isotope At bis At, die dann im Stickstoffstrom bei 450 bis 600 °C sublimiert und an einer gekühlten Platinscheibe abgetrennt werden.
Bei diesem radioaktiven Element wurde mit Hilfe von Massenspektrometrie nachgewiesen, dass es sich chemisch wie die anderen Halogene, besonders wie Iod verhält (es sammelt sich wie dieses in der Schilddrüse an). Astat ist stärker metallisch als Iod.
Forscher am Brookhaven National Laboratory haben Experimente zur Identifikation und Messung von elementaren chemischen Reaktionen durchgeführt, die Astat beinhalten. 
Mit dem On-line-Isotopen-Massenseparator (ISOLDE) am CERN wurde 2013 das Ionisationspotenzial von Astat mit 9,31751(8) Elektronenvolt bestimmt.
Astat hat etwa 20 bekannte Isotope, die alle radioaktiv sind; das langlebigste ist At mit einer Halbwertszeit von 8,3 Stunden.
Organische Astatverbindungen dienen in der Nuklearmedizin zur Bestrahlung bösartiger Tumore. Astat-Isotope eignen sich aufgrund der kurzen Halbwertszeiten innerlich eingenommen als radioaktive Präparate zum Markieren der Schilddrüse. Das Element wird in der Schilddrüse angereichert und in der Leber gespeichert.
Die chemischen Eigenschaften von Astat konnten aufgrund der geringen Mengen bisher nur mit Tracerexperimenten festgestellt werden. Sie ähneln stark denjenigen des Iods, wobei es aber ein schwächeres Oxidationsmittel ist. Bisher konnten diverse Astatide, Interhalogenverbindungen und organische Verbindungen nachgewiesen werden. Auch die Anionen der entsprechenden Sauerstoffsäuren sind bekannt. Wegen des im Vergleich zu anderen Halogenen elektropositiveren Charakters wird es von Silber nur unvollständig ausgefällt. Dafür existiert das komplexstabilisierte Kation At(Py) (Py=Pyridin), wodurch Astat auch kathodisch abgeschieden werden kann. Nachgewiesen wurde auch das Hydrid, Astatwasserstoff HAt. Es spielt in der technischen Chemie jedoch keine Rolle.
Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.

</doc>
<doc id="99" url="https://de.wikipedia.org/wiki?curid=99" title="Alkalimetalle">
Alkalimetalle

Als Alkalimetalle werden die chemischen Elemente Lithium, Natrium, Kalium, Rubidium, Caesium und Francium aus der 1. Hauptgruppe des Periodensystems bezeichnet. Sie sind silbrig glänzende, reaktive Metalle, die in ihrer Valenzschale ein einzelnes Elektron besitzen. Obwohl Wasserstoff in den meisten Darstellungen des Periodensystems in der ersten Hauptgruppe steht und zum Teil ähnliche chemische Eigenschaften wie die Alkalimetalle aufweist, kann er nicht zu diesen gezählt werden, da er unter Standardbedingungen weder fest ist noch metallische Eigenschaften aufweist.
Der Name der Alkalimetalle leitet sich von dem arabischen Wort für „Pottasche“ ab, die alte Bezeichnung für aus Pflanzenaschen gewonnenes Kaliumcarbonat. Humphry Davy stellte im Jahre 1807 erstmals das Element Kalium durch eine Schmelzflusselektrolyse aus Kaliumhydroxid dar. Letzteres gewann er aus Kaliumcarbonat. In einigen Sprachen spiegelt sich dies im Namen wider. So heißt Kalium beispielsweise im Englischen und Französischen "potassium" und im Italienischen "potassio".
Alkalimetalle sind metallisch glänzende, silbrig-weiße (Ausnahme: Caesium hat bei geringster Verunreinigung einen Goldton), weiche Leichtmetalle. Sie sind mit dem Messer schneidbar. Alkalimetalle haben eine geringe Dichte. Sie reagieren mit vielen Stoffen, so beispielsweise mit Wasser, Luft oder Halogenen teilweise äußerst heftig unter starker Wärmeentwicklung. Insbesondere die schwereren Alkalimetalle können sich an der Luft selbst entzünden. Daher werden sie unter Schutzflüssigkeiten, wie Paraffin oder Petroleum (Lithium, Natrium und Kalium), bzw. unter Luftabschluss in Ampullen (Rubidium und Caesium) aufbewahrt.
Als Elemente der ersten Gruppe des Periodensystems besitzen sie nur ein schwach gebundenes s-Elektron, das sie leicht abgeben. Ihre ersten Ionisierungsenergien und ihre Elektronegativitäten sind entsprechend klein. In Verbindungen kommen sie alle fast ausschließlich als einwertige Kationen vor, wenngleich sogar Verbindungen bekannt sind, in denen diese Metalle anionisch vorliegen (z. B. Natride, komplexiert mit sogenannten Kryptanden).
Alkalimetalle und ihre Salze besitzen eine spezifische Flammenfärbung:
Aufgrund dieser Flammenfärbung werden Alkalimetallverbindungen für Feuerwerke benutzt.
In der Atomphysik werden Alkalimetalle eingesetzt, da sie sich aufgrund ihrer besonders einfachen elektronischen Struktur besonders einfach mit Lasern kühlen lassen.
Alle Alkalimetalle kristallisieren in der kubisch-raumzentrierten Struktur. Lediglich Lithium und Natrium kristallisieren in der hexagonal-dichtesten Packung, wenn tiefe Temperaturen vorherrschen.
Der Radius der Elementatome sowie der Kationen nimmt mit steigender Massenzahl zu. Auch viele andere Eigenschaften der Alkalimetalle zeigen einen Trend innerhalb der Gruppe von oben nach unten:
Die Alkalimetalle reagieren mit Wasserstoff unter Bildung salzartiger "Alkalimetallhydride":
Die thermische Beständigkeit der Hydride nimmt vom Lithiumhydrid (LiH) zum Caesiumhydrid (CsH) ab. Alkalihydride werden u. a. als Reduktions- oder Trockenmittel eingesetzt.
Mit Sauerstoff reagieren die Metalle unter Bildung fester, weißer "Alkalimetalloxide" (Lithiumoxid), "Alkalimetallperoxide" (Natriumperoxid) und "Alkalimetallhyperoxide" (Kaliumhyperoxid, Rubidiumhyperoxid, Caesiumhyperoxid):
Die Reaktion mit Wasser zu "Alkalimetallhydroxiden" erfolgt unter Freisetzung von Wasserstoff:
Vom Lithium zum Caesium steigt die Reaktivität stark an; ab dem Kalium erfolgt Selbstentzündung. Hochgeschwindigkeitsaufnahmen der Reaktion von Alkalimetallen mit Wasser legen eine Coulomb-Explosion nahe. Die Alkalimetallhydroxide sind farblose Feststoffe, die sich in Wasser unter starker Erwärmung leicht lösen und dabei stark basisch reagieren. Die Hydroxide und ihre Lösungen wirken stark ätzend.
Mit Halogenen reagieren die Alkalimetalle zu den salzartigen "Alkalimetallhalogeniden":
Die Reaktivität steigt vom Lithium zum Caesium und sinkt vom Fluor zum Iod. So reagiert Natrium mit Iod kaum und mit Brom sehr langsam, während die Reaktion von Kalium mit Brom und Iod explosionsartig erfolgt.
Alkalimetalle können Halogenkohlenwasserstoffen unter Explosionserscheinungen das Halogen unter Bildung von Kohlenstoff und dem entsprechenden Alkalimetallhalogenid entziehen:
Alkalimetalle ergeben mit flüssigem Ammoniak intensiv blau gefärbte Lösungen. Diese Lösungen, die aus positiven Alkalimetall-Ionen und solvatisierten Elektronen besteht, sind ein sehr starkes Reduktionsmittel und werden beispielsweise für die Birch-Reduktion eingesetzt. Wird diesen Lösungen ein geeigneter Komplexbildner zugesetzt, können sich entsprechende Salze mit Alkalimetall-Anionen, die sogenannten Alkalide, bilden.
Wasserstoff, das erste Element der 1. Hauptgruppe, ist unter Normalbedingungen ein Nichtmetall. Er wird deshalb nicht zu den Alkalimetallen gezählt, teilt jedoch mit ihnen einige Eigenschaften. Wasserstoff tritt wie die Alkalimetalle stets einwertig auf und wandelt sich unter extrem hohem Druck in eine metallische Hochdruckmodifikation um, den metallischen Wasserstoff. Umgekehrt haben auch einige Alkalimetalle unter bestimmten Bedingungen Eigenschaften wie Wasserstoff, z. B. besteht Lithium als Gas zu 1 % aus zweiatomigen Molekülen.

</doc>
<doc id="100" url="https://de.wikipedia.org/wiki?curid=100" title="Actinoide">
Actinoide

Actinoide [] („Actiniumähnliche“; griech.: Endung "-οειδής" ("-oeides") „ähnlich“) ist eine Gruppenbezeichnung bestimmter ähnlicher Elemente. Zugerechnet werden ihr das Actinium und die 14 im Periodensystem folgenden Elemente: Thorium, Protactinium, Uran und die Transurane Neptunium, Plutonium, Americium, Curium, Berkelium, Californium, Einsteinium, Fermium, Mendelevium, Nobelium und Lawrencium. Im Sinne des Begriffs gehört Actinium nicht zu den Actiniumähnlichen, jedoch folgt die Nomenklatur der IUPAC hier dem praktischen Gebrauch. Die frühere Bezeichnung Actinide entspricht nicht dem Vorschlag der Nomenklaturkommission, da nach diesem die Endung „-id“ für binäre Verbindungen wie z. B. Chloride reserviert ist; die Bezeichnung ist aber weiterhin erlaubt. Alle Actinoide sind Metalle und werden auch als "Elemente der Actiniumreihe" bezeichnet.
Alle Actinoide sind radioaktiv. Einige sind in feinverteiltem Zustand pyrophor. Die Actinoide gehören wie die Lanthanoide zu den "inneren Übergangselementen" oder "f-Block-Elementen", da in diesen Reihen die f-Unterschalen mit Elektronen aufgefüllt werden.
Alle Actinoide bilden dreifach geladene Ionen, sie werden wie das Actinium als Untergruppe der 3. Nebengruppe aufgefasst. Die „leichteren“ Actinoide (Thorium bis Americium) kommen in einer größeren Anzahl von Oxidationszahlen vor als die entsprechenden Lanthanoide.
Die vierwertigen Oxide der Actinoide kristallisieren im kubischen Kristallsystem; der Strukturtyp ist der CaF-Typ (Fluorit) mit der und den Koordinationszahlen "An"[8], O[4].
Die dreiwertigen Chloride der Actinoide kristallisieren im hexagonalen Kristallsystem. Die Struktur des Uran(III)-chlorids ist die Leitstruktur für eine Reihe weiterer Verbindungen. In dieser werden die Metallatome von je neun Chloratomen umgeben. Als Koordinationspolyeder ergibt sich dabei ein dreifach überkapptes, trigonales Prisma, wie es auch bei den späteren Actinoiden und den Lanthanoiden häufig anzutreffen ist. Es kristallisiert im hexagonalen Kristallsystem in der und zwei Formeleinheiten pro Elementarzelle.

</doc>
<doc id="101" url="https://de.wikipedia.org/wiki?curid=101" title="Americium">
Americium

Americium ist ein chemisches Element mit dem Elementsymbol Am und der Ordnungszahl 95. Im Periodensystem steht es in der Gruppe der Actinoide (7. Periode, f-Block) und zählt auch zu den Transuranen. Americium ist neben Europium das einzige nach einem Erdteil benannte Element. Es ist ein leicht verformbares radioaktives Metall silbrig-weißen Aussehens.
Von Americium gibt es kein stabiles Isotop. Auf der Erde kommt es ausschließlich in künstlich erzeugter Form vor. Das Element wurde erstmals im Spätherbst 1944 erzeugt, die Entdeckung jedoch zunächst nicht veröffentlicht. Kurioserweise wurde dessen Existenz in einer amerikanischen Radiosendung für Kinder durch den Entdecker Glenn T. Seaborg, den Gast der Sendung, der Öffentlichkeit preisgegeben.
Americium wird in Kernreaktoren gebildet, eine Tonne abgebrannten Kernbrennstoffs enthält durchschnittlich etwa 100 g des Elements. Es wird als Quelle ionisierender Strahlung eingesetzt, z. B. in der Fluoreszenzspektroskopie und in Ionisationsrauchmeldern. Das Americiumisotop Am wurde wegen seiner gegenüber Plutonium (Pu) wesentlich längeren Halbwertszeit von 432,2 Jahren zur Befüllung von Radionuklidbatterien (RTG) für Raumsonden vorgeschlagen, welche dann hunderte Jahre lang elektrische Energie zum Betrieb bereitstellen würden.
Americium wurde im Spätherbst 1944 von Glenn T. Seaborg, Ralph A. James, Leon O. Morgan und Albert Ghiorso im 60-Zoll-Cyclotron an der Universität von Kalifornien in Berkeley sowie am metallurgischen Laboratorium der Universität von Chicago (heute: Argonne National Laboratory) erzeugt. Nach Neptunium und Plutonium war Americium das vierte Transuran, das seit dem Jahr 1940 entdeckt wurde; das um eine Ordnungszahl höhere Curium wurde als drittes schon im Sommer 1944 erzeugt. Der Name für das Element wurde in Anlehnung zum Erdteil Amerika gewählt – in Analogie zu Europium, dem Seltene-Erden-Metall, das im Periodensystem genau über Americium steht: "The name americium (after the Americas) and the symbol Am are suggested for the element on the basis of its position as the sixth member of the actinide rare-earth series, analogous to europium, Eu, of the lanthanide series."
Zur Erzeugung des neuen Elements wurden in der Regel die Oxide der Ausgangselemente verwendet. Dazu wurde zunächst Plutoniumnitratlösung (mit dem Isotop Pu) auf eine Platinfolie von etwa 0,5 cm aufgetragen; die Lösung danach eingedampft und der Rückstand dann zum Oxid (PuO) geglüht. Nach dem Beschuss im Cyclotron wurde die Beschichtung mittels Salpetersäure gelöst, anschließend wieder mit einer konzentrierten wässrigen Ammoniaklösung als Hydroxid ausgefällt; der Rückstand wurde in Perchlorsäure gelöst. Die weitere Trennung erfolgte mit Ionenaustauschern. In ihren Versuchsreihen wurden der Reihe nach vier verschiedene Isotope erzeugt: Am, Am, Am und Am.
Als erstes Isotop isolierten sie Am aus einer Plutonium-Probe, die mit Neutronen bestrahlt wurde. Es zerfällt durch Aussendung eines α-Teilchens in Np. Die Halbwertszeit dieses α-Zerfalls wurde zunächst auf 510 ± 20 Jahre bestimmt; der heute allgemein akzeptierte Wert ist 432,2 a.
Als zweites Isotop wurde Am durch erneuten Neutronenbeschuss des zuvor erzeugten Am gefunden. Durch nachfolgenden raschen β-Zerfall entsteht dabei Cm, das zuvor schon entdeckte Curium. Die Halbwertszeit dieses β-Zerfalls wurde zunächst auf 17 Stunden bestimmt, der heute als gültig ermittelte Wert beträgt 16,02 h.
Erstmals öffentlich bekannt gemacht wurde die Entdeckung des Elements in der amerikanischen Radiosendung "Quiz Kids" am 11. November 1945 durch Glenn T. Seaborg, noch vor der eigentlichen Bekanntmachung bei einem Symposium der American Chemical Society: Einer der jungen Zuhörer fragte den Gast der Sendung, Seaborg, ob während des Zweiten Weltkrieges im Zuge der Erforschung von Nuklearwaffen neue Elemente entdeckt wurden. Seaborg bejahte die Frage und enthüllte dabei auch gleichzeitig die Entdeckung des nächsthöheren Elements, Curium.
Americium (Am und Am) und seine Produktion wurde später unter dem Namen "„Element 95 and method of producing said element“" patentiert, wobei als Erfinder nur Glenn T. Seaborg angegeben wurde.
In elementarer Form wurde es erstmals im Jahr 1951 durch Reduktion von Americium(III)-fluorid mit Barium dargestellt.
Americiumisotope entstehen im r-Prozess in Supernovae und kommen auf der Erde wegen ihrer im Vergleich zum Alter der Erde zu geringen Halbwertszeit nicht natürlich vor.
Heutzutage wird jedoch Americium als Nebenprodukt in Kernkraftwerken erbrütet; das Americiumisotop Am entsteht als Zerfallsprodukt (u. a. in abgebrannten Brennstäben) aus dem Plutoniumisotop Pu. Eine Tonne abgebrannten Kernbrennstoffs enthält durchschnittlich etwa 100 g verschiedener Americiumisotope. Es handelt sich dabei hauptsächlich um die α-Strahler Am und Am, die aufgrund ihrer relativ langen Halbwertszeiten in der Endlagerung unerwünscht sind und deshalb zum Transuranabfall zählen. Eine Verminderung der Langzeitradiotoxizität in nuklearen Endlagern wäre durch Abtrennung langlebiger Isotope aus abgebrannten Kernbrennstoffen möglich. Zur Beseitigung des Americiums wird derzeit die Partitioning & Transmutation-Strategie untersucht.
Americium fällt in geringen Mengen in Kernreaktoren an. Es steht heute in Mengen von wenigen Kilogramm zur Verfügung. Durch die aufwändige Gewinnung aus abgebrannten Brennstäben hat es einen sehr hohen Preis. Seit der Markteinführung 1962 soll der Preis für Americium(IV)-oxid mit dem Isotop Am bei etwa 1500 US-Dollar pro Gramm liegen. Das Americiumisotop Am entsteht in geringeren Mengen im Reaktor aus Am und ist deshalb mit 160 US-Dollar pro Milligramm Am noch wesentlich teurer.
Americium wird über das Plutoniumisotop Pu in Kernreaktoren mit hohem U-Anteil zwangsläufig erbrütet, da es aus diesem durch Neutroneneinfang und zwei anschließende β-Zerfälle (über U und Np) entsteht.
Danach wird, wenn es nicht zur Kernspaltung kommt, aus dem Pu, neben anderen Nukliden, durch stufenweisen Neutroneneinfang (n,γ) und anschließenden β-Zerfall Am oder Am erbrütet.
Das Plutonium, welches aus abgebrannten Brennstäben von Leistungsreaktoren gewonnen werden kann, besteht zu etwa 12 % aus dem Isotop Pu. Deshalb erreichen erst 70 Jahre, nachdem der Brutprozess beendet wurde, die abgebrannten Brennstäbe ihren Maximalgehalt von Am; danach nimmt der Gehalt wieder (langsamer als der Anstieg) ab.
Aus dem so entstandenen Am kann durch weiteren Neutroneneinfang im Reaktor Am entstehen. Bei Leichtwasserreaktoren soll aus dem Am zu 79 % Am und zu 10 % Am entstehen:
zu 79 %: formula_5
zu 10 %: formula_6
Für die Erbrütung von Am ist ein vierfacher Neutroneneinfang des Pu erforderlich:
Metallisches Americium kann durch Reduktion aus seinen Verbindungen erhalten werden. Zuerst wurde Americium(III)-fluorid zur Reduktion verwendet. Dieses wird hierzu in wasser- und sauerstofffreier Umgebung in Reaktionsapparaturen aus Tantal und Wolfram mit elementarem Barium zur Reaktion gebracht.
Auch die Reduktion von Americium(IV)-oxid mittels Lanthan oder Thorium ergibt metallisches Americium.
Im Periodensystem steht das Americium mit der Ordnungszahl 95 in der Reihe der Actinoide, sein Vorgänger ist das Plutonium, das nachfolgende Element ist das Curium. Sein Analogon in der Reihe der Lanthanoiden ist das Europium.
Americium ist ein künstliches, radioaktives Element. Frisch hergestelltes Americium ist ein silberweißes Metall, welches jedoch bei Raumtemperatur langsam matt wird. Es ist leicht verformbar. Sein Schmelzpunkt beträgt 1176 °C, der Siedepunkt liegt bei 2607 °C. Die Dichte beträgt 13,67 g·cm. Es tritt in zwei Modifikationen auf.
Die bei Standardbedingungen stabile Modifikation α-Am kristallisiert im hexagonalen Kristallsystem in der mit den Gitterparametern "a" = 346,8 pm und "c" = 1124 pm sowie vier Formeleinheiten pro Elementarzelle. Die Kristallstruktur besteht aus einer doppelt-hexagonal dichtesten Kugelpackung (d.h.c.p.) mit der Schichtfolge ABAC und ist damit isotyp zur Struktur von α-La.
Bei hohem Druck geht α-Am in β-Am über. Die β-Modifikation kristallisiert im kubischen Kristallsystem in der Raumgruppe  mit dem Gitterparameter "a" = 489 pm, was einem kubisch flächenzentrierten Gitter (f.c.c.) beziehungsweise einer kubisch dichtesten Kugelpackung mit der Stapelfolge ABC entspricht.
Die Lösungsenthalpie von Americium-Metall in Salzsäure bei Standardbedingungen beträgt −620,6 ± 1,3 kJ·mol. Ausgehend von diesem Wert erfolgte die erstmalige Berechnung der Standardbildungsenthalpie (Δ"H") von Am auf −621,2 ± 2,0 kJ·mol und des Standardpotentials Am / Am auf −2,08 ± 0,01 V.
Americium ist ein sehr reaktionsfähiges Element, das schon mit Luftsauerstoff reagiert und sich gut in Säuren löst. Gegenüber Alkalien ist es stabil.
Die stabilste Oxidationsstufe für Americium ist +3, die Am(III)-Verbindungen sind gegen Oxidation und Reduktion sehr stabil. Mit dem Americium liegt der erste Vertreter der Actinoiden vor, der in seinem Verhalten eher den Lanthanoiden ähnelt als den d-Block-Elementen.
Es ist auch in den Oxidationsstufen +2 sowie +4, +5, +6 und +7 zu finden. Je nach Oxidationszahl variiert die Farbe von Americium in wässriger Lösung ebenso wie in festen Verbindungen:Am (gelbrosa), Am (gelbrot), AmO (gelb), AmO (zitronengelb), AmO (dunkelgrün).
Im Gegensatz zum homologen Europium – Americium hat eine zu Europium analoge Elektronenkonfiguration – kann Am in wässriger Lösung nicht zu Am reduziert werden.
Verbindungen mit Americium ab Oxidationszahl +4 aufwärts sind starke Oxidationsmittel, vergleichbar dem Permanganat-Ion (MnO) in saurer Lösung.
Die in wässriger Lösung nicht beständigen Am-Ionen lassen sich nur noch mit starken Oxidationsmitteln aus Am(III) darstellen. In fester Form sind zwei Verbindungen des Americiums in der Oxidationsstufe +4 bekannt: Americium(IV)-oxid (AmO) und Americium(IV)-fluorid (AmF).
Der fünfwertige Oxidationszustand wurde beim Americium erstmals 1951 beobachtet. In wässriger Lösung liegen primär AmO-Ionen (sauer) oder AmO-Ionen (alkalisch) vor, die jedoch instabil sind und einer raschen Disproportionierung unterliegen:
Etwas beständiger als Am(IV) und Am(V) sind die Americium(VI)-Verbindungen. Sie lassen sich aus Am(III) durch Oxidation mit Ammoniumperoxodisulfat in verdünnter Salpetersäure herstellen. Der typische rosafarbene Ton verschwindet in Richtung zu einer starken Gelbfärbung. Zudem kann die Oxidation mit Silber(I)-oxid in Perchlorsäure quantitativ erreicht werden. In Natriumcarbonat- oder Natriumhydrogencarbonat-Lösungen ist eine Oxidation mit Ozon oder Natriumperoxodisulfat gleichfalls möglich.
Eine biologische Funktion des Americiums ist nicht bekannt. Vorgeschlagen wurde der Einsatz immobilisierter Bakterienzellen zur Entfernung von Americium und anderen Schwermetallen aus Fließgewässern. So können Enterobakterien der Gattung "Citrobacter" durch die Phosphataseaktivität in ihrer Zellwand bestimmte Americiumnuklide aus wässriger Lösung ausfällen und als Metall-Phosphat-Komplex binden. Ferner wurden die Faktoren untersucht, die die Biosorption und Bioakkumulation des Americiums durch Bakterien und Pilze beeinflussen.
Das Isotop Am hat mit rund 5700 barn den höchsten bisher (10/2008) gemessenen thermischen Spaltquerschnitt. Damit geht eine kleine kritische Masse einher, weswegen Am als Spaltmaterial vorgeschlagen wurde, um beispielsweise Raumschiffe mit Kernenergieantrieb anzutreiben.
Dieses Isotop eignet sich prinzipiell auch zum Bau von Kernwaffen. Die kritische Masse einer reinen Am-Kugel beträgt etwa 9–14 kg. Die Unsicherheiten der verfügbaren Wirkungsquerschnitte lassen derzeit keine genauere Aussage zu. Mit Reflektor beträgt die kritische Masse noch etwa 3–5 kg. In wässriger Lösung wird sie nochmals stark herabgesetzt. Auf diese Weise ließen sich sehr kompakte Sprengköpfe bauen. Nach öffentlichem Kenntnisstand wurden bisher keine Kernwaffen aus Am gebaut, was mit der geringen Verfügbarkeit und dem hohen Preis begründet werden kann.
Aus denselben Gründen wird Am auch nicht als Kernbrennstoff in Kernreaktoren eingesetzt, obwohl es dazu prinzipiell sowohl in thermischen als auch in schnellen Reaktoren geeignet wäre. Auch die beiden anderen häufiger verfügbaren Isotope, Am und Am können in einem schnellen Reaktor eine Kettenreaktion aufrechterhalten. Die kritischen Massen sind hier jedoch sehr hoch. Sie betragen unreflektiert 57,6–75,6 kg bei Am und 209 kg bei Am, so dass sich durch die Verwendung keine Vorteile gegenüber herkömmlichen Spaltstoffen ergeben.
Entsprechend ist Americium rechtlich nach dem deutschen Atomgesetz nicht den Kernbrennstoffen zugeordnet. Es existieren jedoch Vorschläge, sehr kompakte Reaktoren mit einem Americium-Inventar von lediglich knapp 20 g zu konstruieren, die in Krankenhäusern als Neutronenquelle für die Neutroneneinfangtherapie verwendet werden können.
Von Americium sind 16 Isotope und 11 Kernisomere mit Halbwertszeiten zwischen Bruchteilen von Mikrosekunden und 7370 Jahren bekannt. Es gibt zwei langlebige α-strahlende Isotope Am mit 432,2 und Am mit 7370 Jahren Halbwertszeit. Außerdem hat das Kernisomer Am mit 141 Jahren eine lange Halbwertszeit. Die restlichen Kernisomere und Isotope haben mit 0,64 µs bei Am bis 50,8 Stunden bei Am kurze Halbwertszeiten.
Am ist das am häufigsten erbrütete Americiumisotop und liegt auf der Neptunium-Reihe. Es zerfällt mit einer Halbwertszeit von 432,2 Jahren mit einem α-Zerfall zu Np. Am gibt nur mit einer Wahrscheinlichkeit von 0,35 % die gesamte Zerfallsenergie mit dem α-Teilchen ab, sondern emittiert meistens noch ein oder mehrere Gammaquanten.
Am ist kurzlebig und zerfällt mit einer Halbwertszeit von 16,02 h zu 82,7 % durch β-Zerfall zu Cm und zu 17,3 % durch Elektroneneinfang zu Pu. Das Cm zerfällt zu Pu und dieses weiter zu U, das auf der Uran-Radium-Reihe liegt. Das Pu zerfällt über die gleiche Zerfallskette wie Pu. Während jedoch Pu als Seitenarm beim U auf die Zerfallskette kommt, steht Pu noch vor dem U. Pu zerfällt durch α-Zerfall in U, den Beginn der natürlichen Uran-Radium-Reihe.
Am zerfällt mit einer Halbwertszeit von 141 Jahren zu 99,541 % durch Innere Konversion zu Am und zu 0,459 % durch α-Zerfall zu Np. Dieses zerfällt zu Pu und dann weiter zu U, das auf der Uran-Radium-Reihe liegt.
Am ist mit einer Halbwertszeit von 7370 Jahren das langlebigste Americiumisotop. Es geht zunächst durch α-Strahlung in Np über, das durch β-Zerfall weiter zu Pu zerfällt. Das Pu zerfällt durch α-Strahlung zu Uran U, dem offiziellen Anfang der Uran-Actinium-Reihe.
Die Americiumisotope mit ungerader Neutronenzahl, also gerader Massenzahl, sind gut durch thermische Neutronen spaltbar.
"→ Liste der Americiumisotope"
Für die Verwendung von Americium sind vor allem die beiden langlebigsten Isotope Am und Am von Interesse. In der Regel wird es in Form des Oxids (AmO) verwendet.
Die α-Strahlung des Am wird in Ionisationsrauchmeldern genutzt. Es wird gegenüber Ra bevorzugt, da es vergleichsweise wenig γ-Strahlung emittiert. Dafür muss aber die Aktivität gegenüber Radium ca. das Fünffache betragen. Die Zerfallsreihe von Am „endet“ für den Verwendungszeitraum quasi direkt nach dessen α-Zerfall bei Np, das eine Halbwertszeit von rund 2,144 Millionen Jahren besitzt.
Am wurde wegen seiner gegenüber Pu wesentlich längeren Halbwertszeit zur Befüllung von Radionuklidbatterien (RTG) von Raumsonden vorgeschlagen. Dank seiner Halbwertszeit von 432,2 Jahren könnte ein RTG mit Am-Füllung hunderte Jahre lang – anstatt nur einige Jahrzehnte (wie mit einer Pu-Füllung) – elektrische Energie zum Betrieb einer Raumsonde bereitstellen. Es soll voraussichtlich in den Radionuklidbatterien zum Einsatz kommen, deren Entwicklung die ESA erwägt und deren Entwicklung in den 2020er-Jahren abgeschlossen werden könnte.
Am als Oxid mit Beryllium verpresst stellt eine Neutronenquelle dar, die beispielsweise für radiochemische Untersuchungen eingesetzt wird. Hierzu wird der hohe Wirkungsquerschnitt des Berylliums für (α,n)-Kernreaktionen ausgenutzt, wobei das Americium als Produzent der α-Teilchen dient. Die entsprechenden Reaktionsgleichungen lauten:
Derartige Neutronenquellen kommen beispielsweise in der Neutronenradiographie und -tomographie zum Einsatz.
Neben dem häufig verwendeten Po als Ionisator zur Beseitigung von unerwünschter elektrostatischer Aufladung kam auch Am zum Einsatz. Dazu wurde z.B. die Quelle am Kopf einer Bürste montiert mit der man langsam über die zu behandelnden Oberflächen strich und dadurch eine Wiederverschmutzung durch elektrostatisch angezogene Staubpartikel vermeiden konnte.
Americium ist Ausgangsmaterial zur Erzeugung höherer Transurane und auch der Transactinoide. Aus Am entsteht zu 82,7 % Curium (Cm) und zu 17,3 % Plutonium (Pu). Im Kernreaktor wird zwangsläufig in geringen Mengen durch Neutroneneinfang aus Am das Am erbrütet, das durch β-Zerfall zum Curiumisotop Cm zerfällt.
In Teilchenbeschleunigern führt zum Beispiel der Beschuss von Am mit Kohlenstoffkernen (C) beziehungsweise Neonkernen (Ne) zu den Elementen Einsteinium Es beziehungsweise Dubnium Db.
Mit seiner intensiven Gammastrahlungs-Spektrallinie bei 60 keV eignet sich Am gut als Strahlenquelle für die Röntgen-Fluoreszenzspektroskopie. Dies wird auch zur Kalibrierung von Gammaspektrometern im niederenergetischen Bereich verwendet, da die benachbarten Linien vergleichsweise schwach sind und so ein einzeln stehender Peak entsteht. Zudem wird der Peak nur vernachlässigbar durch das Compton-Kontinuum höherenergetischer Linien gestört, da diese ebenfalls höchstens mit einer um mindestens drei Größenordnungen geringeren Intensität auftreten.
Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen, welche eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielt. Eine chemische Gefahr liegt überhaupt nur dann vor, wenn es sich um eine dafür relevante Stoffmenge handelt.
Da von Americium nur radioaktive Isotope existieren, darf es selbst sowie seine Verbindungen nur in geeigneten Laboratorien unter speziellen Vorkehrungen gehandhabt werden. Die meisten gängigen Americiumisotope sind α-Strahler, weshalb eine Inkorporation unbedingt vermieden werden muss. Das breite Spektrum der hieraus resultierenden meist ebenfalls radioaktiven Tochternuklide stellt ein weiteres Risiko dar, das bei der Wahl der Sicherheitsvorkehrungen berücksichtigt werden muss. Am gibt beim radioaktiven Zerfall große Mengen relativ weicher Gammastrahlung ab, die sich gut abschirmen lässt.
Nach Untersuchungen des Forschers Arnulf Seidel vom Institut für Strahlenbiologie des Kernforschungszentrums Karlsruhe erzeugt Americium (wie Plutonium), bei Aufnahme in den Körper, mehr Knochentumore als dieselbe Dosis Radium.
Die biologische Halbwertszeit von Am beträgt in den Knochen 50 Jahre und in der Leber 20 Jahre. In den Gonaden verbleibt es dagegen offensichtlich dauerhaft.
"→ Kategorie: "
Von Americium existieren Oxide der Oxidationsstufen +3 (AmO) und +4 (AmO).
Americium(III)-oxid (AmO) ist ein rotbrauner Feststoff und hat einen Schmelzpunkt von 2205 °C.
Americium(IV)-oxid (AmO) ist die wichtigste Verbindung dieses Elements. Nahezu alle Anwendungen dieses Elements basieren auf dieser Verbindung. Sie entsteht unter anderem implizit in Kernreaktoren beim Bestrahlen von Urandioxid (UO) bzw. Plutoniumdioxid (PuO) mit Neutronen. Es ist ein schwarzer Feststoff und kristallisiert – wie die anderen Actinoiden(IV)-oxide – im kubischen Kristallsystem in der Fluorit-Struktur.
Halogenide sind für die Oxidationsstufen +2, +3 und +4 bekannt. Die stabilste Stufe +3 ist für sämtliche Verbindungen von Fluor bis Iod bekannt und in wässriger Lösung stabil.
Americium(III)-fluorid (AmF) ist schwerlöslich und kann durch die Umsetzung einer wässrigen Americiumlösung mit Fluoridsalzen im schwach Sauren durch Fällung hergestellt werden:
Das tetravalente Americium(IV)-fluorid (AmF) ist durch die Umsetzung von Americium(III)-fluorid mit molekularem Fluor zugänglich:
In der wässrigen Phase wurde das vierwertige Americium auch beobachtet.
Americium(III)-chlorid (AmCl) bildet rosafarbene hexagonale Kristalle. Seine Kristallstruktur ist isotyp mit Uran(III)-chlorid. Der Schmelzpunkt der Verbindung liegt bei 715 °C. Das Hexahydrat (AmCl·6 HO) weist eine monokline Kristallstruktur auf.
Durch Reduktion mit Na-Amalgam aus Am(III)-Verbindungen sind Am(II)-Salze zugänglich: die schwarzen Halogenide AmCl, AmBr und AmI. Sie sind sehr sauerstoffempfindlich, und oxidieren in Wasser unter Freisetzung von Wasserstoff zu Am(III)-Verbindungen.
Von den Chalkogeniden sind bekannt: das Sulfid (AmS), zwei Selenide (AmSe und AmSe) und zwei Telluride (AmTe und AmTe).
Die Pentelide des Americiums (Am) des Typs AmX sind für die Elemente Phosphor, Arsen, Antimon und Bismut dargestellt worden. Sie kristallisieren im NaCl-Gitter.
Americiummonosilicid (AmSi) und Americium„disilicid“ (AmSi mit: 1,87 < x < 2,0) wurden durch Reduktion von Americium(III)-fluorid mit elementaren Silicium im Vakuum bei 1050 °C (AmSi) und 1150–1200 °C (AmSi) dargestellt. AmSi ist eine schwarze Masse, isomorph mit LaSi. AmSi ist eine hellsilbrige Verbindung mit einem tetragonalen Kristallgitter.
Boride der Zusammensetzungen AmB und AmB sind gleichfalls bekannt.
Analog zu Uranocen, einer Organometallverbindung in der Uran von zwei Cyclooctatetraen-Liganden komplexiert ist, wurden die entsprechenden Komplexe von Thorium, Protactinium, Neptunium, Plutonium und auch des Americiums, (η-CH)Am, dargestellt.

</doc>
<doc id="102" url="https://de.wikipedia.org/wiki?curid=102" title="Atom">
Atom

<onlyinclude>Atome (von "átomos"‚ unteilbar) sind die Bausteine, aus denen alle festen, flüssigen oder gasförmigen Stoffe bestehen. Alle Materialeigenschaften dieser Stoffe sowie ihr Verhalten in chemischen Reaktionen werden durch die Eigenschaften und die räumliche Anordnung der Atome, aus denen sie aufgebaut sind, festgelegt. Jedes Atom gehört zu einem bestimmten chemischen Element und bildet dessen kleinste Einheit. Zurzeit sind 118 Elemente bekannt, von denen etwa 90 auf der Erde natürlich vorkommen. Atome verschiedener Elemente unterscheiden sich in ihrer Größe und Masse und vor allem in ihrer Fähigkeit, mit anderen Atomen chemisch zu reagieren und sich zu Molekülen oder festen Körpern zu verbinden. Die Durchmesser von Atomen liegen im Bereich von 6 · 10 m (Helium) bis 5 · 10 m (Cäsium), ihre Massen in einem Bereich von 1,7 · 10 kg (Wasserstoff) bis knapp 5 · 10 kg (die derzeit schwersten synthetisch hergestellten Kerne).
Atome sind nicht unteilbar, wie zum Zeitpunkt der Namensgebung angenommen, sondern zeigen einen wohlbestimmten Aufbau aus noch kleineren Teilchen. Sie bestehen aus einem Atomkern und einer Atomhülle. Der Atomkern hat einen Durchmesser von etwa einem Zehn- bis Hunderttausendstel des gesamten Atomdurchmessers, enthält jedoch über 99,9 % der Atommasse. Er besteht aus positiv geladenen Protonen und einer Anzahl von etwa gleich schweren, elektrisch neutralen Neutronen. Diese Nukleonen sind durch die starke Wechselwirkung aneinander gebunden. Die Hülle besteht aus negativ geladenen Elektronen. Sie trägt mit weniger als 0,06 % zur Masse bei, bestimmt jedoch die Größe des Atoms. Der positive Kern und die negative Hülle sind durch elektrostatische Anziehung aneinander gebunden. In der elektrisch neutralen Grundform des Atoms ist die Anzahl der Elektronen in der Hülle gleich der Anzahl der Protonen im Kern. Diese Zahl legt den genauen Aufbau der Hülle und damit auch das chemische Verhalten des Atoms fest und wird deshalb als "chemische Ordnungszahl" bezeichnet. Alle Atome desselben Elements haben die gleiche chemische Ordnungszahl. Sind zusätzliche Elektronen vorhanden oder fehlen welche, ist das Atom negativ bzw. positiv geladen und wird als Ion bezeichnet.
Die Vorstellung vom atomaren Aufbau der Materie existierte bereits in der Antike, war jedoch bis in die Neuzeit umstritten. Der endgültige Nachweis konnte erst Anfang des 20. Jahrhunderts erbracht werden und gilt als eine der bedeutendsten Entdeckungen in Physik und Chemie.</onlyinclude> Einzelne Atome sind selbst mit den stärksten Lichtmikroskopen nicht zu erkennen. Eine direkte Beobachtung einzelner Atome ist erst seit Mitte des 20. Jahrhunderts mit Feldionenmikroskopen möglich, seit einigen Jahren auch mit Rastertunnelmikroskopen und hochauflösenden Elektronenmikroskopen. Die Atomphysik, die neben dem Aufbau der Atome auch die Vorgänge in ihrem Inneren und ihre Wechselwirkungen mit anderen Atomen erforscht, hat entscheidend zur Entwicklung der modernen Physik und insbesondere der Quantenmechanik beigetragen.
Die Vorstellung vom atomaren Aufbau der Materie existierte bereits in der Antike. Aufgrund ihrer extrem geringen Größe sind einzelne Atome selbst mit den stärksten Lichtmikroskopen nicht zu erkennen, noch Anfang des 20. Jahrhunderts war ihre Existenz umstritten. Der endgültige Nachweis gilt als eine der bedeutendsten Entdeckungen in Physik und Chemie. Einen entscheidenden Beitrag lieferte Albert Einstein 1905, indem er die bereits seit langem bekannte, im Mikroskop direkt sichtbare Brownsche Bewegung kleiner Körnchen durch zufällige Stöße von Atomen oder Molekülen in deren Umgebung erklärte. Erst seit wenigen Jahrzehnten erlauben Feldionenmikroskope und Rastertunnelmikroskope, seit einigen Jahren zudem auch Elektronenmikroskope, einzelne Atome direkt zu beobachten.
Das Konzept des Atomismus, nämlich dass Materie aus Grundeinheiten aufgebaut ist – „kleinsten Teilchen“, die nicht immer weiter in kleinere Stücke zerteilt werden können – existiert seit Jahrtausenden, genauso wie das Gegenkonzept, Materie sei ein beliebig teilbares Kontinuum. Doch diese Ideen beruhten zunächst ausschließlich auf philosophischen Überlegungen und nicht auf empirischer experimenteller Untersuchung. Dabei wurden den Atomen verschiedene Eigenschaften zugeschrieben, und zwar je nach Zeitalter, Kultur und philosophischer Schule sehr unterschiedliche. Experimentell arbeitende Naturwissenschaftler machten sich Ende des 18. Jahrhunderts die Hypothese vom Atom zu eigen, weil diese Hypothese im Rahmen eines Teilchenmodells der Materie eine elegante Erklärung für neue Entdeckungen in der Chemie bot. Doch wurde gleichzeitig die gegenteilige Vorstellung, Materie sei ein Kontinuum, von Philosophen und auch unter Naturwissenschaftlern noch bis ins 20. Jahrhundert hinein aufrechterhalten.
Eine frühe Erwähnung des Atomkonzepts in der Philosophie ist aus Indien bekannt. Die Nyaya- und Vaisheshika-Schulen entwickelten ausgearbeitete Theorien, wie sich Atome zu komplexeren Gebilden zusammenschlössen (erst in Paaren, dann je drei Paare). In der griechischen Philosophie ist die Atomvorstellung erstmals im 5. Jahrhundert v. Chr. bei Leukipp überliefert. Sein Schüler Demokrit systematisierte sie und führte den Begriff "átomos" ein, was etwa „das Unzerschneidbare“ bedeutet, also ein nicht weiter zerteilbares Objekt. Diese Bezeichnung wurde Ende des 18. Jahrhunderts für die damals hypothetischen kleinsten Einheiten der chemischen Elemente der beginnenden modernen Chemie übernommen, denn mit chemischen Methoden lassen sich Atome in der Tat nicht „zerschneiden“.
Im Rahmen der wissenschaftlichen Erforschung konnte die Existenz von Atomen bestätigt werden. Es wurden viele verschiedene Atommodelle entwickelt, um ihren Aufbau zu beschreiben. Insbesondere das Wasserstoffatom als das einfachste aller Atome war dabei wichtig. Einige der Modelle werden heute nicht mehr verwendet und sind nur von wissenschaftsgeschichtlichem Interesse. Andere gelten je nach Anwendungsbereich als Näherung noch heute. In der Regel wird das einfachste Modell genommen, welches im gegebenen Zusammenhang noch ausreicht, um die auftretenden Fragen zu klären.
Robert Boyle vertrat 1661 in seinem Werk "The Sceptical Chymist" die Meinung, die Materie sei aus diversen Kombinationen verschiedener "corpuscules" aufgebaut und nicht aus den vier Elementen der Alchemie: Wasser, Erde, Feuer, Luft. Damit bereitete er die Überwindung der Alchemie durch den Element- und Atombegriff der modernen Chemie vor.
Daniel Bernoulli zeigte 1740, dass der gleichmäßige Druck von Gasen auf die Behälterwände und insbesondere das Gesetz von Boyle und Mariotte sich durch zahllose Stöße kleinster Teilchen erklären lässt. Damit wurde seine Forschung zum Vorläufer der kinetischen Gastheorie und statistischen Mechanik.
Ab Ende des 18. Jahrhunderts wurde die Vorstellung von Atomen genutzt, um die wohlbestimmten Winkel an den Kanten und Ecken der Edelsteine auf die verschiedenen möglichen Schichtungen von harten Kugeln zurückzuführen.
Nachdem Antoine Lavoisier 1789 den heutigen Begriff des chemischen Elements geprägt und die ersten Elemente richtig identifiziert hatte, benutzte 1803 John Dalton das Atomkonzept, um zu erklären, wieso Elemente immer in Mengenverhältnissen kleiner ganzer Zahlen miteinander reagieren (Gesetz der multiplen Proportionen). Er nahm an, dass jedes Element aus gleichartigen Atomen besteht, die sich nach festen Regeln miteinander verbinden können und so Stoffe mit anderen Materialeigenschaften bilden. Außerdem ging er davon aus, dass alle Atome eines Elements die gleiche Masse hätten, und begründete den Begriff Atomgewicht.
Die Beobachtungen zum chemischen und physikalischen Verhalten von Gasen konnte Amedeo Avogadro 1811 dahingehend zusammenfassen, dass zwei näherungsweise ideale Gase bei gleichen Werten von Volumen, Druck und Temperatur des Gases immer aus gleich vielen identischen Teilchen („Molekülen“) bestehen. Die Moleküle bestehen bei elementaren Gasen wie Wasserstoff, Sauerstoff oder Stickstoff immer aus zwei Atomen des Elements (Avogadrosches Gesetz).
1866 konnte Johann Loschmidt die Größe der Luftmoleküle bestimmen, indem er mit der von James C. Maxwell aus der kinetischen Gastheorie gewonnenen Formel die von George Stokes gemessenen Werte für die innere Reibung in Luft auswertete. Damit konnte er das Gewicht eines Luftmoleküls bestimmen. Außerdem erhielt er die nach ihm benannte Loschmidtsche Zahl als Anzahl der Luftmoleküle pro Kubikzentimeter.
Infolge der Arbeiten von Avogadro und Stanislao Cannizzaro wurde angenommen, dass Atome nicht als einzelne Teilchen auftreten, sondern nur als Bestandteile von Molekülen aus mindestens zwei Atomen. Doch 1876 gelang August Kundt und Emil Warburg der erste Nachweis eines einatomigen Gases. Sie bestimmten den Adiabatenexponenten von Quecksilber-Dampf bei hoher Temperatur und erhielten einen Wert, wie er nach der kinetischen Gastheorie nur für Teilchen in Gestalt echter Massepunkte auftreten kann. Ab 1895 kamen entsprechende Beobachtungen an den neu entdeckten Edelgasen hinzu.
Nach Erscheinen seiner Dissertation über die Bestimmung von Moleküldimensionen schlug Albert Einstein im selben Jahr 1905 ein Experiment vor, um die Hypothese von der Existenz der Atome anhand der Zitterbewegung kleiner Partikel in Wasser quantitativ zu prüfen. Nach seiner Theorie müssten die Partikel aufgrund der Unregelmäßigkeit der Stöße durch die Wassermoleküle kleine, aber immerhin unter dem Mikroskop sichtbare Bewegungen ausführen. Es war Einstein dabei zunächst nicht bekannt, dass er damit die seit 1827 bekannte Brownsche Bewegung von Pollen quantitativ erklärt hatte, für deren Ursache schon 1863 Christian Wiener erstmals Molekularstöße angenommen hatte. Der französische Physiker Jean Perrin bestimmte auf der Grundlage von Einsteins Theorie die Masse und Größe von Molekülen experimentell und fand ähnliche Ergebnisse wie Loschmidt. Diese Arbeiten trugen entscheidend zur allgemeinen Anerkennung der bis dahin so genannten „Atomhypothese“ bei.
Joseph John Thomson entdeckte 1897, dass die Kathodenstrahlen aus Teilchen bestimmter Ladung und Masse bestehen; deren Masse ist kleiner als ein Tausendstel der Atommasse. Diese Teilchen wurden als "Elektronen" bezeichnet und erwiesen sich als ein Bestandteil aller Materie, was dem Konzept des Atoms als unzerteilbarer Einheit widersprach. Thomson glaubte, dass die Elektronen dem Atom seine Masse verliehen und dass sie im Atom in einem masselosen, positiv geladenen Medium verteilt seien wie „Rosinen in einem Kuchen“ (Thomsonsches Atommodell).
Die kurz zuvor entdeckte Radioaktivität wurde 1903 von Ernest Rutherford und Frederick Soddy mit Umwandlungen verschiedener Atomsorten ineinander in Verbindung gebracht. Sie konnten 1908 nachweisen, dass α-Teilchen, die bei Alphastrahlung ausgesandt werden, Helium-Atome bilden.
Zusammen mit seiner Forschergruppe beschoss Ernest Rutherford 1909 eine Goldfolie mit α-Teilchen. Er stellte fest, dass die meisten der Teilchen die Folie fast ungehindert durchdrangen, einige wenige aber um sehr viel größere Winkel abgelenkt wurden, als nach Thomsons Modell möglich. Rutherford schloss daraus, dass fast die ganze Masse des Atoms in einem sehr viel kleineren, geladenen Atomkern in der Mitte des Atoms konzentriert sei (Rutherfordsches Atommodell). Die stark abgelenkten α-Teilchen waren diejenigen, die einem Kern zufällig näher als etwa ein Hundertstel des Atomradius gekommen waren. Die Ladungszahl des Atomkerns entpuppte sich als die chemische Ordnungszahl des betreffenden Elements, und α-Teilchen erwiesen sich als die Atomkerne des Heliums.
Der Chemiker Frederick Soddy stellte 1911 fest, dass manche der natürlichen radioaktiven Elemente aus Atomen mit unterschiedlichen Massen und unterschiedlicher Radioaktivität bestehen mussten. Der Begriff Isotop für physikalisch verschiedene Atome desselben chemischen Elements wurde 1913 von Margaret Todd vorgeschlagen. Da die Isotope desselben Elements an ihrem chemischen Verhalten nicht zu unterscheiden waren, entwickelte der Physiker J.J. Thomson ein erstes Massenspektrometer zu ihrer physikalischen Trennung. Damit konnte er 1913 am Beispiel von Neon nachweisen, dass es auch stabile Elemente mit mehreren Isotopen gibt.
1918 fand Francis William Aston mit einem Massenspektrometer von erheblich größerer Genauigkeit heraus, dass fast alle Elemente Gemische aus mehreren Isotopen sind, wobei die Massen der einzelnen Isotope immer (nahezu) ganzzahlige Vielfache der Masse des Wasserstoffatoms sind. Rutherford wies 1919 in der ersten beobachteten Kernreaktion nach, dass durch Beschuss mit α-Teilchen aus den Kernen von Stickstoffatomen die Kerne von Wasserstoffatomen herausgeschossen werden können. Diesen gab er den Namen Proton und entwickelte ein Atommodell, in dem die Atome nur aus Protonen und Elektronen bestehen, wobei die Protonen und ein Teil der Elektronen den kleinen, schweren Atomkern bilden, die übrigen Elektronen die große, leichte Atomhülle. Die Vorstellung von Elektronen im Atomkern stellte sich jedoch als falsch heraus und wurde fallengelassen, nachdem 1932 von James Chadwick das Neutron als ein neutraler Kernbaustein mit etwa gleicher Masse wie das Proton nachgewiesen wurde. Damit entstand das heutige Atommodell: Der Atomkern ist zusammengesetzt aus so vielen Protonen wie die Ordnungszahl angibt, und zusätzlich so vielen Neutronen, dass die betreffende Isotopenmasse erreicht wird. 
1913 konnte Niels Bohr, aufbauend auf Rutherfords Atommodell aus Kern und Hülle, erstmals erklären, wie es in den optischen Spektren reiner Elemente zu den Spektrallinien kommt, die für das jeweilige Element absolut charakteristisch sind (Spektralanalyse nach Robert Wilhelm Bunsen und Gustav Robert Kirchhoff 1859). Bohr nahm an, dass die Elektronen sich nur auf bestimmten quantisierten Umlaufbahnen (Schalen) aufhalten und von einer zur anderen „springen“, sich jedoch nicht dazwischen aufhalten können. Beim Quantensprung von einer äußeren zu einer weiter innen liegenden Bahn muss das Elektron eine bestimmte Menge an Energie abgeben, die als Lichtquant bestimmter Wellenlänge erscheint. Im Franck-Hertz-Versuch konnte die quantisierte Energieaufnahme und -abgabe an Quecksilberatomen experimentell bestätigt werden. Das Bohrsche Atommodell ergab zwar nur für Systeme mit lediglich einem Elektron (Wasserstoff und ionisiertes Helium) quantitativ richtige Resultate. Jedoch bildete es im Laufe des folgenden Jahrzehnts das Fundament für eine Reihe von Verfeinerungen, die zu einem qualitativen Verständnis des Aufbaus der Elektronenhüllen aller Elemente führten. Damit wurde das Bohrsche Atommodell zur Grundlage des populären Bildes vom Atom als einem kleinen Planetensystem.
1916 versuchte Gilbert Newton Lewis, im Rahmen des Bohrschen Atommodells die chemische Bindung durch Wechselwirkung der Elektronen eines Atoms mit einem anderen Atom zu erklären. Walther Kossel ging 1916 erstmals von abgeschlossenen „Elektronenschalen“ bei den Edelgasen aus, um zu erklären, dass die chemischen Eigenschaften der Elemente grob periodisch mit der Ordnungszahl variieren, wobei sich benachbarte Elemente durch ein oder zwei zusätzliche oder fehlende Elektronen unterscheiden. Dies wurde bis 1921 von Niels Bohr zum „Aufbauprinzip“ weiterentwickelt, wonach mit zunehmender Kernladungszahl jedes weitere Elektron in die jeweils energetisch niedrigste Elektronenschale der Atomhülle, die noch Plätze frei hat, aufgenommen wird, ohne dass die schon vorhandenen Elektronen sich wesentlich umordnen.
Aufbauend auf dem von Louis de Broglie 1924 postulierten Welle-Teilchen-Dualismus entwickelte Erwin Schrödinger 1926 die Wellenmechanik. Sie beschreibt die Elektronen nicht als Massenpunkte auf bestimmten Bahnen, sondern als dreidimensionale "Materiewellen". Als Folge dieser Beschreibung ist es unter anderem unzulässig, einem Elektron gleichzeitig genaue Werte für Ort und Impuls zuzuschreiben. Dieser Sachverhalt wurde 1926 von Werner Heisenberg in der Unschärferelation formuliert. Demnach können statt der Bewegung auf bestimmten Bahnen nur "Wahrscheinlichkeitsverteilungen" für Wertebereiche von Ort und Impuls angegeben werden, eine Vorstellung, die nur schwer zu veranschaulichen ist. Den quantisierten Umlaufbahnen des Bohrschen Modells entsprechen hier stehende Materiewellen oder „Atomorbitale“. Sie geben unter anderem an, wie sich in der Nähe des Atomkerns die Aufenthaltswahrscheinlichkeit der Elektronen konzentriert, und bestimmen damit die wirkliche Größe des Atoms.
Die Beschreibung der Eigenschaften der Atome gelang mit diesem ersten vollständig quantenmechanischen Atommodell sehr viel besser als mit den Vorläufermodellen. Insbesondere ließen sich auch bei Atomen mit mehreren Elektronen die Spektrallinien und die Struktur der Atomhülle in räumlicher und energetischer Hinsicht darstellen, einschließlich der genauen Möglichkeiten, mit den Atomhüllen anderer Atome gebundene Zustände zu bilden, also stabile Moleküle. Daher wurde das Bohrsche Atommodell zugunsten des quantenmechanischen Orbitalmodells des Atoms verworfen.
Das Orbitalmodell ist bis heute Grundlage und Ausgangspunkt genauer quantenmechanischer Berechnungen fast aller Eigenschaften der Atome. Das gilt insbesondere für ihre Fähigkeit, sich mit anderen Atomen zu einzelnen Molekülen oder zu ausgedehnten Festkörpern zu verbinden. Bei Atomen mit mehreren Elektronen muss dafür außer dem Pauli-Prinzip auch die elektrostatische Wechselwirkung jedes Elektrons mit allen anderen berücksichtigt werden. Diese hängt u. a. von der Form der besetzten Orbitale ab. Andererseits wirkt sich umgekehrt die Wechselwirkung auf die Form und Energie der Orbitale aus. Es ergibt sich das Problem, die Orbitale in selbstkonsistenter Weise so zu bestimmen, dass sich ein stabiles System ergibt. Die Hartree-Fock-Methode geht von Orbitalen einer bestimmten Form aus und variiert diese systematisch, bis die Rechnung eine minimale Gesamtenergie ergibt. Wenn man die Orbitale nach der Dichtefunktionaltheorie bestimmen will, geht man von einer ortsabhängigen Gesamtdichte der Elektronen aus und bildet daraus eine Schrödingergleichung zur Bestimmung der Orbitale der einzelnen Elektronen. Hier wird die anfänglich angenommene Gesamtdichte variiert, bis sie mit der Gesamtdichte, die aus den besetzten Orbitalen zu berechnen ist, gut übereinstimmt.
Das Orbitalmodell bei einem Atom mit mehr als einem Elektron ist physikalisch als eine Näherung zu bezeichnen, nämlich als eine 1-Teilchen-Näherung. Sie besteht darin, dass jedem einzelnen Elektron ein bestimmtes Orbital zugeschrieben wird. Ein so gebildeter Zustand gehört zu der einfachsten Art von Mehrteilchenzuständen und wird hier als Konfiguration des Atoms bezeichnet. Genauere Modelle berücksichtigen, dass nach den Regeln der Quantenmechanik die Hülle auch in einem Zustand sein kann, der durch Superposition verschiedener Konfigurationen entsteht, wo also mit verschiedenen Wahrscheinlichkeitsamplituden gleichzeitig verschiedene Elektronenkonfigurationen vorliegen (eine sogenannte Konfigurationsmischung). Hiermit werden die genauesten Berechnungen von Energieniveaus und Wechselwirkungen der Atome möglich. Wegen des dazu nötigen mathematischen Aufwands werden jedoch, wo es möglich ist, auch weiterhin einfachere Atommodelle genutzt. Zu nennen ist hier das Thomas-Fermi-Modell, in dem die Elektronenhülle pauschal wie ein im Potentialtopf gebundenes ideales Elektronengas, das Fermigas, behandelt wird, dessen Dichte wiederum die Form des Potentialtopfs bestimmt.
Die Elektronen der Atomhülle sind aufgrund ihrer negativen Ladung durch elektrostatische Anziehung an den positiven Atomkern gebunden. Anschaulich bilden sie eine Elektronenwolke ohne scharfen Rand. Ein neutrales Atom enthält genauso viele Elektronen in der Hülle wie Protonen im Kern. Die Hülle hat einen etwa zehn- bis hunderttausend Mal größeren Durchmesser als der Kern, trägt jedoch weniger als 0,06 % zur Atommasse bei. Sie ist für energiereiche freie Teilchen (z. B. Photonen der Röntgenstrahlung oder Elektronen und Alphateilchen der radioaktiven Strahlung mit Energien ab einigen hundert Elektronenvolt (eV)) sehr durchlässig. Daher wird das Atom zuweilen als „weitgehend leer“ beschrieben.
Für geladene Teilchen geringer Energie im Bereich bis zu einigen zehn eV ist die Hülle aber praktisch undurchdringlich. In diesem Bereich liegen auch die kinetische Energie und die Bindungsenergie der Elektronen im äußeren Teil der Hülle. Daher erfahren zwei Atome immer eine starke Abstoßungskraft, wenn sie sich so weit annähern, dass sich ihre Hüllen merklich überschneiden würden. Der Bereich der kinetischen Energien ganzer Atome und Moleküle, wie sie unter normalen Bedingungen auf der Erde vorkommen, liegt noch deutlich darunter. Z. B. beträgt die thermische Energie formula_1 (formula_2 Boltzmannkonstante, formula_3 absolute Temperatur), die für die Größenordnung dieses Energiebereichs typisch ist, bei Raumtemperatur nur ungefähr 0,025 eV. Unter diesen Bedingungen ist die Atomhülle daher erstens stabil, weil ihr keine Elektronen entrissen werden, und zweitens undurchdringlich, weil sie sich nicht merklich mit den Hüllen anderer Atome überschneidet. Damit wird das Atom zum universellen Baustein der alltäglichen makroskopischen Materie. Seine, wenn auch nicht ganz scharf definierte, Größe verdankt es der gegenseitigen Undurchdringlichkeit der Hüllen.
Wenn sich die Hüllen zweier Atome aber nur geringfügig mit ihren äußeren Randbereichen überschneiden, kann zwischen ihnen eine anziehende Kraft entstehen. Sie ist die Ursache für die Entstehung von stabilen Molekülen, also den kleinsten Teilchen einer chemischen Verbindung. Bedingung ist, dass insgesamt ein Gewinn an Bindungsenergie damit einhergeht, dass ein oder zwei Elektronen von einer Hülle ganz oder mit gewisser Wahrscheinlichkeit zu der anderen Hülle überwechseln oder an beiden Hüllen beteiligt sind. Das ist nur bei genau passendem Aufbau beider Hüllen gegeben. Daher treten chemische Bindungen nur bei entsprechend geeigneten Kombinationen von Atomen auf.
Bei größeren Abständen, etwa bei einigen Atomdurchmessern, ziehen sich hingegen Atome aller Arten gegenseitig schwach an, unabhängig von der Möglichkeit, eine chemische Bindung einzugehen. Diese Van-der-Waals-Kräfte bewirken, dass jedes Gas bei genügend niedriger Temperatur zu einer Flüssigkeit oder einem Feststoff kondensiert. Sie sind also für den Wechsel der Aggregatzustände verantwortlich und wirken zwischen den neutralen Atomen bzw. Molekülen, sind aber auch elektrischen Ursprungs. Sie werden dadurch erklärt, dass sich zwei Atome durch leichte räumliche Verschiebung ihrer Elektronenwolken gegenseitig elektrische Dipolmomente induzieren, die einander elektrostatisch anziehen.
Der Chemiker Otto Hahn, ein Schüler Rutherfords, versuchte im Jahr 1938, durch Einfang von Neutronen an Urankernen Atome mit größerer Masse (Transurane) herzustellen, wie das bei leichteren Elementen seit Jahren gelungen war. Fritz Straßmann wies jedoch überraschenderweise nach, dass dabei das viel leichtere Barium entstanden war. Die Physiker Lise Meitner und Otto Frisch konnten den Vorgang als Kernspaltung identifizieren, indem sie mittels einer Ionisationskammer mehrere radioaktive Spaltprodukte nachwiesen.
Ab den 1950er Jahren konnten Atome durch die Entwicklung verbesserter Teilchenbeschleuniger und Teilchendetektoren beim Beschuss mit Teilchen sehr hoher Energie untersucht werden. Ende der 1960er Jahre zeigte sich in der „tiefinelastischen Streuung“ von Elektronen an Atomkernen, dass auch Neutronen und Protonen keine unteilbaren Einheiten sind, sondern aus Quarks zusammengesetzt sind.
1951 entwickelte Erwin Müller das Feldionenmikroskop und konnte damit von einer Nadelspitze erstmals ein Abbild erzeugen, das auf direkte Weise so stark vergrößert war, dass einzelne Atome darin sichtbar wurden (wenn auch nur als verschwommene Flecken). 1953 entwickelte Wolfgang Paul die magnetische Ionenfalle (Paulfalle), in der einzelne Ionen gespeichert und mit immer höherer Genauigkeit untersucht werden konnten.
1985 entwickelte eine Arbeitsgruppe um Steven Chu die Laserkühlung, ein Verfahren, die Temperatur einer Ansammlung von Atomen mittels Laser­strahlung stark zu verringern. Im selben Jahr gelang es einer Gruppe um William D. Phillips, neutrale Natriumatome in einer magneto-optischen Falle einzuschließen. Durch Kombination dieser Verfahren mit einer Methode, die den Dopplereffekt nutzt, gelang es einer Arbeitsgruppe um Claude Cohen-Tannoudji, geringe Mengen von Atomen auf Temperaturen von einigen Mikrokelvin zu kühlen. Mit diesem Verfahren können Atome mit höchster Genauigkeit untersucht werden; außerdem ermöglichte es auch die experimentelle Realisierung der Bose-Einstein-Kondensation.
Anfang der 1980er Jahre wurde von Gerd Binnig und Heinrich Rohrer das Rastertunnelmikroskop entwickelt, in dem eine Nadelspitze eine Oberfläche mittels des Tunneleffekts so fein abtastet, dass einzelne Atome sichtbar werden. Damit wurde es auch möglich, Atome einzeln an bestimmte Plätze zu setzen. In den 1990er Jahren konnten Serge Haroche und David Wineland in Experimenten die Wechselwirkung eines einzelnen Atoms mit einem einzelnen Photon erfolgreich untersuchen. In den 2000er Jahren wurde die Handhabbarkeit einzelner Atome unter anderem genutzt, um einen Transistor aus nur einem Metallatom mit organischen Liganden herzustellen.
Viele dieser Entdeckungen wurden mit dem Nobelpreis (Physik oder Chemie) ausgezeichnet.
Die Unterscheidung und Bezeichnung verschiedener Atomsorten geht zunächst vom Aufbau des Atomkerns aus, während der Zustand der Hülle gegebenenfalls durch zusätzliche Symbole angegeben wird. Kennzahlen sind die Protonenzahl (Ordnungszahl, Kernladungszahl) "Z", die Neutronenzahl "N" des Kerns, und die daraus gebildete Massenzahl "A=Z+N". Abhängig von der Protonenzahl gehören die Atome zu einem der 118 bekannten chemischen Elemente, von Wasserstoff mit "Z"=1 bis Oganesson mit "Z"=118. Davon sind 91 in natürlichen Vorkommen entdeckt worden, 27 nur nach künstlicher Herstellung durch Kernreaktionen. Die Ordnung der Elemente wird im Periodensystem – wichtig für die Chemie – graphisch veranschaulicht. Darin werden die Elemente mit aufsteigender Ordnungszahl in Form einer Tabelle angeordnet. Jede Zeile wird als Periode des Periodensystems bezeichnet und endet, wenn das Orbital mit Elektronen voll besetzt ist (Edelgas). In den nächsten Zeilen wiederholt sich aufgrund der schrittweisen Elektronenbesetzung der nächsten Orbitale der chemische Charakter der Elemente. So stehen Elemente mit ähnlichen chemischen Eigenschaften in einer Spalte untereinander, sie bilden eine Gruppe des Periodensystems.
Atome eines Elements, die sich in der Neutronenzahl unterscheiden, gehören zu verschiedenen Isotopen des Elements. Insgesamt bestehen die 118 Elemente aus etwa 2800 Isotopen, wovon 2500 künstlich erzeugt wurden. Isotope werden – bis auf die Ausnahmen der Wasserstoffisotope Deuterium und Tritium – nach dem chemischen Element und der Massenzahl bezeichnet. Das Symbol für ein bestimmtes Isotop des Elements X hat die Form formula_4, formula_5 oder X-"A" (Beispiele: formula_6, formula_7, Pb-208). Die explizite Angabe der Protonenzahl "Z" ist redundant, da sich "Z" schon aus der Ordnungszahl des Elements ergibt.
Der Begriff Nuklid wird häufig als Sammelbezeichnung für Isotope verschiedener Elemente gleichzeitig verwendet, aber auch, wenn von Atomen mit einem Kern in einem bestimmten angeregten Zustand die Rede ist. Die Nuklidkarte oder Isotopenkarte – wichtig für die Kernphysik und ihre Anwendungen – ist eine Tabelle, in der jedes Isotop einen eigenen Platz erhält. Dazu wird auf einer Achse die Anzahl der Protonen, auf der anderen die der Neutronen aufgetragen. Verschiedene Nuklide desselben Isotops teilen sich einen Platz. Häufig wird die Stabilität der Isotope und bei instabilen Isotopen auch die Art der Umwandlung oder die Größenordnung der Halbwertszeit durch bestimmte Farben dargestellt.
Der Atomkern eines Nuklids formula_8 kann im energetischen Grundzustand und in verschiedenen Anregungszuständen vorliegen. Wenn darunter relativ langlebige, sogenannte metastabile Zustände sind, werden diese als Isomere bezeichnet und als eigene Nuklide gezählt (Symbol formula_9, formula_10 o.ä.). Nach dieser Definition sind mit dem Stand von 2003 insgesamt etwa 3200 Nuklide bekannt.
In der Kernphysik werden Nuklide mit unterschiedlichen Protonenzahlen, aber gleicher Massenzahl formula_11 als Isobare bezeichnet. Seltener werden unter dem Namen Isotone Nuklide mit verschiedenen Protonenzahlen, aber gleicher Neutronenzahl zusammengefasst.
Nur etwa 250 Isotope von 80 Elementen haben einen stabilen Kern. Alle anderen Atome sind instabil und wandeln sich über kurz oder lang in Atome eines stabilen Isotops um. Da sie dabei im Allgemeinen ionisierende Strahlung aussenden, heißen sie auch Radioisotope oder Radionuklide. Auf der Erde wurden in den natürlichen Vorkommen neben allen 250 stabilen Isotopen 30 Radioisotope gefunden, die sich auf 10 radioaktive Elemente verteilen und die natürliche Radioaktivität verursachen. Viele weitere kurzlebige Isotope existieren im Inneren von Sternen, insbesondere während der Supernova-Phase.
Als Rydberg-Atom wird ein Atom bezeichnet, in dem ein Elektron in einem so hohen Energiezustand angeregt ist, dass es den Atomkern, teilweise auch den gesamten Atomrumpf, bestehend aus dem Atomkern und den restlichen Elektronen, in weitem Abstand umkreist und sein Verhalten damit dem eines klassischen Teilchens ähnelt. Rydberg-Atome können über 100.000 mal größer sein als nicht angeregte Atome. Da sie extrem empfindlich auf äußere Felder reagieren, kann man mit ihnen z. B. die Wechselwirkung mit einem einzelnen Photon im Detail untersuchen. Sind zwei oder mehr Elektronen in solchen Zuständen angeregt, spricht man von planetarischen Atomen.
Im teils übertragenen Sinn werden als exotische Atome auch solche Systeme bezeichnet, die in physikalischer Hinsicht gewisse Ähnlichkeiten zu den gewöhnlichen Atomen aufweisen. In ihnen kann z. B. eines der Protonen, Neutronen oder Elektronen durch ein anderes Teilchen derselben Ladung ersetzt worden sein. Wird etwa ein Elektron durch ein schwereres Myon ersetzt, bildet sich ein myonisches Atom. Als Positronium wird ein exotisches Atom bezeichnet, in dem ein Elektron statt an ein Proton an ein Positron, das ist das positiv geladene Antiteilchen des Elektrons, gebunden ist. Auch Atome, die gänzlich aus Antiteilchen zur normalen Materie aufgebaut sind, sind möglich. So wurden erstmals 1995 am Genfer CERN Antiwasserstoffatome künstlich hergestellt und nachgewiesen. An solchen exotischen Atomen lassen sich unter anderem fundamentale physikalische Theorien überprüfen.
Des Weiteren wird der Name Atom manchmal auch für 2-Teilchen-Systeme verwendet, die nicht durch elektromagnetische Wechselwirkung zusammengehalten werden, sondern durch die starke Wechselwirkung. Bei einem solchen Quarkonium handelt es sich um ein kurzlebiges Elementarteilchen vom Typ Meson, das aus einem Quark und seinem Antiteilchen aufgebaut ist. Ein Quarkonium-Atom lässt sich in seinen verschiedenen metastabilen Zuständen so durch Quantenzahlen klassifizieren wie das Wasserstoffatom.
Etwa eine Sekunde nach dem Urknall kamen die ständigen Umwandlungen zwischen den Elementarteilchen zur Ruhe, übrig blieben Elektronen, Protonen und Neutronen. In den darauf folgenden drei Minuten verbanden sich in der primordialen Nukleosynthese die vorhandenen Neutronen mit Protonen zu den einfachsten Kernen: Deuterium, Helium, in geringerem Umfang auch Lithium und möglicherweise in noch kleineren Mengen Beryllium und Bor. Die übrigen Protonen (86 %) blieben erhalten. Die ersten neutralen Atome mit dauerhaft gebundenen Elektronen wurden erst 380.000 Jahre nach dem Urknall in der Rekombinationsphase gebildet, als das Universum durch Expansion so weit abgekühlt war, dass die Atome nicht sogleich wieder ionisiert wurden.
Die Kerne aller schwereren Atome wurden und werden durch verschiedene Prozesse der Kernfusion erzeugt. Am wichtigsten ist die stellare Nukleosynthese, durch die in Sternen zunächst Helium, anschließend auch die schwereren Elemente bis zum Eisen gebildet werden. Elemente mit höheren Kernladungszahlen als Eisen entstehen in explosionsartigen Vorgängen wie im r-Prozess in Supernovae und im s-Prozess in AGB-Sternen, die kurz vor dem Ende ihrer Lebensdauer sind.
Kleine Mengen verschiedener Elemente und Isotope werden auch dadurch gebildet, dass schwere Kerne wieder geteilt werden. Das geschieht durch radioaktive Zerfälle (siehe Zerfallsreihe), die u. a. für einen Teil des Vorkommens von Helium und Blei verantwortlich sind, und Spallationen, die für die Entstehung von Lithium, Beryllium und Bor wichtig sind.
Im beobachtbaren Universum liegen die Atome mit einer mittleren Dichte von 0,25 Atome/m³ vor. Nach dem Urknallmodell (Lambda-CDM-Modell) bilden sie etwa 4,9 % der gesamten Energiedichte. Der Rest, dessen Natur noch weitgehend unklar ist, setzt sich aus etwa 27 % dunkler Materie und 68 % dunkler Energie zusammen, sowie kleinen Beiträgen von Neutrinos und elektromagnetischer Strahlung. Im Inneren einer Galaxie wie etwa der Milchstraße ist im interstellaren Medium (ISM) die Dichte der Atome wesentlich höher und liegt zwischen 10 und 10 Atome/m. Die Sonne befindet sich in der weitgehend staubfreien lokalen Blase, daher ist die Dichte in der Umgebung des Sonnensystems nur etwa 10 Atome/m. In festen Himmelskörpern wie der Erde beträgt die Atomdichte etwa 10 Atome/m.
In der Verteilung der unterschiedlichen Elemente dominiert im Universum Wasserstoff mit rund 75 % der Masse, danach folgt Helium mit etwa 25 %. Alle schwereren Elemente sind viel seltener und machen nur einen kleinen Teil der im Universum vorhandenen Atome aus. Ihre Häufigkeiten werden von den verschiedenen Mechanismen der Nukleosynthese bestimmt.
Im Sonnensystem sind Wasserstoff und Helium vorwiegend in der Sonne und den Gasplaneten enthalten. Dagegen überwiegen auf der Erde die schweren Elemente. Die häufigsten Elemente sind hier Sauerstoff, Eisen, Silicium und Magnesium. Der Erdkern besteht vorwiegend aus Eisen, während in der Erdkruste Sauerstoff und Silicium vorherrschen.
Die beiden Hauptbestandteile eines Atoms sind der Atomkern und die Atomhülle. Die Hülle besteht aus Elektronen. Sie trägt mit weniger als 0,06 % zur Masse des Atoms bei, bestimmt aber dessen Größe und dessen Verhalten gegenüber anderen Atomen, wenn sie einander nahe kommen. Der Kern besteht aus Protonen und Neutronen, ist im Durchmesser zehn- bis hunderttausendmal kleiner als die Hülle, enthält aber mehr als 99,9 % der Masse des Atoms.
Die in einem Atom vorhandenen Protonen und Neutronen, zusammen auch als Nukleonen bezeichnet, sind aneinander gebundenen und bilden den Atomkern. Die Nukleonen zählen zu den Hadronen. Das Proton ist positiv geladen, das Neutron ist elektrisch neutral. Proton und Neutron haben einen Durchmesser von etwa 1,6 fm (Femtometer) und sind selber keine Elementarteilchen, sondern nach dem Standardmodell der Elementarteilchenphysik aus den punktförmigen Quarks aufgebaut. Jeweils drei Quarks binden sich durch die starke Wechselwirkung, die durch Gluonen vermittelt wird, zu einem Nukleon. Die starke Wechselwirkung ist darüber hinaus für den Zusammenhalt der Nukleonen im Atomkern verantwortlich, insbesondere ist die Anziehung bis zu etwa 2,5 fm Abstand deutlich stärker als die gegenseitige elektrische Abstoßung der Protonen. Unterhalb von etwa 1,6 fm wird die starke Wechselwirkung der Hadronen jedoch stark abstoßend. Anschaulich gesprochen verhalten sich die Nukleonen im Kern also etwa wie harte Kugeln, die aneinander haften. Daher steigt das Volumen des Kerns proportional zur Nukleonenzahl (Massenzahl) formula_11. Sein Radius beträgt etwa formula_13 fm.
Der leichteste Atomkern besteht aus nur einem Proton. Mehrere Protonen stoßen sich zwar gemäß der Elektrostatik ab, können zusammen mit einer geeigneten Anzahl von Neutronen aber ein stabiles System bilden. Doch schon bei kleinen Abweichungen von dem energetisch günstigsten Zahlenverhältnis ist der Kern instabil und wandelt sich spontan um, indem aus einem Neutron ein Proton wird oder umgekehrt und die frei werdende Energie und Ladung als Betastrahlung abgegeben wird. Kerne mit bis zu etwa 20 Protonen sind nur bei annähernd gleich großer Neutronenzahl stabil. Darüber steigt in den stabilen Atomkernen das Verhältnis von Neutronen zu Protonen von 1:1 bis auf etwa 1,5:1, weil bei größeren Protonenzahlen wegen ihrer elektrostatischen Abstoßung die Anzahl der Neutronen schneller anwachsen muss als die der Protonen (Details siehe Tröpfchenmodell). Die Bindungsenergie liegt in stabilen Kernen (abgesehen von den leichtesten) oberhalb von 7 MeV pro Nukleon (siehe Abbildung) und übertrifft damit die Bindungsenergie der äußeren Elektronen der Atomhülle oder die chemische Bindungsenergie in stabilen Molekülen um das ca. 10-fache. Kerne mit bestimmten Nukleonenzahlen, die als Magische Zahl bezeichnet werden, beispielsweise Helium-4, Sauerstoff-16 oder Blei-208, sind besonders stabil, was mit dem Schalenmodell des Atomkerns erklärt werden kann.
Oberhalb einer Zahl von 82 Protonen (also jenseits von Blei) sind alle Kerne instabil. Sie wandeln sich durch Ausstoßen eines Kerns He-4 in leichtere Kerne um (Alphastrahlung). Dies wiederholt sich, zusammen mit Betastrahlung, so lange, bis ein stabiler Kern erreicht ist; mehrere Zerfallsstufen bilden eine Zerfallsreihe. Auch zu den Protonenzahlen 43 (Technetium) und 61 (Promethium) existiert kein stabiler Kern. Daher kann es insgesamt nur 80 verschiedene stabile chemische Elemente geben, alle weiteren sind radioaktiv. Sie kommen auf der Erde nur dann natürlich vor, wenn sie selber oder eine ihrer Muttersubstanzen eine genügend lange Halbwertzeit haben.
Da der Großteil der Atommasse von den Neutronen und Protonen stammt und diese etwa gleich schwer sind, wird die Gesamtzahl dieser Teilchen in einem Atom als Massenzahl bezeichnet. Die genaue Masse eines Atoms wird oft in der atomaren Masseneinheit u angegeben; ihr Zahlenwert ist dann etwa gleich der Massenzahl. Kleinere Abweichungen entstehen durch den Massendefekt der Atomkerne. Die atomare Masseneinheit ergibt sich aus der Definition der SI-Einheit des Mols in der Art und Weise, dass ein Atom des Kohlenstoffisotops C (im Grundzustand inklusive seiner Hüllenelektronen) eine Masse von exakt 12 u besitzt. Damit beträgt 1 u gleich 1,66053904 · 10 kg. Ein Atom des leichtesten Wasserstoffisotops hat eine Masse von 1,007825 u. Das schwerste stabile Nuklid ist das Bleiisotop Pb mit einer Masse von 207,9766521 u.
Da makroskopische Stoffmengen so viele Atome enthalten, dass die Angabe ihrer Anzahl als natürliche Zahl unhandlich wäre, erhielt die Stoffmenge eine eigene Einheit, das Mol. Ein Mol sind etwa 6,022 · 10 Atome (oder auch Moleküle oder andere Teilchen; die betrachtete Teilchenart muss immer mitgenannt werden). Die Masse von 1 Mol Atomen der Atommasse "X" u ist daher exakt "X" g. Daher ist es in der Chemie üblich, Atommassen statt in u auch indirekt in g/mol anzugeben.
In welcher Art ein instabiler Atomkern zerfällt, ist für das jeweilige Radionuklid typisch. Bei manchen Nukliden können die (untereinander völlig gleichen) Kerne auch auf verschiedene Arten zerfallen, so dass mehrere Zerfallskanäle mit bestimmten Anteilen beteiligt sind. Die wichtigsten radioaktiven Zerfälle sind
Die Energien der Strahlungen sind für das jeweilige Nuklid charakteristisch, ebenso wie die Halbwertszeit, die angibt, wie lange es dauert, bis die Hälfte einer Probe des Nuklids zerfallen ist.
Durch Anlagerung eines Neutrons kann sich ein Kern in das nächstschwerere Isotop desselben Elements verwandeln. Durch den Beschuss mit Neutronen oder anderen Atomkernen kann ein großer Atomkern in mehrere kleinere Kerne gespalten werden. Einige schwere Nuklide können sich auch ohne äußere Einwirkung spontan spalten.
Größere Atomkerne können aus kleineren Kernen gebildet werden. Dieser Vorgang wird Kernfusion genannt. Für eine Fusion müssen sich Atomkerne sehr nahe kommen. Diesem Annähern steht die elektrostatische Abstoßung beider Kerne, der sogenannte Coulombwall, entgegen. Aus diesem Grund ist eine Kernfusion (außer in bestimmten Experimenten) nur unter sehr hohen Temperaturen von mehreren Millionen Grad und hohen Drücken, wie sie im Inneren von Sternen herrschen, möglich. Die Kernfusion ist bei Nukliden bis zum Nickel-62 eine exotherme Reaktion, so dass sie im Großen selbsterhaltend ablaufen kann. Sie ist die Energiequelle der Sterne. Bei Atomkernen jenseits des Nickels nimmt die Bindungsenergie pro Nukleon ab; die Fusion schwererer Atomkerne ist daher endotherm und damit kein selbsterhaltender Prozess. Die Kernfusion in Sternen kommt daher zum Erliegen, wenn die leichten Atomkerne aufgebraucht sind.
Die Atomhülle besteht aus Elektronen, die aufgrund ihrer negativen Ladung an den positiven Atomkern gebunden sind. Sie wird oft auch als Elektronenhülle bezeichnet. Bei einem neutralen Atom beträgt die durchschnittliche Bindungsenergie der formula_14 Elektronen der Hülle etwa formula_15. Sie nimmt daher mit steigender Teilchenzahl erheblich zu, im Gegensatz zur durchschnittlichen Bindungsenergie pro Nukleon im Kern. Zur Erklärung wird angeführt, dass zwischen Nukleonen nur Bindungskräfte kurzer Reichweite wirken, die kaum über die benachbarten Teilchen hinausreichen, während die Hülle durch die elektrostatische Anziehungskraft gebunden ist, die als langreichweitige Wechselwirkung mit größerem Abstand vom Kern vergleichsweise schwach abnimmt.
Abgesehen von der Masse, die zu über 99,9 % im Atomkern konzentriert ist, ist die Atomhülle für praktisch alle äußeren Eigenschaften des Atoms verantwortlich. Der Begriff Atommodell bezieht sich daher im engeren Sinn meist nur auf die Hülle (siehe Liste der Atommodelle). Ein einfaches Atommodell ist das Schalenmodell, nach dem die Elektronen sich in bestimmten Schalen um den Kern anordnen, in denen jeweils für eine bestimmte Anzahl Elektronen Platz ist. Allerdings haben diese Schalen weder einen bestimmten Radius noch eine bestimmte Dicke, sondern überlappen und durchdringen einander teilweise.
Wesentliche Eigenschaften der Hülle sind oben unter Quantenmechanische Atommodelle und Erklärung grundlegender Atomeigenschaften dargestellt. In den nachfolgenden Abschnitten folgen weitere Details.
Die Atomhülle bestimmt die Stärke und Abstandsabhängigkeit der Kräfte zwischen zwei Atomen. Im Abstandsbereich mehrerer Atomdurchmesser polarisieren sich die gesamten Atomhüllen wechselseitig, sodass durch elektrostatische Anziehung anziehende Kräfte, die Van-der-Waals-Kräfte, entstehen. Sie bewirken vor allem die Kondensation der Gase zu Flüssigkeiten, also einen Wechsel der Aggregatzustände.
Die (näherungsweise) Inkompressibilität der Flüssigkeiten und Festkörper hingegen beruht darauf, dass alle Atome bei starker Annäherung einander stark abstoßen, sobald sich ihre Hüllen im Raum merklich überschneiden und daher verformen müssen. Außer im Fall zweier Wasserstoff­atome, die jeweils nur ein Elektron in der Hülle haben, spielt die elektrostatische Abstoßung der beiden Atomkerne dabei nur eine geringe Rolle.
In einem mittleren Abstandsbereich zwischen dem Vorherrschen der schwach anziehenden Van-der-Waals-Kräfte und der starken Abstoßung kommt es zwischen zwei oder mehr zueinander passenden Atomhüllen zu einer besonders starken Anziehung, der chemischen Bindung. Bei Atomen bestimmter Elemente kann diese Anziehung zu einem stabilen Molekül führen, das aus Atomen in zahlenmäßig genau festgelegter Beteiligung und räumlicher Anordnung aufgebaut ist. Die Moleküle sind die kleinsten Stoffeinheiten der chemischen Verbindungen, also der homogenen Materialien in all ihrer Vielfalt. Vermittelt über die Hüllen ihrer Atome ziehen auch Moleküle einander an. Ein fester Körper entsteht, wenn viele Moleküle sich aneinander binden und dabei, weil es energetisch günstig ist, eine feste Anordnung einhalten. Ist diese Anordnung regelmäßig, bildet sich ein Kristallgitter. Infolge dieser Bindung ist der feste Körper nicht nur weitgehend inkompressibel wie eine Flüssigkeit, sondern im Unterschied zu dieser auch auf Zug belastbar und deutlich weniger leicht verformbar. Verbinden sich Atome metallischer Elemente miteinander, ist ihre Anzahl nicht festgelegt und es können sich nach Größe und Gestalt beliebige Körper bilden. Vor allem chemisch reine Metalle zeigen dann meist auch eine große Verformbarkeit. Verbindungen verschiedener Metalle werden Legierung genannt. Die Art der Bindung von Metallatomen erklärt, warum Elektronen sich fast frei durch das Kristallgitter bewegen können, was die große elektrische Leitfähigkeit und Wärmeleitfähigkeit der Metalle verursacht. Zusammengefasst ergeben sich aus der Wechselwirkung der Atomhüllen miteinander die mechanische Stabilität und viele weitere Eigenschaften der makroskopischen Materialien.
Aufgrund des unscharfen Randes der Atomhülle liegt die Größe der Atome nicht eindeutig fest. Die als Atomradien tabellierten Werte sind aus der Bindungslänge gewonnen, das ist der energetisch günstigste Abstand zwischen den Atomkernen in einer chemischen Bindung. Insgesamt zeigt sich mit steigender Ordnungszahl eine in etwa periodische Variation der Atomgröße, die mit der periodischen Variation des chemischen Verhaltens gut übereinstimmt. Im Periodensystem der Elemente gilt allgemein, dass innerhalb einer Periode, also einer Zeile des Systems, eine bestimmte Schale aufgefüllt wird. Von links nach rechts nimmt die Größe der Atome dabei ab, weil die Kernladung anwächst und daher alle Schalen stärker angezogen werden. Wenn eine bestimmte Schale mit den stark gebundenen Elektronen gefüllt ist, gehört das Atom zu den Edelgasen. Mit dem nächsten Elektron beginnt die Besetzung der Schale mit nächstgrößerer Energie, was mit einem größeren Radius verbunden ist. Innerhalb einer Gruppe, also einer Spalte des Periodensystems, nimmt die Größe daher von oben nach unten zu. Dementsprechend ist das kleinste Atom das Heliumatom am Ende der ersten Periode mit einem Radius von 32 pm, während eines der größten Atome das Caesium­atom ist, das erste Atom der 5. Periode. Es hat einen Radius von 225 pm.
Die dem Schalenmodell zugrundeliegenden Elektronenschalen ergeben sich durch die Quantisierung der Elektronenenergien im Kraftfeld des Atomkerns nach den Regeln der Quantenmechanik. Um den Kern herum bilden sich verschiedene Atomorbitale, das sind unscharf begrenzte Wahrscheinlichkeitsverteilungen für "mögliche" räumliche Zustände der Elektronen. Jedes Orbital kann aufgrund des Pauli-Prinzips mit maximal zwei Elektronen besetzt werden, dem Elektronenpaar. Die Orbitale, die unter Vernachlässigung der gegenseitigen Abstoßung der Elektronen und der Feinstruktur theoretisch die gleiche Energie hätten, bilden eine Schale. Die Schalen werden mit der Hauptquantenzahl durchnummeriert oder fortlaufend mit den Buchstaben "K, L, M,"… bezeichnet. Genauere Messungen zeigen, dass ab der zweiten Schale nicht alle Elektronen einer Schale die gleiche Energie besitzen. Falls erforderlich, wird durch die Nebenquantenzahl oder Drehimpulsquantenzahl eine bestimmte Unterschale identifiziert.
Sind die Orbitale, angefangen vom energetisch niedrigsten, so weit mit Elektronen besetzt, dass die gesamte Elektronenzahl gleich der Protonenzahl des Kerns ist, ist das Atom neutral und befindet sich im Grundzustand. Werden in einem Atom ein oder mehrere Elektronen in energetisch höherliegende Orbitale versetzt, ist das Atom in einem angeregten Zustand. Die Energien der angeregten Zustände haben für jedes Atom wohlbestimmte Werte, die sein Termschema bilden. Ein angeregtes Atom kann seine Überschussenergie abgeben durch Stöße mit anderen Atomen, durch Emission eines der Elektronen (Auger-Effekt) oder durch Emission eines Photons, also durch Erzeugung von Licht oder Röntgenstrahlung. Bei sehr hoher Temperatur oder in Gasentladungen können die Atome durch Stöße Elektronen verlieren (siehe Ionisationsenergie), es entsteht ein Plasma, so z. B. in einer heißen Flamme oder in einem Stern.
Da die Energien der Quanten der emittierten Strahlung je nach Atom bzw. Molekül und den beteiligten Zuständen verschieden sind, lässt sich durch Spektroskopie dieser Strahlung die Quelle im Allgemeinen eindeutig identifizieren. Beispielsweise zeigen die einzelnen Atome ihr elementspezifisches optisches Linienspektrum. Bekannt ist etwa die Natrium-D-Linie, eine Doppellinie im gelben Spektralbereich bei 588,99 nm und 589,59 nm, die auch in nebenstehender Abbildung mit D-1 bezeichnet wird. Ihr Aufleuchten zeigt die Anwesenheit von angeregten Natrium-Atomen an, sei es auf der Sonne oder über der Herdflamme bei Anwesenheit von Natrium oder seinen Salzen. Da diese Strahlung einem Atom auch durch Absorption dieselbe Energie zuführen kann, lassen sich die Spektrallinien der Elemente sowohl in Absorptions- als auch in Emissionsspektren beobachten. Diese Spektrallinien lassen sich auch verwenden, um Frequenzen sehr präzise zu vermessen, beispielsweise für Atomuhren.
Obwohl Elektronen sich untereinander elektrostatisch abstoßen, können zusätzlich bis zu zwei weitere Elektronen gebunden werden, wenn es bei der höchsten vorkommenden Elektronenenergie noch Orbitale mit weiteren freien Plätzen gibt (siehe Elektronenaffinität).
Chemische Reaktionen, d. h. die Verbindung mehrerer Atome zu einem Molekül oder sehr vieler Atome zu einem Festkörper, werden dadurch erklärt, dass ein oder zwei Elektronen aus einem der äußeren Orbitale eines Atoms (Valenzelektronen) unter Energiegewinn auf einen freien Platz in einem Orbital eines benachbarten Atoms ganz hinüberwechseln (Ionenbindung) oder sich mit einer gewissen Wahrscheinlichkeit dort aufhalten (kovalente Bindung durch ein bindendes Elektronenpaar). Dabei bestimmt die Elektronegativität der Elemente, bei welchem Atom sich die Elektronen wahrscheinlicher aufhalten. In der Regel werden chemische Bindungen so gebildet, dass die Atome die Elektronenkonfiguration eines Edelgases erhalten (Edelgasregel). Für das chemische Verhalten des Atoms sind also Form und Besetzung seiner Orbitale entscheidend. Da diese allein von der Protonenzahl bestimmt werden, zeigen alle Atome mit gleicher Protonenzahl, also die Isotope eines Elements, nahezu das gleiche chemische Verhalten.
Nähern sich zwei Atome über die chemische Bindung hinaus noch stärker an, müssen die Elektronen eines Atoms wegen des Pauli-Prinzips auf freie, aber energetisch ungünstige Orbitale des anderen Atoms ausweichen, was einen erhöhten Energiebedarf und damit eine abstoßende Kraft nach sich zieht.
Mit großer Genauigkeit wird die Wechselwirkung zwischen Kern und Hülle schon durch den einfachen Ansatz beschrieben, in dem der Kern eine punktförmige Quelle eines elektrostatischen Felds nach dem Coulomb-Gesetz darstellt. Alle genannten Atommodelle beruhen hierauf. Aufgrund zusätzlicher Effekte, die in erweiterten Modellen behandelt werden, sind nur extrem kleine Korrekturen nötig, die unter dem Namen Hyperfeinstruktur zusammengefasst werden. Zu berücksichtigen sind hier drei Effekte: erstens die endliche Ausdehnung, die jeder Kern besitzt, zweitens eine magnetische Dipolwechselwirkung, wenn sowohl Kern als auch Hülle eine Drehimpulsquantenzahl von mindestens ½ haben, und drittens eine elektrische Quadrupolwechselwirkung, wenn beide Drehimpulsquantenzahlen mindestens 1 sind.
Die endliche Ausdehnung des Kerns – verglichen mit einer theoretischen Punktladung – bewirkt eine schwächere Anziehung derjenigen Elektronen, deren Aufenthaltswahrscheinlichkeit bis in den Kern hineinreicht. Betroffen sind nur "s"-Orbitale (Bahndrehimpuls Null). Bei Atomen mittlerer Ordnungszahl liegt die Korrektur in der Größenordnung von 1 %. Die magnetischen Dipol- bzw. elektrischen Quadrupol-Momente von Hülle und Kern bewirken eine Kopplung mit der Folge, dass die Gesamtenergie eines freien Atoms je nach Quantenzahl seines Gesamtdrehimpulses äußerst geringfügig aufgespalten ist. Im H-Atom beträgt die Aufspaltung etwa ein Millionstel der Bindungsenergie des Elektrons (siehe 21-cm-Linie). Anschaulich gesprochen hängt die Energie davon ab, in welchem Winkel die Achsen des magnetischen Dipolmoments bzw. elektrischen Quadrupolmoments von Kern und Hülle zueinander stehen.
Auch bei Atomen in Flüssigkeiten und Festkörpern treten diese Wechselwirkungen in entsprechend modifizierter Form auf. Trotz der Kleinheit der dadurch verursachten Effekte haben sie eine große Rolle in der Atom- und Kernforschung gespielt und sind in besonderen Fällen auch bei modernen Anwendungen wichtig.
Indirekte Möglichkeiten, Atome zu erkennen, beruhen auf der Beobachtung der von ihnen ausgehenden Strahlung. So kann aus Atomspektren beispielsweise die Elementzusammensetzung entfernter Sterne bestimmt werden. Die verschiedenen Elemente lassen sich durch charakteristische Spektrallinien identifizieren, die auf Emission oder Absorption durch Atome des entsprechenden Elements in der Sternatmosphäre zurückgehen. Gasentladungslampen, die dasselbe Element enthalten, zeigen diese Linien als Emissionslinien. Auf diese Weise wurde z. B. 1868 Helium im Spektrum der Sonne nachgewiesen – über 10 Jahre bevor es auf der Erde entdeckt wurde.
Ein Atom kann ionisiert werden, indem eines seiner Elektronen entfernt wird. Die elektrische Ladung sorgt dafür, dass die Flugbahn eines Ions von einem Magnetfeld abgelenkt wird. Dabei werden leichte Ionen stärker abgelenkt als schwere. Das Massenspektrometer nutzt dieses Prinzip, um das Masse-zu-Ladung-Verhältnis von Ionen und damit die Atommassen zu bestimmen.
Die Elektronenenergieverlustspektroskopie misst den Energieverlust eines Elektronenstrahls bei der Wechselwirkung mit einer Probe in einem Transmissionselektronenmikroskop.
Eine direkte Abbildung, die einzelne Atome erkennen lässt, wurde erstmals 1951 mit dem Feldionenmikroskop (oder Feldemissionsmikroskop) erzielt, das heute auch im Physikunterricht in der Schule gezeigt werden kann. Auf einem kugelförmigen Bildschirm, in dessen Mittelpunkt sich eine extrem feine Nadelspitze befindet, erscheint ein etwa millionenfach vergrößertes Bild, in dem die einzelnen Atome in der Spitze nebeneinander als Lichtpunkte zu erkennen sind. Das Bild entsteht in Echtzeit und erlaubt z. B. die Betrachtung der Wärmebewegung einzelner Fremdatome auf der Spitze.
Auch das Rastertunnelmikroskop ist ein Gerät, das einzelne Atome an der Oberfläche eines Körpers sichtbar macht. Es verwendet den Tunneleffekt, der es Teilchen erlaubt, eine Energiebarriere zu passieren, die sie nach klassischer Physik nicht überwinden könnten. Bei diesem Gerät tunneln Elektronen zwischen einer elektrisch leitenden Spitze und der elektrisch leitenden Probe. Bei Seitwärtsbewegungen zur Abrasterung der Probe wird die Höhe der Spitze so nachgeregelt, dass immer derselbe Strom fließt. Die Bewegung der Spitze bildet die Topographie und Elektronenstruktur der Probenoberfläche ab. Da der Tunnelstrom sehr stark vom Abstand abhängt, ist die laterale Auflösung viel feiner als der Radius der Spitze, manchmal atomar.
Eine tomographische Atomsonde erstellt ein dreidimensionales Bild mit einer Auflösung unterhalb eines Nanometers und kann einzelne Atome ihrem chemischen Element zuordnen.

</doc>
<doc id="104" url="https://de.wikipedia.org/wiki?curid=104" title="Arzt">
Arzt

Ein Arzt bzw. eine Ärztin beschäftigt sich mit der Vorbeugung (Prävention), Erkennung (Diagnose), Behandlung (Therapie) und Nachsorge von Krankheiten, und stellt sich damit in den Dienst der Gesundheit des Menschen. Bei seinem Handeln ist er moralischen und ethischen Grundsätzen verpflichtet (Vergleiche Genfer Deklaration des Weltärztebundes, Eid des Hippokrates). Die Vielfalt an Krankheiten und Behandlungsmöglichkeiten hat in der Humanmedizin mit der Zeit zu einer großen Anzahl von Fachgebieten und weiteren Differenzierungen geführt (→ Liste medizinischer Fachgebiete).
Die Bezeichnung "Arzt" (mittelhochdeutsch "arzât", neuniederländisch "arts") zog während des Mittelalters aus der lateinischen Gelehrtensprache ins Deutsche ein, und zwar über die latinisierte Variante "archiater" des griechischen ' ("archiatros"; klassische Aussprache []) „Oberarzt, Leibarzt“, einer Zusammensetzung aus ' ("arche"; kl. Ausspr. []) „Herrschaft, Kommando“ und ' ("iatros"; kl. Ausspr. []) „Arzt“. In vielen fachsprachlichen Komposita tritt das ursprüngliche griechische Wort ' bzw. die latinisierte Form "-iater" als Wortbestandteil auf: "iatrogen" „durch ärztliches Handeln verursacht“; "Psychiater" „Seelenarzt“ usw. Über deutsche Vermittlung gelang das Wort in andere Sprachen, so lettisch "ārsts", estnisch "arst".
Die germanische Bezeichnung für den Heilberuf (althochdeutsch "lâchi") ist beispielsweise im dänischen "læge", im schwedischen "läkare", im englischen "leech" („Blutegel“) oder im deutschen Familiennamen "Lachmann" erhalten und hat sich in andere Sprachen verbreitet, z. B. finnisch "lääkäri", gälisch "dochtúir leighis". Im polnischen "lekarz" und tschechischen "lékař" ist die germanische Wurzel mit einem slawischen Suffix ("-arz", "-ař") verbunden.
Die lateinische Bezeichnung "medicus" „Arzt, Wundarzt“ oder eine davon abgeleitete Form findet sich vor allem in den romanischen Sprachen, etwa italienisch "medico", spanisch/portugiesisch "médico", rumänisch "medic", französisch "médecin", aber unter romanischem Einfluss auch in anderen Sprachen: baskisch "mediku", englisch "medic". Die Bezeichnung "physicus" meinte meist einen akademisch ausgebildeten Arzt.
In vielen Sprachen wird der Arzt umgangssprachlich nach seinem zumeist geführten akademischen Grad "Doktor" genannt.
Die Funktion des Arztes ist eine der ältesten der Menschheit. Medizingeschichtlich gesehen entstand der Arztberuf aus dem Stand der Heilkundigen, die schon unter den Priestern des Altertums zu finden waren.
Während die körperliche Gesundheit von männlichen Ärzten mit derjenigen der allgemeinen männlichen Bevölkerung vergleichbar zu sein scheint, scheint die körperliche Gesundheit von Ärztinnen besser zu sein als die der allgemeinen weiblichen Bevölkerung.
Hinsichtlich der psychischen Gesundheit fällt auf, dass Depressionen und Suchterkrankungen bei Ärzten häufiger vorkommen als in der restlichen Bevölkerung. Ein weiteres bei Medizinern häufig auftretendes Krankheitsbild ist das Burnout-Syndrom, das bereits bei Medizinstudenten in einer erhöhten Rate nachgewiesen werden kann.
Mehrere Studien zeigten eine gegenüber der allgemeinen Bevölkerung erhöhte Suizidrate unter Ärzten. Das gegenüber der Normalbevölkerung erhöhte relative Risiko, einen Suizid zu begehen, lag für Ärzte bei 1,1–3,4 und für Ärztinnen bei 2,5–3,7. Da in den Studien meist nur eine kleine Zahl von Suiziden untersucht wurde, waren die Vertrauensbereiche des wahren Wertes der Risikoerhöhung weit. Es wird vermutet, dass eine beträchtliche Anzahl von Selbstmorden nicht erfasst werden, da diese fälschlicherweise als Vergiftungen oder Unfälle deklariert werden. Von den verschiedenen beruflichen Spezialisierungen sind insbesondere Psychiater, Anästhesisten und Allgemeinmediziner von einer erhöhten Suizidrate betroffen. Als Ursachen des erhöhten Suizidrisikos werden verschiedene Faktoren diskutiert. Ein Persönlichkeitsprofil mit zwanghaften Zügen kann infolge der beruflichen Anforderungen zu einer depressiven Störung führen. Die Schwierigkeiten, Familie und Karrierewunsch miteinander zu vereinbaren, können insbesondere bei Ärztinnen zu Erschöpfung und Depression führen. Suchterkrankungen (wie beispielsweise Alkohol-, Drogen- und Medikamentenabhängigkeit), die bei Ärzten häufiger auftreten, gehen ihrerseits häufiger mit Depressionen und einer erhöhten Suizidrate einher. Dieses für Ärzte und Ärztinnen festgestellte Risikoprofil ist berufsunabhängig und trifft für die meisten Suizidenten zu.
Psychische Probleme korrelieren häufig mit Zeitdruck und mangelnder Autonomie am Arbeitsplatz sowie belastenden Patient-Arzt-Beziehungen. Ärzte werden seltener krankgeschrieben und zeigen eine mangelhafte Inanspruchnahme medizinischer Versorgungsleistungen. Häufig behandeln sich Ärzte selbst. Die eigenständige Behandlung eigener psychischer Störungen ist jedoch häufig ineffektiv.
Die Heiligen Zwillingsbrüder Cosmas und Damian gelten wegen ihres Arztberufs unter anderem auch als Schutzpatrone der Ärzte. Ein weiterer Schutzpatron ist der heilige Pantaleon, einer der Vierzehn Nothelfer.
Der Arzt gehört in Deutschland zu den Freien Berufen und ist ein klassischer Kammerberuf.
Ärzte unterliegen einer staatlichen Überwachung der Zulassung (Approbation in Deutschland, s. u. in anderen EU-Ländern) und unter anderem dem Arztwerberecht, welches weitgehende Einschränkungen in der Publikation und Veröffentlichungen bedeutet. Ärzte haften ihren Patienten zwar nicht auf Erfolg ihres Handelns, können ihnen aber unter dem Gesichtspunkt der Arzthaftung zum Schadenersatz verpflichtet sein.
Die freie Ausübung der Heilkunde ist in Deutschland nur approbierten Ärzten erlaubt, mit festgelegten Einschränkungen dürfen auch Heilpraktiker Kranke behandeln, wobei die klar festgelegten Grenzen einzuhalten sind. Ausnahmsweise werden spezielle Bereiche der Diagnostik und Therapie auch (meist auf Veranlassung von Ärzten) von Angehörigen der Gesundheitsfachberufe durchgeführt.
Ab dem Zeitpunkt der ärztlichen Approbation darf der Arzt die gesetzlich geschützte Bezeichnung „Arzt“ führen und erhält mit ihr die staatliche Erlaubnis zur eigenverantwortlichen und selbstständigen ärztlichen Tätigkeit. Die bundesweit einheitliche Approbationsordnung regelt das zuvor erfolgreich abzuleistende mindestens sechsjährige Medizinstudium bezüglich der Dauer und der Inhalte der Ausbildung in den einzelnen Fächern, sowie der Prüfungen. Das Studium der Medizin umfasst u. a. drei Staatsexamina sowie 1 Jahr praktische Tätigkeit (sog. Praktisches Jahr). Von Oktober 1988 bis Oktober 2004 war zur Erlangung der Vollapprobation zusätzlich eine 18-monatige Tätigkeit als „Arzt im Praktikum“ unter Aufsicht eines approbierten Arztes notwendig. Meist arbeitet ein approbierter Arzt für mehrere Jahre als Assistenzarzt an einer von der Landesärztekammer anerkannten Weiterbildungsstätte (z. B. Klinik oder Praxis), um sich auf einem oder mehreren Spezialgebieten der Medizin weiterzubilden und evtl. nach zusätzlich mindestens 4-jähriger Weiterbildungszeit einen Facharzttitel zu erwerben. Einzelheiten dazu sind in den Weiterbildungsordnungen der Landesärztekammern geregelt. Niedergelassene Ärzte arbeiten in freier Praxis, gegebenenfalls auch mit mehreren Ärzten in einer Berufsausübungsgemeinschaft (früher: Gemeinschaftspraxis) oder Praxisgemeinschaft (s. a. Vertragsarztrechtsänderungsgesetz). Honorarärzte arbeiten auf Honorarbasis für verschiedene niedergelassene Ärzte oder Kliniken.
Jeder Arzt ist Pflichtmitglied der Ärztekammer (Landesärztekammer), in deren Gebiet er seine ärztliche Tätigkeit ausübt. Im Jahr 2012 waren in Deutschland bei den Landesärztekammern 459.021 Ärzte gemeldet. Zur Behandlung von Versicherten der Gesetzlichen Krankenversicherungen benötigt der Arzt eine Zulassung (Arzt in eigener Praxis) oder Ermächtigung (als Arzt in einem Krankenhaus oder ähnlicher Institution) und ist dann auch Pflichtmitglied der Kassenärztlichen Vereinigung seines Niederlassungsbezirks. Die Kassenärztliche Zulassung besitzen 135.388 Ärzte (Stand 31. Dezember 2008): 58.095 Hausärzte und 77.293 Fachärzte. In den Kliniken sind 146.300 Ärzte beschäftigt. Ende 2013 arbeiteten 35.893 ausländische Ärzte in Deutschland. 2013 betrug die Zahl der berufstätigen Ärzte in Deutschland 357.252.
Strafrechtlich sind ärztliche Eingriffe der Körperverletzung gleichgesetzt. Diese ist nicht strafbar, wenn die Einwilligung der behandelten Person nach einer Aufklärung vorliegt und die Handlung auf dem Stand des aktuellen medizinischen Wissens vorgenommen wird (§§ 223 ff StGB). Ausnahmen bestehen, wenn der Patient aufgrund seines Zustandes (z. B. Bewusstlosigkeit) nicht in der Lage ist, seine Entscheidung mitzuteilen, und durch die Unterlassung des Eingriffs die Gefahr von negativen gesundheitlichen Folgen oder sogar dem Tod des Patienten besteht. Zudem können eingeschränkt- oder nichteinwilligungsfähige Personen, wie z. B. Kinder oder in bestimmten Fällen seelisch Erkrankte, auch gegen ihren Willen behandelt werden. Hierfür existieren strenge rechtliche Regelungen und Verfahrenswege, bei welchen neben dem Arzt auch andere Institutionen, z. B. Amtsgericht oder gesetzlicher Betreuer, an der Entscheidung mitwirken.
Aus Sicht des Bundesgerichtshofs (BGH) zählen Kassenärzte nicht als Beauftragte einer Krankenkasse oder als Amtsträger. Sie machen sich daher nicht im Sinne des Bestechungsparagraphs 299 des Strafgesetzbuches strafbar, wenn sie Geschenke von Pharmaunternehmen bzw. -vertretern annehmen. Zu diesem Urteil ist der BGH im März 2012 gekommen.
Die Verordnung von rezeptpflichtigen Arzneimitteln und die meisten invasiven Maßnahmen sind in Deutschland ausnahmslos dem approbierten Arzt vorbehalten. Hierbei ist er persönlich zur Einhaltung des anerkannten wissenschaftlichen Standes und ethischer Vorgaben verpflichtet. Weiter unterliegen Ärzte speziellen Regelungen, wie dem Berufs- und Standesrecht, welches auch an die Genfer Konvention anknüpft. Insbesondere ist auch im Strafrecht die Einhaltung der ärztlichen Schweigepflicht nach § 203 StGB festgehalten.
In Deutschland sind aus historischen Gründen eine Reihe unterschiedlicher medizinischer akademischer Grade anzutreffen. Diese weisen im Gegensatz zum Facharzttitel nicht auf eine besondere Fachkompetenz hin, sondern dienen vorrangig als Beleg einer wissenschaftlichen Leistung in einem medizinischen Bereich. Überwiegend wird man die folgenden akademischen Grade antreffen:
Laut einer Studie des Instituts für Qualität und Wirtschaftlichkeit im Gesundheitswesen haben deutsche Ärzte trotz längerer Arbeitszeiten je Patient die kürzeste Sprechzeit in Europa. Sie liegt 30 % niedriger als der europäische Durchschnitt.
Die Einkommen von Ärzten in Deutschland variieren stark, da das Spektrum medizinischer Tätigkeiten sehr breit gefächert ist. Auch finden sich unter Ärzten erhebliche Unterschiede bei der Arbeitszeit, insbesondere zwischen klinisch tätigen (bspw. 24 Std.-Schichten sowie eine hohe Anzahl an Überstunden) und niedergelassenen (hoher Anteil „nicht-medizinischer“-Tätigkeit aufgrund der Selbständigkeit).
Nach Schätzungen des Spitzenverband Bund der Krankenkassen wird das Durchschnittseinkommen der niedergelassenen Ärzte 2010 auf 164.000 Euro brutto steigen.
Um einem Mangel an Landärzten entgegenzuwirken, will die Bundesregierung in einem neuen „Versorgungsgesetz“ das Einkommen von Landärzten erhöhen.
Neben den strengen rechtlichen Vorgaben zur Ausübung seines Berufs ist der Arzt auch bei der Außendarstellung bzw. Werbung zu seinen Leistungen und seiner Praxis mit umfangreichen Verordnungen und Gesetzen konfrontiert. Im Unterschied zu anderen Branchen ist Ärzten anpreisende oder vergleichende Werbung absolut verboten. Seit dem 105. Deutschen Ärztetag sind sachliche, berufsbezogene Informationen über ihre Tätigkeit gestattet. Hauptkriterium ist dabei das schützenswerte Interesse des mündigen Patienten.
Ende 2006 waren in Deutschland ca. 407.000 Ärzte gemeldet, davon sind 95.700 ohne ärztliche Tätigkeit (siehe Abb.). Die Kassenärztliche Zulassung besitzen 59.000 Hausärzte und 60.600 Fachärzte. In den Kliniken sind 148.300 Ärzte beschäftigt.
Im Jahr 2011 wurden in Deutschland rund 342.100 berufstätige Ärzte und rund 107.300 Ärzte ohne ärztliche Tätigkeit gezählt. Auf durchschnittlich 239 Einwohner kam ein berufstätiger Arzt.
Die chronologische Entwicklung kann aus der folgenden Tabelle und der Abbildung abgelesen werden.
Arztbesuche: Deutsche Erwachsene (zwischen 18 und 79 Jahren) gehen im Durchschnitt 9,2-mal pro Jahr zum Arzt.
In Österreich ist man mit der Sponsion zunächst "Doktor der gesamten Heilkunde" (Doctor medicinae universae/Dr. med. univ.). Mittlerweile handelt es sich entgegen der Bezeichnung nicht um einen Doktorgrad, sondern um einen Diplomgrad ähnlich dem Magister oder dem Diplomingenieur. Vor dem Wintersemester 2002/03 war das Medizinstudium in Österreich ein Doktoratsstudium, welches auch Übergangsregelungen kannte. Der eigentliche Doktorgrad der Medizin ("Doctor scientae medicinae" bzw. "Dr. scient. med.") kann seitdem im Anschluss an das Diplomstudium in einem dreijährigen Doktoratsstudium erworben werden.
Selbständig als Arzt tätig werden darf man nur, wenn für drei Jahre im Rahmen des „Turnus“ verschiedene (definierte) Disziplinen durchlaufen wurden und die Arbeit vom jeweiligen Abteilungsvorstand positiv bewertet wurde. Danach ist eine weiter abschließende Prüfung abzulegen. Damit hat man das „jus practicandi“ erworben, also die Berechtigung zur selbständigen Berufsausübung als Arzt für Allgemeinmedizin. Alternativ kann sofort nach der Sponsion die (meist sechsjährige) Ausbildung zu einem Facharzt erfolgen, nach der wiederum eine Prüfung abzulegen ist. Viele Fachärzte absolvieren den Turnus vor Beginn der Ausbildung ganz oder teilweise. Es hat sich in Österreich eingebürgert, die Ausbildung zum Allgemeinmediziner zuvor abzuleisten. Viele Krankenhäuser nehmen nur Assistenzärzte mit abgeschlossener Turnusausbildung in Dienst, da diese einen Nacht- oder Wochenenddienst alleine ableisten dürfen.
Ärzte aus anderen EU-Staaten können um Anerkennung als "approbierte Ärzte" ansuchen.
Am 14. Dezember 2010 hat die EU-Kommission in ihrem Amtsblatt C377/10 eine Änderungsmitteilung für die EU-Richtlinie 2005/36, Anhang 5.1.1. veröffentlicht, wonach ab diesem zeitpunkt sämtliche Absolventen des österreichischen Medizinstudiums bereits mit der Promotion ihr Grunddiplom abgeschlossen haben und somit innerhalb des gesamten EU- und EWR-Raumes sowie der Schweiz und Liechtenstein eine selbständige Tätigkeit bzw. Ausbildung zum Facharzt unter denselben Voraussetzungen wie einheimische Mediziner aufnehmen dürfen. Bis dahin hatten Mediziner aus Österreich erst mit dem Abschließen der Ausbildung zum Allgemeinmediziner bzw. Facharzt ein Anrecht auf automatische Anrechnung ihres Diploms in den übrigen Mitgliedsstaaten.
Der (niedergelassene) Arzt gehört in Österreich zu den Freien Berufen (Berufe von öffentlicher Bedeutung).
In der Schweiz ist man nach dem mit dem Staatsexamen abgeschlossenen sechsjährigen Studium zunächst eidgenössisch diplomierter Arzt und als solcher zur Arbeit als Assistenzarzt in Krankenhäusern und Arztpraxen befugt.
Die Weiterbildung zum zur selbständigen Berufsausübung befugten Facharzt dauert je nach Fach zwischen drei („praktischer Arzt“) und 8 Jahren nach dem Studienabschluss. Für einen Facharzttitel muss zudem eine Facharztprüfung abgelegt werden. Danach darf sich der Arzt „Facharzt für <Fachgebiet> FMH“ nennen. Die Erlaubnis zur Praxiseröffnung ist kantonal geregelt, die Zulassung zur Berufsausübung zulasten der Krankenkassen wird vom Krankenkassenzentralverband Santésuisse erteilt, ist aber nur eine Formalität. Aktuell besteht aber ein Praxiseröffnungs-Stopp, welcher die Berufsausübung zulasten der Krankenkassen einschränkt. Lediglich bei Bedarfsnachweis, z. B. bei einer Praxisübernahme, ist eine Zulassung möglich.
Die jeweilige Fachgesellschaft prüft, ob jeder Facharzt seiner Fortbildungspflicht (je nach Fachgebiet 60–100 Stunden pro Jahr) nachkommt.
Seit dem 1. Januar 2005 gilt für die Assistenzärzte und Oberärzte eine durch das landesweit gültige Arbeitszeitgesetz begründete maximale Wochenarbeitszeit von 50 Stunden. Bis dahin waren Verträge mit der Formulierung „Die Arbeitszeit richtet sich nach den Bedürfnissen des Spitals“ üblich, wodurch Arbeitszeiten oft über 60 und 70 Stunden pro Woche, ohne finanziellen Ausgleich zu leisten waren.
Die Entgelte der Assistenzärzte liegen deswegen auf dem Niveau der Pflegenden im oberen Kader (Pflegedienstleitungen).
Die Leitenden Ärzte und Chefärzte sind finanziell in der Gesamtvergütung deutlich höher gestellt. Sie sind aus dem Arbeitszeitgesetz ausgegliedert.
Nationales:

</doc>
